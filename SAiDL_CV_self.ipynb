{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SAiDL_CV_self.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe284e36ed1f4249a871ececae311646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b748098cbc444c6a91de079086c822ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3601fb5eeefe426ab0a9b729eb9abbf0",
              "IPY_MODEL_48bc5e90bb3c497799dc3185777f2f69"
            ]
          }
        },
        "b748098cbc444c6a91de079086c822ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3601fb5eeefe426ab0a9b729eb9abbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_04daaaa4d89745b0be3d5239c3b6d945",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2640397119,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2640397119,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c818a5f9b41479d8b8eb0c6d71a7fff"
          }
        },
        "48bc5e90bb3c497799dc3185777f2f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b3e32058513d437292b6c7c1a75a6fbb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2640397312/? [06:41&lt;00:00, 6568963.66it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9270895935b143a686ca4766a0d2fced"
          }
        },
        "04daaaa4d89745b0be3d5239c3b6d945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c818a5f9b41479d8b8eb0c6d71a7fff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3e32058513d437292b6c7c1a75a6fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9270895935b143a686ca4766a0d2fced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM9KsArYy-Yw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as f\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader,Dataset,random_split\n",
        "import PIL\n",
        "import random\n",
        "if (torch.cuda.is_available()):\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZWNFzvKiPmt"
      },
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self,in_channels):\n",
        "    super().__init__()\n",
        "    self.m1 = nn.Sequential(\n",
        "                             nn.Conv2d(in_channels,in_channels,3,padding = 1),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Conv2d(in_channels,in_channels,3,padding = 1),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Conv2d(in_channels,in_channels*2,3,padding = 1,stride = 2),\n",
        "                             nn.ReLU()\n",
        "    )\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.m1(x)\n",
        "\n",
        "class Encode(nn.Module):\n",
        "  def __init__(self,in_channels):\n",
        "    super().__init__()\n",
        "    self.e = nn.Sequential(\n",
        "                              nn.Conv2d(3,in_channels,3,padding=1),\n",
        "                              nn.ReLU(),\n",
        "                              Block(in_channels),\n",
        "                              Block(in_channels*2),\n",
        "                              Block(in_channels*4),                     \n",
        "                              Block(in_channels*8),\n",
        "                              Block(in_channels*16)\n",
        "      )\n",
        "    self.e2 = nn.Sequential(\n",
        "                              nn.Linear(9216,2048),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(2048,1024)\n",
        "                              )\n",
        "  def forward(self,x,flag = True):\n",
        "    x = self.e(x)\n",
        "    x = torch.reshape(x,(x.shape[0],-1,1,1))\n",
        "    x = torch.squeeze(x,dim =2)\n",
        "    x = torch.squeeze(x,dim =2)\n",
        "    if flag:\n",
        "      x = self.e2(x)\n",
        "    return x\n",
        "\n",
        "class Network(nn.Module):\n",
        "   def __init__(self,in_channels):\n",
        "    super().__init__()\n",
        "    self.e = Encode(in_channels)\n",
        "    self.c = nn.Sequential(\n",
        "                            nn.Linear(9216,10),\n",
        "                            nn.Softmax()                     \n",
        "    )\n",
        "\n",
        "  \n",
        "   def forward(self,x,encode = True,classify = True):\n",
        "    if encode:\n",
        "      x = self.e(x,True)      \n",
        "    if classify:\n",
        "      x = self.e(x,False)\n",
        "      x = self.c(x)\n",
        "    return x\n",
        "model = Network(32)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQCGzTrmHZYB"
      },
      "source": [
        "class Unsoupdata(Dataset):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    transform = transforms.ToTensor()\n",
        "    self.t = transforms.Compose([\n",
        "                            transforms.RandomVerticalFlip(0.5),\n",
        "                            transforms.RandomHorizontalFlip(0.5),\n",
        "                            transforms.RandomApply([transforms.ColorJitter()],p=0.5)\n",
        "    ])\n",
        "    self.data = torchvision.datasets.STL10(root = '/',split = 'unlabeled',download = True,transform = transform)\n",
        "    self.n = len(self.data)\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self,i):\n",
        "    decide = random.randint(0,self.n-1)\n",
        "    while (decide==i):    \n",
        "      decide = random.randint(0,self.n-1)\n",
        "    return self.data[i][0],self.t(self.data[i][0]),self.data[decide][0],self.t(self.data[decide][0])\n",
        "\n",
        "class Soupdata(Dataset):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    transform = transforms.ToTensor()\n",
        "    self.data = torchvision.datasets.STL10(root = '/',split = 'train',download = True,transform = transform)\n",
        "    self.n = len(self.data)\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self,i):\n",
        "    return self.data[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "fe284e36ed1f4249a871ececae311646",
            "b748098cbc444c6a91de079086c822ab",
            "3601fb5eeefe426ab0a9b729eb9abbf0",
            "48bc5e90bb3c497799dc3185777f2f69",
            "04daaaa4d89745b0be3d5239c3b6d945",
            "6c818a5f9b41479d8b8eb0c6d71a7fff",
            "b3e32058513d437292b6c7c1a75a6fbb",
            "9270895935b143a686ca4766a0d2fced"
          ]
        },
        "id": "F_CGRUCySvql",
        "outputId": "cfdc11d6-689f-4032-cc24-7d2652a31475"
      },
      "source": [
        "batch_size = 128\n",
        "unsoup = Unsoupdata()\n",
        "unsouploader = DataLoader(dataset=unsoup,batch_size = 128,shuffle = True)\n",
        "soup =  Soupdata()\n",
        "train_data,test_val = random_split(soup,[len(soup)-500,500])\n",
        "test_data = torchvision.datasets.STL10(root = '/',split = 'test',download = True,transform = transforms.ToTensor())\n",
        "souptrain = DataLoader(dataset=train_data,batch_size = batch_size,shuffle = True)\n",
        "soupval = DataLoader(dataset=test_val,batch_size = len(test_val),shuffle = True)\n",
        "souptest = DataLoader(dataset=test_data,batch_size = len(test_data),shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to /stl10_binary.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe284e36ed1f4249a871ececae311646",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2640397119.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting /stl10_binary.tar.gz to /\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1a9xI6416Bn"
      },
      "source": [
        "coste = nn.TripletMarginLoss()\n",
        "costc = nn.CrossEntropyLoss()\n",
        "sim = []\n",
        "def loss(intake,t = 1,eps = 1e-8):\n",
        "  siml = nn.CosineSimilarity(dim = 0)\n",
        "  positive = siml(intake[0],intake[1])*siml(intake[2],intake[3])\n",
        "  negative = siml(intake[0],intake[2])+siml(intake[0],intake[3])+siml(intake[1],intake[2])+siml(intake[1],intake[3])\n",
        "  negative = negative**2\n",
        "  positive = torch.exp(positive)/t\n",
        "  negative = torch.exp(negative)/t\n",
        "  los = -1*torch.log(positive/negative)\n",
        "  los = los.mean()\n",
        "  return los"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIinezqgc6Ga"
      },
      "source": [
        "optimizere = torch.optim.Adam(model.e.parameters(),lr = 0.001)\n",
        "optimizerc = torch.optim.Adam(model.c.parameters(),lr = 0.001)\n",
        "optimizerf = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "def test(data):\n",
        "    c = 0\n",
        "    s = 0\n",
        "    for i,(x,y) in enumerate(data):\n",
        "        with torch.no_grad():\n",
        "            x =x.to(device)\n",
        "            y = y.to(device,dtype = torch.int64)\n",
        "            yt = model(x,False,True)\n",
        "            yt = torch.argmax(yt, dim= 1)\n",
        "            c = (y == yt).sum()\n",
        "            s = y.shape[0]\n",
        "        break\n",
        "    return (100*c/s).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kCT1lYNNfD2w",
        "outputId": "542a8d55-6901-4d55-f5ec-1dfdfbfaff59"
      },
      "source": [
        "epochs = 100\n",
        "for j in range(epochs):\n",
        "  for i,(x1,x2,x3,x4) in enumerate(unsouploader) :\n",
        "    x1 = x1.to(device)\n",
        "    x2 = x2.to(device)\n",
        "    x3 = x3.to(device)\n",
        "    x4 = x4.to(device)\n",
        "    y1 = model(x1,True,False)\n",
        "    y2 = model(x2,True,False)\n",
        "    y3 = model(x3,True,False)\n",
        "    y4 = model(x4,True,False)\n",
        "    y1 = torch.unsqueeze(y1,dim = 0)\n",
        "    y2 = torch.unsqueeze(y2,dim = 0)\n",
        "    y3 = torch.unsqueeze(y3,dim = 0)\n",
        "    y4 = torch.unsqueeze(y4,dim = 0)\n",
        "    a = torch.cat([y1,y2,y3,y4],dim = 0)\n",
        "    losse = loss(a)          \n",
        "    optimizere.zero_grad()\n",
        "    losse.backward()\n",
        "    optimizere.step()\n",
        "    print(f'epoch {j+1} step {i} loss {losse}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1 step 0 loss 15.0\n",
            "epoch 1 step 1 loss 14.999998092651367\n",
            "epoch 1 step 2 loss 15.0\n",
            "epoch 1 step 3 loss 14.999999046325684\n",
            "epoch 1 step 4 loss 14.990612983703613\n",
            "epoch 1 step 5 loss 15.0\n",
            "epoch 1 step 6 loss 14.999999046325684\n",
            "epoch 1 step 7 loss 14.999988555908203\n",
            "epoch 1 step 8 loss 14.999225616455078\n",
            "epoch 1 step 9 loss 14.998809814453125\n",
            "epoch 1 step 10 loss 14.949789047241211\n",
            "epoch 1 step 11 loss 14.989103317260742\n",
            "epoch 1 step 12 loss 14.999991416931152\n",
            "epoch 1 step 13 loss 14.999320983886719\n",
            "epoch 1 step 14 loss 14.999996185302734\n",
            "epoch 1 step 15 loss 14.999985694885254\n",
            "epoch 1 step 16 loss 14.99995231628418\n",
            "epoch 1 step 17 loss 14.9996337890625\n",
            "epoch 1 step 18 loss 14.999763488769531\n",
            "epoch 1 step 19 loss 14.999542236328125\n",
            "epoch 1 step 20 loss 14.99697494506836\n",
            "epoch 1 step 21 loss 14.975410461425781\n",
            "epoch 1 step 22 loss 14.979767799377441\n",
            "epoch 1 step 23 loss 14.797109603881836\n",
            "epoch 1 step 24 loss 12.336801528930664\n",
            "epoch 1 step 25 loss 7.178368091583252\n",
            "epoch 1 step 26 loss 4.373058319091797\n",
            "epoch 1 step 27 loss 0.532991886138916\n",
            "epoch 1 step 28 loss -0.4268724024295807\n",
            "epoch 1 step 29 loss -0.48045697808265686\n",
            "epoch 1 step 30 loss -0.9462487101554871\n",
            "epoch 1 step 31 loss -0.962072491645813\n",
            "epoch 1 step 32 loss -0.2736251652240753\n",
            "epoch 1 step 33 loss -0.9837968945503235\n",
            "epoch 1 step 34 loss -0.8995806574821472\n",
            "epoch 1 step 35 loss -0.9701624512672424\n",
            "epoch 1 step 36 loss -0.5883913040161133\n",
            "epoch 1 step 37 loss -0.7615127563476562\n",
            "epoch 1 step 38 loss 0.09245403110980988\n",
            "epoch 1 step 39 loss -0.9687424302101135\n",
            "epoch 1 step 40 loss -0.9463576078414917\n",
            "epoch 1 step 41 loss -0.9588804244995117\n",
            "epoch 1 step 42 loss -0.9199532270431519\n",
            "epoch 1 step 43 loss -0.9041950702667236\n",
            "epoch 1 step 44 loss -0.9843555688858032\n",
            "epoch 1 step 45 loss -0.9924866557121277\n",
            "epoch 1 step 46 loss -0.822118878364563\n",
            "epoch 1 step 47 loss -0.990213930606842\n",
            "epoch 1 step 48 loss -0.9781025648117065\n",
            "epoch 1 step 49 loss -0.9940356016159058\n",
            "epoch 1 step 50 loss -0.9917147159576416\n",
            "epoch 1 step 51 loss -0.902824342250824\n",
            "epoch 1 step 52 loss -0.9640417695045471\n",
            "epoch 1 step 53 loss -0.996077299118042\n",
            "epoch 1 step 54 loss -0.9955608248710632\n",
            "epoch 1 step 55 loss -0.9626877903938293\n",
            "epoch 1 step 56 loss 0.040726594626903534\n",
            "epoch 1 step 57 loss -0.9964116811752319\n",
            "epoch 1 step 58 loss -0.8921892046928406\n",
            "epoch 1 step 59 loss -0.9805850982666016\n",
            "epoch 1 step 60 loss -0.7422494888305664\n",
            "epoch 1 step 61 loss -0.5809112787246704\n",
            "epoch 1 step 62 loss -0.9438673257827759\n",
            "epoch 1 step 63 loss -0.7939624786376953\n",
            "epoch 1 step 64 loss -0.9541800022125244\n",
            "epoch 1 step 65 loss -0.9531083703041077\n",
            "epoch 1 step 66 loss -0.9659268856048584\n",
            "epoch 1 step 67 loss -0.9724103808403015\n",
            "epoch 1 step 68 loss -0.9920007586479187\n",
            "epoch 1 step 69 loss -0.9999922513961792\n",
            "epoch 1 step 70 loss -0.9993101358413696\n",
            "epoch 1 step 71 loss -0.9717270731925964\n",
            "epoch 1 step 72 loss -0.9993483424186707\n",
            "epoch 1 step 73 loss -0.999993622303009\n",
            "epoch 1 step 74 loss -0.8082001209259033\n",
            "epoch 1 step 75 loss -0.998430609703064\n",
            "epoch 1 step 76 loss -0.8212108016014099\n",
            "epoch 1 step 77 loss -0.9952387809753418\n",
            "epoch 1 step 78 loss -0.9520103931427002\n",
            "epoch 1 step 79 loss -0.7010542750358582\n",
            "epoch 1 step 80 loss -0.20786777138710022\n",
            "epoch 1 step 81 loss -0.8057795763015747\n",
            "epoch 1 step 82 loss 0.9031631350517273\n",
            "epoch 1 step 83 loss -0.28373396396636963\n",
            "epoch 1 step 84 loss -0.8523240089416504\n",
            "epoch 1 step 85 loss 0.06826826930046082\n",
            "epoch 1 step 86 loss -0.6173498034477234\n",
            "epoch 1 step 87 loss -0.8093993663787842\n",
            "epoch 1 step 88 loss -0.9834641218185425\n",
            "epoch 1 step 89 loss -0.4567337930202484\n",
            "epoch 1 step 90 loss -0.8985714316368103\n",
            "epoch 1 step 91 loss -0.9657111167907715\n",
            "epoch 1 step 92 loss -0.9894986748695374\n",
            "epoch 1 step 93 loss -0.9943274259567261\n",
            "epoch 1 step 94 loss -0.9817451238632202\n",
            "epoch 1 step 95 loss -0.9992082118988037\n",
            "epoch 1 step 96 loss -0.8979014158248901\n",
            "epoch 1 step 97 loss -0.9502763152122498\n",
            "epoch 1 step 98 loss -0.9962082505226135\n",
            "epoch 1 step 99 loss -0.988517165184021\n",
            "epoch 1 step 100 loss -0.9950829148292542\n",
            "epoch 1 step 101 loss -0.9996411800384521\n",
            "epoch 1 step 102 loss -0.9973787665367126\n",
            "epoch 1 step 103 loss -0.9983773231506348\n",
            "epoch 1 step 104 loss -0.990227997303009\n",
            "epoch 1 step 105 loss -0.9888749122619629\n",
            "epoch 1 step 106 loss -0.9985068440437317\n",
            "epoch 1 step 107 loss -0.9984551668167114\n",
            "epoch 1 step 108 loss -0.9994503855705261\n",
            "epoch 1 step 109 loss -0.5770319700241089\n",
            "epoch 1 step 110 loss -0.9957090616226196\n",
            "epoch 1 step 111 loss -0.9884798526763916\n",
            "epoch 1 step 112 loss -0.9738116264343262\n",
            "epoch 1 step 113 loss -0.8290317058563232\n",
            "epoch 1 step 114 loss -0.9332194328308105\n",
            "epoch 1 step 115 loss -0.9960813522338867\n",
            "epoch 1 step 116 loss -0.9968778491020203\n",
            "epoch 1 step 117 loss -0.167953222990036\n",
            "epoch 1 step 118 loss -0.9227809906005859\n",
            "epoch 1 step 119 loss -0.9741116166114807\n",
            "epoch 1 step 120 loss -0.9964728355407715\n",
            "epoch 1 step 121 loss -0.9620186686515808\n",
            "epoch 1 step 122 loss -0.9930490255355835\n",
            "epoch 1 step 123 loss 5.17080020904541\n",
            "epoch 1 step 124 loss 0.1019505187869072\n",
            "epoch 1 step 125 loss -0.9048810601234436\n",
            "epoch 1 step 126 loss -0.8967894911766052\n",
            "epoch 1 step 127 loss -0.5917512774467468\n",
            "epoch 1 step 128 loss -0.17184913158416748\n",
            "epoch 1 step 129 loss 0.8974981307983398\n",
            "epoch 1 step 130 loss -0.6701723337173462\n",
            "epoch 1 step 131 loss -0.5277361869812012\n",
            "epoch 1 step 132 loss -0.7030817270278931\n",
            "epoch 1 step 133 loss -0.8804873824119568\n",
            "epoch 1 step 134 loss -0.9605557918548584\n",
            "epoch 1 step 135 loss -0.7175571322441101\n",
            "epoch 1 step 136 loss -0.9651076793670654\n",
            "epoch 1 step 137 loss -0.9146148562431335\n",
            "epoch 1 step 138 loss -0.9955084323883057\n",
            "epoch 1 step 139 loss -0.9859038591384888\n",
            "epoch 1 step 140 loss 11.828033447265625\n",
            "epoch 1 step 141 loss 11.733074188232422\n",
            "epoch 1 step 142 loss 7.943502426147461\n",
            "epoch 1 step 143 loss 6.923938751220703\n",
            "epoch 1 step 144 loss 11.33296012878418\n",
            "epoch 1 step 145 loss -0.9717603921890259\n",
            "epoch 1 step 146 loss 4.023334503173828\n",
            "epoch 1 step 147 loss -0.969472348690033\n",
            "epoch 1 step 148 loss -0.10958591103553772\n",
            "epoch 1 step 149 loss 0.2886781394481659\n",
            "epoch 1 step 150 loss 0.20140069723129272\n",
            "epoch 1 step 151 loss 0.04810374975204468\n",
            "epoch 1 step 152 loss 0.022367723286151886\n",
            "epoch 1 step 153 loss 0.0033556923735886812\n",
            "epoch 1 step 154 loss 0.002328809117898345\n",
            "epoch 1 step 155 loss 0.006531992927193642\n",
            "epoch 1 step 156 loss 0.004060480277985334\n",
            "epoch 1 step 157 loss 0.007580566219985485\n",
            "epoch 1 step 158 loss 0.010226133279502392\n",
            "epoch 1 step 159 loss 0.011792341247200966\n",
            "epoch 1 step 160 loss 0.014580799266695976\n",
            "epoch 1 step 161 loss 0.010095865465700626\n",
            "epoch 1 step 162 loss 0.0015061961021274328\n",
            "epoch 1 step 163 loss 0.00401794770732522\n",
            "epoch 1 step 164 loss 0.0024462013971060514\n",
            "epoch 1 step 165 loss 0.0003273345937486738\n",
            "epoch 1 step 166 loss 0.0\n",
            "epoch 1 step 167 loss 0.0\n",
            "epoch 1 step 168 loss 0.0\n",
            "epoch 1 step 169 loss 0.0\n",
            "epoch 1 step 170 loss 0.0\n",
            "epoch 1 step 171 loss 0.0\n",
            "epoch 1 step 172 loss 0.0\n",
            "epoch 1 step 173 loss 0.0\n",
            "epoch 1 step 174 loss 0.0\n",
            "epoch 1 step 175 loss 0.0\n",
            "epoch 1 step 176 loss 0.0\n",
            "epoch 1 step 177 loss 0.0\n",
            "epoch 1 step 178 loss 0.0\n",
            "epoch 1 step 179 loss 0.0\n",
            "epoch 1 step 180 loss 0.0\n",
            "epoch 1 step 181 loss 0.0\n",
            "epoch 1 step 182 loss 0.0\n",
            "epoch 1 step 183 loss 0.0\n",
            "epoch 1 step 184 loss 0.0\n",
            "epoch 1 step 185 loss 0.0\n",
            "epoch 1 step 186 loss 0.0\n",
            "epoch 1 step 187 loss 0.0\n",
            "epoch 1 step 188 loss 0.0\n",
            "epoch 1 step 189 loss 0.0\n",
            "epoch 1 step 190 loss 0.0\n",
            "epoch 1 step 191 loss 0.0\n",
            "epoch 1 step 192 loss 0.0\n",
            "epoch 1 step 193 loss 0.0\n",
            "epoch 1 step 194 loss 0.0\n",
            "epoch 1 step 195 loss 0.0\n",
            "epoch 1 step 196 loss 0.0\n",
            "epoch 1 step 197 loss 0.0\n",
            "epoch 1 step 198 loss 0.0\n",
            "epoch 1 step 199 loss 0.0\n",
            "epoch 1 step 200 loss 0.0\n",
            "epoch 1 step 201 loss 0.0\n",
            "epoch 1 step 202 loss 0.0\n",
            "epoch 1 step 203 loss 0.0\n",
            "epoch 1 step 204 loss 0.0\n",
            "epoch 1 step 205 loss 0.0\n",
            "epoch 1 step 206 loss 0.0\n",
            "epoch 1 step 207 loss 0.0\n",
            "epoch 1 step 208 loss 0.0\n",
            "epoch 1 step 209 loss 0.0\n",
            "epoch 1 step 210 loss 0.0\n",
            "epoch 1 step 211 loss 0.0\n",
            "epoch 1 step 212 loss 0.0\n",
            "epoch 1 step 213 loss 0.0\n",
            "epoch 1 step 214 loss 0.0\n",
            "epoch 1 step 215 loss 0.0\n",
            "epoch 1 step 216 loss 0.0\n",
            "epoch 1 step 217 loss 0.0\n",
            "epoch 1 step 218 loss 0.0\n",
            "epoch 1 step 219 loss 0.0\n",
            "epoch 1 step 220 loss 0.0\n",
            "epoch 1 step 221 loss 0.0\n",
            "epoch 1 step 222 loss 0.0\n",
            "epoch 1 step 223 loss 0.0\n",
            "epoch 1 step 224 loss 0.0\n",
            "epoch 1 step 225 loss 0.0\n",
            "epoch 1 step 226 loss 0.0\n",
            "epoch 1 step 227 loss 0.0\n",
            "epoch 1 step 228 loss 0.0\n",
            "epoch 1 step 229 loss 0.0\n",
            "epoch 1 step 230 loss 0.0\n",
            "epoch 1 step 231 loss 0.0\n",
            "epoch 1 step 232 loss 0.0\n",
            "epoch 1 step 233 loss 0.0\n",
            "epoch 1 step 234 loss 0.0\n",
            "epoch 1 step 235 loss 0.0\n",
            "epoch 1 step 236 loss 0.0\n",
            "epoch 1 step 237 loss 0.0\n",
            "epoch 1 step 238 loss 0.0\n",
            "epoch 1 step 239 loss 0.0\n",
            "epoch 1 step 240 loss 0.0\n",
            "epoch 1 step 241 loss 0.0\n",
            "epoch 1 step 242 loss 0.0\n",
            "epoch 1 step 243 loss 0.0\n",
            "epoch 1 step 244 loss 0.0\n",
            "epoch 1 step 245 loss 0.0\n",
            "epoch 1 step 246 loss 0.0\n",
            "epoch 1 step 247 loss 0.0\n",
            "epoch 1 step 248 loss 0.0\n",
            "epoch 1 step 249 loss 0.0\n",
            "epoch 1 step 250 loss 0.0\n",
            "epoch 1 step 251 loss 0.0\n",
            "epoch 1 step 252 loss 0.0\n",
            "epoch 1 step 253 loss 0.0\n",
            "epoch 1 step 254 loss 0.0\n",
            "epoch 1 step 255 loss 0.0\n",
            "epoch 1 step 256 loss 0.0\n",
            "epoch 1 step 257 loss 0.0\n",
            "epoch 1 step 258 loss 0.0\n",
            "epoch 1 step 259 loss 0.0\n",
            "epoch 1 step 260 loss 0.0\n",
            "epoch 1 step 261 loss 0.0\n",
            "epoch 1 step 262 loss 0.0\n",
            "epoch 1 step 263 loss 0.0\n",
            "epoch 1 step 264 loss 0.0\n",
            "epoch 1 step 265 loss 0.0\n",
            "epoch 1 step 266 loss 0.0\n",
            "epoch 1 step 267 loss 0.0\n",
            "epoch 1 step 268 loss 0.0\n",
            "epoch 1 step 269 loss 0.0\n",
            "epoch 1 step 270 loss 0.0\n",
            "epoch 1 step 271 loss 0.0\n",
            "epoch 1 step 272 loss 0.0\n",
            "epoch 1 step 273 loss 0.0\n",
            "epoch 1 step 274 loss 0.0\n",
            "epoch 1 step 275 loss 0.0\n",
            "epoch 1 step 276 loss 0.0\n",
            "epoch 1 step 277 loss 0.0\n",
            "epoch 1 step 278 loss 0.0\n",
            "epoch 1 step 279 loss 0.0\n",
            "epoch 1 step 280 loss 0.0\n",
            "epoch 1 step 281 loss 0.0\n",
            "epoch 1 step 282 loss 0.0\n",
            "epoch 1 step 283 loss 0.0\n",
            "epoch 1 step 284 loss 0.0\n",
            "epoch 1 step 285 loss 0.0\n",
            "epoch 1 step 286 loss 0.0\n",
            "epoch 1 step 287 loss 0.0\n",
            "epoch 1 step 288 loss 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-de36777918c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsouploader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9c13c7211f35>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdecide\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mdecide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecide\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecide\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSoupdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jOQLMXUdM4zV",
        "outputId": "61273794-eaeb-4c7e-ccf8-1893e7a3bfc7"
      },
      "source": [
        "epochs = 40\n",
        "for j in range(epochs):\n",
        "   for i,(xt,yt) in enumerate(souptrain) :\n",
        "    xt = xt.to(device)\n",
        "    #xt = torch.reshape(xt,[xt.shape[0],-1,1,1])\n",
        "    yt = yt.to(device)\n",
        "    #xt = torch.squeeze(xt,dim = 2)\n",
        "    #xt = torch.squeeze(xt,dim = 2)\n",
        "    y_pred = model(xt,False,True)\n",
        "    y_pred = torch.squeeze(y_pred)\n",
        "    lossc = costc(y_pred,yt)          \n",
        "    optimizerc.zero_grad()\n",
        "    optimizere.zero_grad()\n",
        "    lossc.backward()\n",
        "    optimizerc.step()\n",
        "    if (i%20==0):\n",
        "      acc = test(soupval)\n",
        "      print(f'epoch {j+1} step {i} loss {lossc} test_accuracy {acc} train_accuracy {test(souptrain)}')\n",
        "    else:\n",
        "      print(f'epoch {j+1} step {i} loss {lossc}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 step 0 loss 2.312713384628296 test_accuracy 11.200000762939453 train_accuracy 8.59375\n",
            "epoch 1 step 1 loss 2.359588384628296\n",
            "epoch 1 step 2 loss 2.359588384628296\n",
            "epoch 1 step 3 loss 2.375213384628296\n",
            "epoch 1 step 4 loss 2.383025884628296\n",
            "epoch 1 step 5 loss 2.375213384628296\n",
            "epoch 1 step 6 loss 2.328338384628296\n",
            "epoch 1 step 7 loss 2.343963384628296\n",
            "epoch 1 step 8 loss 2.406463384628296\n",
            "epoch 1 step 9 loss 2.328338384628296\n",
            "epoch 1 step 10 loss 2.343963384628296\n",
            "epoch 1 step 11 loss 2.406463384628296\n",
            "epoch 1 step 12 loss 2.359588384628296\n",
            "epoch 1 step 13 loss 2.383025884628296\n",
            "epoch 1 step 14 loss 2.390838384628296\n",
            "epoch 1 step 15 loss 2.328338384628296\n",
            "epoch 1 step 16 loss 2.343963384628296\n",
            "epoch 1 step 17 loss 2.406463384628296\n",
            "epoch 1 step 18 loss 2.383025884628296\n",
            "epoch 1 step 19 loss 2.375213384628296\n",
            "epoch 1 step 20 loss 2.351775884628296 test_accuracy 11.200000762939453 train_accuracy 10.15625\n",
            "epoch 1 step 21 loss 2.343963384628296\n",
            "epoch 1 step 22 loss 2.383025884628296\n",
            "epoch 1 step 23 loss 2.328338384628296\n",
            "epoch 1 step 24 loss 2.359588384628296\n",
            "epoch 1 step 25 loss 2.343963384628296\n",
            "epoch 1 step 26 loss 2.359588384628296\n",
            "epoch 1 step 27 loss 2.367400884628296\n",
            "epoch 1 step 28 loss 2.390838384628296\n",
            "epoch 1 step 29 loss 2.336150884628296\n",
            "epoch 1 step 30 loss 2.312713384628296\n",
            "epoch 1 step 31 loss 2.383025884628296\n",
            "epoch 1 step 32 loss 2.359588384628296\n",
            "epoch 1 step 33 loss 2.414275884628296\n",
            "epoch 1 step 34 loss 2.343963384628296\n",
            "epoch 1 step 35 loss 2.26115083694458\n",
            "epoch 2 step 0 loss 2.398650884628296 test_accuracy 11.200000762939453 train_accuracy 7.03125\n",
            "epoch 2 step 1 loss 2.336150884628296\n",
            "epoch 2 step 2 loss 2.375213384628296\n",
            "epoch 2 step 3 loss 2.328338384628296\n",
            "epoch 2 step 4 loss 2.375213384628296\n",
            "epoch 2 step 5 loss 2.375213384628296\n",
            "epoch 2 step 6 loss 2.383025884628296\n",
            "epoch 2 step 7 loss 2.390838384628296\n",
            "epoch 2 step 8 loss 2.367400884628296\n",
            "epoch 2 step 9 loss 2.343963384628296\n",
            "epoch 2 step 10 loss 2.359588384628296\n",
            "epoch 2 step 11 loss 2.343963384628296\n",
            "epoch 2 step 12 loss 2.375213384628296\n",
            "epoch 2 step 13 loss 2.383025884628296\n",
            "epoch 2 step 14 loss 2.336150884628296\n",
            "epoch 2 step 15 loss 2.351775884628296\n",
            "epoch 2 step 16 loss 2.343963384628296\n",
            "epoch 2 step 17 loss 2.383025884628296\n",
            "epoch 2 step 18 loss 2.351775884628296\n",
            "epoch 2 step 19 loss 2.359588384628296\n",
            "epoch 2 step 20 loss 2.336150884628296 test_accuracy 11.200000762939453 train_accuracy 14.0625\n",
            "epoch 2 step 21 loss 2.375213384628296\n",
            "epoch 2 step 22 loss 2.390838384628296\n",
            "epoch 2 step 23 loss 2.367400884628296\n",
            "epoch 2 step 24 loss 2.359588384628296\n",
            "epoch 2 step 25 loss 2.383025884628296\n",
            "epoch 2 step 26 loss 2.336150884628296\n",
            "epoch 2 step 27 loss 2.320525884628296\n",
            "epoch 2 step 28 loss 2.351775884628296\n",
            "epoch 2 step 29 loss 2.328338384628296\n",
            "epoch 2 step 30 loss 2.351775884628296\n",
            "epoch 2 step 31 loss 2.375213384628296\n",
            "epoch 2 step 32 loss 2.383025884628296\n",
            "epoch 2 step 33 loss 2.367400884628296\n",
            "epoch 2 step 34 loss 2.359588384628296\n",
            "epoch 2 step 35 loss 2.4111506938934326\n",
            "epoch 3 step 0 loss 2.383025884628296 test_accuracy 11.200000762939453 train_accuracy 9.375\n",
            "epoch 3 step 1 loss 2.320525884628296\n",
            "epoch 3 step 2 loss 2.343963384628296\n",
            "epoch 3 step 3 loss 2.367400884628296\n",
            "epoch 3 step 4 loss 2.304900884628296\n",
            "epoch 3 step 5 loss 2.351775884628296\n",
            "epoch 3 step 6 loss 2.398650884628296\n",
            "epoch 3 step 7 loss 2.383025884628296\n",
            "epoch 3 step 8 loss 2.375213384628296\n",
            "epoch 3 step 9 loss 2.383025884628296\n",
            "epoch 3 step 10 loss 2.390838384628296\n",
            "epoch 3 step 11 loss 2.328338384628296\n",
            "epoch 3 step 12 loss 2.359588384628296\n",
            "epoch 3 step 13 loss 2.367400884628296\n",
            "epoch 3 step 14 loss 2.390838384628296\n",
            "epoch 3 step 15 loss 2.367400884628296\n",
            "epoch 3 step 16 loss 2.359588384628296\n",
            "epoch 3 step 17 loss 2.359588384628296\n",
            "epoch 3 step 18 loss 2.383025884628296\n",
            "epoch 3 step 19 loss 2.367400884628296\n",
            "epoch 3 step 20 loss 2.336150884628296 test_accuracy 11.200000762939453 train_accuracy 14.0625\n",
            "epoch 3 step 21 loss 2.367400884628296\n",
            "epoch 3 step 22 loss 2.375213384628296\n",
            "epoch 3 step 23 loss 2.336150884628296\n",
            "epoch 3 step 24 loss 2.328338384628296\n",
            "epoch 3 step 25 loss 2.398650884628296\n",
            "epoch 3 step 26 loss 2.367400884628296\n",
            "epoch 3 step 27 loss 2.351775884628296\n",
            "epoch 3 step 28 loss 2.375213384628296\n",
            "epoch 3 step 29 loss 2.375213384628296\n",
            "epoch 3 step 30 loss 2.343963384628296\n",
            "epoch 3 step 31 loss 2.328338384628296\n",
            "epoch 3 step 32 loss 2.359588384628296\n",
            "epoch 3 step 33 loss 2.351775884628296\n",
            "epoch 3 step 34 loss 2.367400884628296\n",
            "epoch 3 step 35 loss 2.4111506938934326\n",
            "epoch 4 step 0 loss 2.328338384628296 test_accuracy 11.200000762939453 train_accuracy 6.25\n",
            "epoch 4 step 1 loss 2.383025884628296\n",
            "epoch 4 step 2 loss 2.359588384628296\n",
            "epoch 4 step 3 loss 2.343963384628296\n",
            "epoch 4 step 4 loss 2.359588384628296\n",
            "epoch 4 step 5 loss 2.351775884628296\n",
            "epoch 4 step 6 loss 2.343963384628296\n",
            "epoch 4 step 7 loss 2.343963384628296\n",
            "epoch 4 step 8 loss 2.367400884628296\n",
            "epoch 4 step 9 loss 2.367400884628296\n",
            "epoch 4 step 10 loss 2.406463384628296\n",
            "epoch 4 step 11 loss 2.351775884628296\n",
            "epoch 4 step 12 loss 2.375213384628296\n",
            "epoch 4 step 13 loss 2.328338384628296\n",
            "epoch 4 step 14 loss 2.343963384628296\n",
            "epoch 4 step 15 loss 2.336150884628296\n",
            "epoch 4 step 16 loss 2.390838384628296\n",
            "epoch 4 step 17 loss 2.343963384628296\n",
            "epoch 4 step 18 loss 2.336150884628296\n",
            "epoch 4 step 19 loss 2.359588384628296\n",
            "epoch 4 step 20 loss 2.343963384628296 test_accuracy 11.200000762939453 train_accuracy 9.375\n",
            "epoch 4 step 21 loss 2.367400884628296\n",
            "epoch 4 step 22 loss 2.343963384628296\n",
            "epoch 4 step 23 loss 2.343963384628296\n",
            "epoch 4 step 24 loss 2.351775884628296\n",
            "epoch 4 step 25 loss 2.383025884628296\n",
            "epoch 4 step 26 loss 2.390838384628296\n",
            "epoch 4 step 27 loss 2.359588384628296\n",
            "epoch 4 step 28 loss 2.367400884628296\n",
            "epoch 4 step 29 loss 2.406463384628296\n",
            "epoch 4 step 30 loss 2.351775884628296\n",
            "epoch 4 step 31 loss 2.375213384628296\n",
            "epoch 4 step 32 loss 2.406463384628296\n",
            "epoch 4 step 33 loss 2.383025884628296\n",
            "epoch 4 step 34 loss 2.359588384628296\n",
            "epoch 4 step 35 loss 2.3611507415771484\n",
            "epoch 5 step 0 loss 2.351775884628296 test_accuracy 11.200000762939453 train_accuracy 13.28125\n",
            "epoch 5 step 1 loss 2.359588384628296\n",
            "epoch 5 step 2 loss 2.383025884628296\n",
            "epoch 5 step 3 loss 2.375213384628296\n",
            "epoch 5 step 4 loss 2.398650884628296\n",
            "epoch 5 step 5 loss 2.383025884628296\n",
            "epoch 5 step 6 loss 2.375213384628296\n",
            "epoch 5 step 7 loss 2.351775884628296\n",
            "epoch 5 step 8 loss 2.359588384628296\n",
            "epoch 5 step 9 loss 2.351775884628296\n",
            "epoch 5 step 10 loss 2.375213384628296\n",
            "epoch 5 step 11 loss 2.351775884628296\n",
            "epoch 5 step 12 loss 2.359588384628296\n",
            "epoch 5 step 13 loss 2.304900884628296\n",
            "epoch 5 step 14 loss 2.336150884628296\n",
            "epoch 5 step 15 loss 2.304900884628296\n",
            "epoch 5 step 16 loss 2.359588384628296\n",
            "epoch 5 step 17 loss 2.367400884628296\n",
            "epoch 5 step 18 loss 2.351775884628296\n",
            "epoch 5 step 19 loss 2.359588384628296\n",
            "epoch 5 step 20 loss 2.383025884628296 test_accuracy 11.200000762939453 train_accuracy 6.25\n",
            "epoch 5 step 21 loss 2.359588384628296\n",
            "epoch 5 step 22 loss 2.367400884628296\n",
            "epoch 5 step 23 loss 2.328338384628296\n",
            "epoch 5 step 24 loss 2.367400884628296\n",
            "epoch 5 step 25 loss 2.390838384628296\n",
            "epoch 5 step 26 loss 2.336150884628296\n",
            "epoch 5 step 27 loss 2.359588384628296\n",
            "epoch 5 step 28 loss 2.351775884628296\n",
            "epoch 5 step 29 loss 2.367400884628296\n",
            "epoch 5 step 30 loss 2.343963384628296\n",
            "epoch 5 step 31 loss 2.406463384628296\n",
            "epoch 5 step 32 loss 2.390838384628296\n",
            "epoch 5 step 33 loss 2.367400884628296\n",
            "epoch 5 step 34 loss 2.375213384628296\n",
            "epoch 5 step 35 loss 2.3611507415771484\n",
            "epoch 6 step 0 loss 2.359588384628296 test_accuracy 11.200000762939453 train_accuracy 13.28125\n",
            "epoch 6 step 1 loss 2.359588384628296\n",
            "epoch 6 step 2 loss 2.320525884628296\n",
            "epoch 6 step 3 loss 2.359588384628296\n",
            "epoch 6 step 4 loss 2.336150884628296\n",
            "epoch 6 step 5 loss 2.359588384628296\n",
            "epoch 6 step 6 loss 2.367400884628296\n",
            "epoch 6 step 7 loss 2.390838384628296\n",
            "epoch 6 step 8 loss 2.390838384628296\n",
            "epoch 6 step 9 loss 2.343963384628296\n",
            "epoch 6 step 10 loss 2.367400884628296\n",
            "epoch 6 step 11 loss 2.390838384628296\n",
            "epoch 6 step 12 loss 2.398650884628296\n",
            "epoch 6 step 13 loss 2.367400884628296\n",
            "epoch 6 step 14 loss 2.375213384628296\n",
            "epoch 6 step 15 loss 2.328338384628296\n",
            "epoch 6 step 16 loss 2.336150884628296\n",
            "epoch 6 step 17 loss 2.351775884628296\n",
            "epoch 6 step 18 loss 2.359588384628296\n",
            "epoch 6 step 19 loss 2.328338384628296\n",
            "epoch 6 step 20 loss 2.383025884628296 test_accuracy 11.200000762939453 train_accuracy 8.59375\n",
            "epoch 6 step 21 loss 2.304900884628296\n",
            "epoch 6 step 22 loss 2.406463384628296\n",
            "epoch 6 step 23 loss 2.336150884628296\n",
            "epoch 6 step 24 loss 2.375213384628296\n",
            "epoch 6 step 25 loss 2.343963384628296\n",
            "epoch 6 step 26 loss 2.390838384628296\n",
            "epoch 6 step 27 loss 2.359588384628296\n",
            "epoch 6 step 28 loss 2.359588384628296\n",
            "epoch 6 step 29 loss 2.297088384628296\n",
            "epoch 6 step 30 loss 2.312713384628296\n",
            "epoch 6 step 31 loss 2.414275884628296\n",
            "epoch 6 step 32 loss 2.390838384628296\n",
            "epoch 6 step 33 loss 2.390838384628296\n",
            "epoch 6 step 34 loss 2.383025884628296\n",
            "epoch 6 step 35 loss 2.461150646209717\n",
            "epoch 7 step 0 loss 2.336150884628296 test_accuracy 11.200000762939453 train_accuracy 6.25\n",
            "epoch 7 step 1 loss 2.351775884628296\n",
            "epoch 7 step 2 loss 2.336150884628296\n",
            "epoch 7 step 3 loss 2.375213384628296\n",
            "epoch 7 step 4 loss 2.336150884628296\n",
            "epoch 7 step 5 loss 2.336150884628296\n",
            "epoch 7 step 6 loss 2.383025884628296\n",
            "epoch 7 step 7 loss 2.383025884628296\n",
            "epoch 7 step 8 loss 2.359588384628296\n",
            "epoch 7 step 9 loss 2.336150884628296\n",
            "epoch 7 step 10 loss 2.390838384628296\n",
            "epoch 7 step 11 loss 2.351775884628296\n",
            "epoch 7 step 12 loss 2.328338384628296\n",
            "epoch 7 step 13 loss 2.390838384628296\n",
            "epoch 7 step 14 loss 2.367400884628296\n",
            "epoch 7 step 15 loss 2.359588384628296\n",
            "epoch 7 step 16 loss 2.328338384628296\n",
            "epoch 7 step 17 loss 2.328338384628296\n",
            "epoch 7 step 18 loss 2.398650884628296\n",
            "epoch 7 step 19 loss 2.398650884628296\n",
            "epoch 7 step 20 loss 2.297088384628296 test_accuracy 11.200000762939453 train_accuracy 5.46875\n",
            "epoch 7 step 21 loss 2.343963384628296\n",
            "epoch 7 step 22 loss 2.398650884628296\n",
            "epoch 7 step 23 loss 2.383025884628296\n",
            "epoch 7 step 24 loss 2.406463384628296\n",
            "epoch 7 step 25 loss 2.359588384628296\n",
            "epoch 7 step 26 loss 2.367400884628296\n",
            "epoch 7 step 27 loss 2.351775884628296\n",
            "epoch 7 step 28 loss 2.320525884628296\n",
            "epoch 7 step 29 loss 2.351775884628296\n",
            "epoch 7 step 30 loss 2.375213384628296\n",
            "epoch 7 step 31 loss 2.383025884628296\n",
            "epoch 7 step 32 loss 2.375213384628296\n",
            "epoch 7 step 33 loss 2.359588384628296\n",
            "epoch 7 step 34 loss 2.406463384628296\n",
            "epoch 7 step 35 loss 2.3611507415771484\n",
            "epoch 8 step 0 loss 2.398650884628296 test_accuracy 11.200000762939453 train_accuracy 10.15625\n",
            "epoch 8 step 1 loss 2.390838384628296\n",
            "epoch 8 step 2 loss 2.383025884628296\n",
            "epoch 8 step 3 loss 2.336150884628296\n",
            "epoch 8 step 4 loss 2.383025884628296\n",
            "epoch 8 step 5 loss 2.406463384628296\n",
            "epoch 8 step 6 loss 2.398650884628296\n",
            "epoch 8 step 7 loss 2.383025884628296\n",
            "epoch 8 step 8 loss 2.398650884628296\n",
            "epoch 8 step 9 loss 2.390838384628296\n",
            "epoch 8 step 10 loss 2.359588384628296\n",
            "epoch 8 step 11 loss 2.359588384628296\n",
            "epoch 8 step 12 loss 2.383025884628296\n",
            "epoch 8 step 13 loss 2.359588384628296\n",
            "epoch 8 step 14 loss 2.383025884628296\n",
            "epoch 8 step 15 loss 2.367400884628296\n",
            "epoch 8 step 16 loss 2.328338384628296\n",
            "epoch 8 step 17 loss 2.328338384628296\n",
            "epoch 8 step 18 loss 2.351775884628296\n",
            "epoch 8 step 19 loss 2.297088384628296\n",
            "epoch 8 step 20 loss 2.367400884628296 test_accuracy 11.200000762939453 train_accuracy 7.03125\n",
            "epoch 8 step 21 loss 2.297088384628296\n",
            "epoch 8 step 22 loss 2.320525884628296\n",
            "epoch 8 step 23 loss 2.312713384628296\n",
            "epoch 8 step 24 loss 2.336150884628296\n",
            "epoch 8 step 25 loss 2.367400884628296\n",
            "epoch 8 step 26 loss 2.343963384628296\n",
            "epoch 8 step 27 loss 2.406463384628296\n",
            "epoch 8 step 28 loss 2.312713384628296\n",
            "epoch 8 step 29 loss 2.367400884628296\n",
            "epoch 8 step 30 loss 2.390838384628296\n",
            "epoch 8 step 31 loss 2.343963384628296\n",
            "epoch 8 step 32 loss 2.375213384628296\n",
            "epoch 8 step 33 loss 2.367400884628296\n",
            "epoch 8 step 34 loss 2.351775884628296\n",
            "epoch 8 step 35 loss 2.4111506938934326\n",
            "epoch 9 step 0 loss 2.367400884628296 test_accuracy 11.200000762939453 train_accuracy 10.9375\n",
            "epoch 9 step 1 loss 2.328338384628296\n",
            "epoch 9 step 2 loss 2.351775884628296\n",
            "epoch 9 step 3 loss 2.367400884628296\n",
            "epoch 9 step 4 loss 2.351775884628296\n",
            "epoch 9 step 5 loss 2.398650884628296\n",
            "epoch 9 step 6 loss 2.398650884628296\n",
            "epoch 9 step 7 loss 2.351775884628296\n",
            "epoch 9 step 8 loss 2.336150884628296\n",
            "epoch 9 step 9 loss 2.367400884628296\n",
            "epoch 9 step 10 loss 2.383025884628296\n",
            "epoch 9 step 11 loss 2.383025884628296\n",
            "epoch 9 step 12 loss 2.367400884628296\n",
            "epoch 9 step 13 loss 2.336150884628296\n",
            "epoch 9 step 14 loss 2.359588384628296\n",
            "epoch 9 step 15 loss 2.359588384628296\n",
            "epoch 9 step 16 loss 2.312713384628296\n",
            "epoch 9 step 17 loss 2.375213384628296\n",
            "epoch 9 step 18 loss 2.359588384628296\n",
            "epoch 9 step 19 loss 2.312713384628296\n",
            "epoch 9 step 20 loss 2.414275884628296 test_accuracy 11.200000762939453 train_accuracy 9.375\n",
            "epoch 9 step 21 loss 2.359588384628296\n",
            "epoch 9 step 22 loss 2.336150884628296\n",
            "epoch 9 step 23 loss 2.375213384628296\n",
            "epoch 9 step 24 loss 2.359588384628296\n",
            "epoch 9 step 25 loss 2.359588384628296\n",
            "epoch 9 step 26 loss 2.383025884628296\n",
            "epoch 9 step 27 loss 2.351775884628296\n",
            "epoch 9 step 28 loss 2.383025884628296\n",
            "epoch 9 step 29 loss 2.367400884628296\n",
            "epoch 9 step 30 loss 2.312713384628296\n",
            "epoch 9 step 31 loss 2.328338384628296\n",
            "epoch 9 step 32 loss 2.398650884628296\n",
            "epoch 9 step 33 loss 2.414275884628296\n",
            "epoch 9 step 34 loss 2.351775884628296\n",
            "epoch 9 step 35 loss 2.3111507892608643\n",
            "epoch 10 step 0 loss 2.336150884628296 test_accuracy 11.200000762939453 train_accuracy 8.59375\n",
            "epoch 10 step 1 loss 2.375213384628296\n",
            "epoch 10 step 2 loss 2.375213384628296\n",
            "epoch 10 step 3 loss 2.406463384628296\n",
            "epoch 10 step 4 loss 2.367400884628296\n",
            "epoch 10 step 5 loss 2.328338384628296\n",
            "epoch 10 step 6 loss 2.336150884628296\n",
            "epoch 10 step 7 loss 2.375213384628296\n",
            "epoch 10 step 8 loss 2.383025884628296\n",
            "epoch 10 step 9 loss 2.375213384628296\n",
            "epoch 10 step 10 loss 2.383025884628296\n",
            "epoch 10 step 11 loss 2.351775884628296\n",
            "epoch 10 step 12 loss 2.320525884628296\n",
            "epoch 10 step 13 loss 2.359588384628296\n",
            "epoch 10 step 14 loss 2.351775884628296\n",
            "epoch 10 step 15 loss 2.375213384628296\n",
            "epoch 10 step 16 loss 2.312713384628296\n",
            "epoch 10 step 17 loss 2.359588384628296\n",
            "epoch 10 step 18 loss 2.351775884628296\n",
            "epoch 10 step 19 loss 2.343963384628296\n",
            "epoch 10 step 20 loss 2.351775884628296 test_accuracy 11.200000762939453 train_accuracy 8.59375\n",
            "epoch 10 step 21 loss 2.312713384628296\n",
            "epoch 10 step 22 loss 2.351775884628296\n",
            "epoch 10 step 23 loss 2.375213384628296\n",
            "epoch 10 step 24 loss 2.383025884628296\n",
            "epoch 10 step 25 loss 2.328338384628296\n",
            "epoch 10 step 26 loss 2.359588384628296\n",
            "epoch 10 step 27 loss 2.406463384628296\n",
            "epoch 10 step 28 loss 2.367400884628296\n",
            "epoch 10 step 29 loss 2.390838384628296\n",
            "epoch 10 step 30 loss 2.367400884628296\n",
            "epoch 10 step 31 loss 2.375213384628296\n",
            "epoch 10 step 32 loss 2.351775884628296\n",
            "epoch 10 step 33 loss 2.351775884628296\n",
            "epoch 10 step 34 loss 2.414275884628296\n",
            "epoch 10 step 35 loss 2.3611507415771484\n",
            "epoch 11 step 0 loss 2.343963384628296 test_accuracy 11.200000762939453 train_accuracy 7.8125\n",
            "epoch 11 step 1 loss 2.359588384628296\n",
            "epoch 11 step 2 loss 2.367400884628296\n",
            "epoch 11 step 3 loss 2.320525884628296\n",
            "epoch 11 step 4 loss 2.351775884628296\n",
            "epoch 11 step 5 loss 2.351775884628296\n",
            "epoch 11 step 6 loss 2.367400884628296\n",
            "epoch 11 step 7 loss 2.406463384628296\n",
            "epoch 11 step 8 loss 2.351775884628296\n",
            "epoch 11 step 9 loss 2.351775884628296\n",
            "epoch 11 step 10 loss 2.390838384628296\n",
            "epoch 11 step 11 loss 2.367400884628296\n",
            "epoch 11 step 12 loss 2.383025884628296\n",
            "epoch 11 step 13 loss 2.390838384628296\n",
            "epoch 11 step 14 loss 2.367400884628296\n",
            "epoch 11 step 15 loss 2.383025884628296\n",
            "epoch 11 step 16 loss 2.398650884628296\n",
            "epoch 11 step 17 loss 2.390838384628296\n",
            "epoch 11 step 18 loss 2.351775884628296\n",
            "epoch 11 step 19 loss 2.375213384628296\n",
            "epoch 11 step 20 loss 2.375213384628296 test_accuracy 11.200000762939453 train_accuracy 8.59375\n",
            "epoch 11 step 21 loss 2.351775884628296\n",
            "epoch 11 step 22 loss 2.375213384628296\n",
            "epoch 11 step 23 loss 2.383025884628296\n",
            "epoch 11 step 24 loss 2.359588384628296\n",
            "epoch 11 step 25 loss 2.351775884628296\n",
            "epoch 11 step 26 loss 2.359588384628296\n",
            "epoch 11 step 27 loss 2.336150884628296\n",
            "epoch 11 step 28 loss 2.351775884628296\n",
            "epoch 11 step 29 loss 2.336150884628296\n",
            "epoch 11 step 30 loss 2.328338384628296\n",
            "epoch 11 step 31 loss 2.351775884628296\n",
            "epoch 11 step 32 loss 2.336150884628296\n",
            "epoch 11 step 33 loss 2.367400884628296\n",
            "epoch 11 step 34 loss 2.336150884628296\n",
            "epoch 11 step 35 loss 2.26115083694458\n",
            "epoch 12 step 0 loss 2.351775884628296 test_accuracy 11.200000762939453 train_accuracy 16.40625\n",
            "epoch 12 step 1 loss 2.336150884628296\n",
            "epoch 12 step 2 loss 2.406463384628296\n",
            "epoch 12 step 3 loss 2.414275884628296\n",
            "epoch 12 step 4 loss 2.336150884628296\n",
            "epoch 12 step 5 loss 2.328338384628296\n",
            "epoch 12 step 6 loss 2.359588384628296\n",
            "epoch 12 step 7 loss 2.351775884628296\n",
            "epoch 12 step 8 loss 2.398650884628296\n",
            "epoch 12 step 9 loss 2.375213384628296\n",
            "epoch 12 step 10 loss 2.359588384628296\n",
            "epoch 12 step 11 loss 2.343963384628296\n",
            "epoch 12 step 12 loss 2.359588384628296\n",
            "epoch 12 step 13 loss 2.375213384628296\n",
            "epoch 12 step 14 loss 2.383025884628296\n",
            "epoch 12 step 15 loss 2.422088384628296\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-347dde3d3ab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch {j+1} step {i} loss {lossc} test_accuracy {acc} train_accuracy {test(souptrain)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch {j+1} step {i} loss {lossc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "zbzN9Iu9rph5",
        "outputId": "b1284033-a052-47bf-97e8-1c84a9fab991"
      },
      "source": [
        "test(souptest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9c442b4c06d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msouptest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-00eebef65798>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-319df450a3c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encode, classify)\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-319df450a3c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, flag)\u001b[0m\n\u001b[1;32m     32\u001b[0m                               )\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 8.79 GiB (GPU 0; 11.17 GiB total capacity; 1.20 GiB already allocated; 7.86 GiB free; 2.84 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    }
  ]
}