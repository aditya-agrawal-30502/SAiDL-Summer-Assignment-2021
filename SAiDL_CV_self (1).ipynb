{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SAiDL_CV_self.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "82782acb0c6140daa839d58ccb8201b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f76563354e2f4def95fa70300d693899",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_19662acf7b5a42b18afe6baaa64ccdfb",
              "IPY_MODEL_d2792869320e43fd946c9e89c5922ca0",
              "IPY_MODEL_736bd8907ef04f04951db2a2aa695293"
            ]
          }
        },
        "f76563354e2f4def95fa70300d693899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19662acf7b5a42b18afe6baaa64ccdfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_38f9af274a7d4644a643c56d277cbbf9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b764e628f59f4ab8bb56285cbfcac1da"
          }
        },
        "d2792869320e43fd946c9e89c5922ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e761a7bc62954233a4c62deb2197c39c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2640397119,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2640397119,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d01e9de09ff4417796620d4101e44463"
          }
        },
        "736bd8907ef04f04951db2a2aa695293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9f18fc447bbc4037936722650e4e0071",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2640397312/? [01:58&lt;00:00, 16048348.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ba59faecf3c42dfa910c582e7769a35"
          }
        },
        "38f9af274a7d4644a643c56d277cbbf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b764e628f59f4ab8bb56285cbfcac1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e761a7bc62954233a4c62deb2197c39c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d01e9de09ff4417796620d4101e44463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f18fc447bbc4037936722650e4e0071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ba59faecf3c42dfa910c582e7769a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM9KsArYy-Yw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as f\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader,Dataset,random_split\n",
        "import PIL\n",
        "import random\n",
        "if (torch.cuda.is_available()):\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZWNFzvKiPmt"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):   \n",
        "        super().__init__()\n",
        "        self.l1 = nn.Conv2d(3,32,3,padding = 1)\n",
        "        self.n1 = nn.BatchNorm2d(32)\n",
        "        self.l2 = nn.Conv2d(32,32,3,padding = 1)\n",
        "        self.n2 = nn.BatchNorm2d(32)\n",
        "        self.l3 = nn.Conv2d(32,64,3,padding = 1)\n",
        "        self.n3 = nn.BatchNorm2d(64)\n",
        "        self.l4 = nn.Conv2d(64,64,3,padding = 1)\n",
        "        self.n4 = nn.BatchNorm2d(64)\n",
        "        self.l5 = nn.Conv2d(64,128,3,padding = 1)\n",
        "        self.n5 = nn.BatchNorm2d(128)\n",
        "        self.l6 = nn.Conv2d(128,128,3,padding = 1)\n",
        "        self.n6 = nn.BatchNorm2d(128)\n",
        "        self.l7 = nn.Conv2d(128,128,3,padding = 1)\n",
        "        self.n7 = nn.BatchNorm2d(128)\n",
        "        self.l8 = nn.Conv2d(128,256,3,padding = 1)\n",
        "        self.n8 = nn.BatchNorm2d(256)\n",
        "        self.l9 = nn.Conv2d(256,256,3,padding = 1)\n",
        "        self.n9 = nn.BatchNorm2d(256)\n",
        "        self.l10 = nn.Linear(9216,1024)\n",
        "        self.l11 = nn.Linear(1024,1024)\n",
        "        self.c = nn.Sequential(\n",
        "            nn.Linear(9216,10),\n",
        "            nn.Softmax(dim = 1)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(p = 0.3)\n",
        "        self.drop2d = nn.Dropout2d(p = 0.3)\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.pool = nn.MaxPool2d(2,stride = 2)\n",
        "    def forward(self,xt,flag = False):\n",
        "      x = xt\n",
        "      convert = torch.zeros((x.shape[0],32-x.shape[1],x.shape[2],x.shape[3]))\n",
        "      convert = convert.to(device)\n",
        "      xt = torch.cat([xt,convert],dim = 1)\n",
        "      x = self.l1(x)\n",
        "      x = self.n1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.drop2d(x)\n",
        "      x = self.l2(x)\n",
        "      x = x+xt\n",
        "      x = self.n2(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.pool(x)\n",
        "      xt = x\n",
        "      x = self.l3(x)\n",
        "      x = self.n3(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.drop2d(x)\n",
        "      x = self.l4(x)\n",
        "      convert = torch.zeros(xt.shape,device=device)\n",
        "      xt = torch.cat([xt,convert],dim = 1)\n",
        "      x = x + torch.reshape(xt,x.shape)\n",
        "      x = self.n4(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.pool(x)\n",
        "      xt = x\n",
        "      x = self.l5(x)\n",
        "      x = self.n5(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.drop2d(x)\n",
        "      x = self.l6(x)\n",
        "      convert = torch.zeros(xt.shape,device=device)\n",
        "      xt = torch.cat([xt,convert],dim = 1)\n",
        "      x = x + torch.reshape(xt,x.shape)\n",
        "      x = self.n6(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.drop2d(x)\n",
        "      x = self.pool(x)\n",
        "      xt = x\n",
        "      x = self.l7(x)\n",
        "      x = self.n7(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.drop2d(x)\n",
        "      x = self.l8(x)\n",
        "      convert = torch.zeros(xt.shape,device=device)\n",
        "      xt = torch.cat([xt,convert],dim = 1)\n",
        "      x = x + torch.reshape(xt,x.shape)\n",
        "      x = self.n8(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.pool(x)\n",
        "      x = self.l9(x)\n",
        "      x = self.n9(x)\n",
        "      x = self.relu(x)\n",
        "      x = torch.reshape(x,(x.shape[0],-1,1,1))\n",
        "      x = torch.squeeze(x,dim = 2)\n",
        "      x = torch.squeeze(x,dim = 2)\n",
        "      if flag:\n",
        "        x = self.l10(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.l11(x)\n",
        "      else:\n",
        "        x = self.c(x)\n",
        "      return x\n",
        "model = Network()\n",
        "model = model.to(device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQCGzTrmHZYB"
      },
      "source": [
        "class Unsoupdata(Dataset):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    transform = transforms.ToTensor()\n",
        "    self.t = transforms.Compose([\n",
        "                            transforms.RandomVerticalFlip(0.5),\n",
        "                            transforms.RandomHorizontalFlip(0.5),\n",
        "                            transforms.RandomApply([transforms.ColorJitter()],p=0.5)\n",
        "    ])\n",
        "    self.data = torchvision.datasets.STL10(root = '/',split = 'unlabeled',download = True,transform = transform)\n",
        "    self.n = len(self.data)\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self,i):\n",
        "    decide = random.randint(0,self.n-1)\n",
        "    while (decide==i):    \n",
        "      decide = random.randint(0,self.n-1)\n",
        "    return self.data[i][0],self.t(self.data[i][0]),self.data[decide][0],self.t(self.data[decide][0])\n",
        "\n",
        "class Soupdata(Dataset):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    transform = transforms.ToTensor()\n",
        "    self.data = torchvision.datasets.STL10(root = '/',split = 'train',download = True,transform = transform)\n",
        "    self.n = len(self.data)\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self,i):\n",
        "    return self.data[i]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "82782acb0c6140daa839d58ccb8201b7",
            "f76563354e2f4def95fa70300d693899",
            "19662acf7b5a42b18afe6baaa64ccdfb",
            "d2792869320e43fd946c9e89c5922ca0",
            "736bd8907ef04f04951db2a2aa695293",
            "38f9af274a7d4644a643c56d277cbbf9",
            "b764e628f59f4ab8bb56285cbfcac1da",
            "e761a7bc62954233a4c62deb2197c39c",
            "d01e9de09ff4417796620d4101e44463",
            "9f18fc447bbc4037936722650e4e0071",
            "5ba59faecf3c42dfa910c582e7769a35"
          ]
        },
        "id": "F_CGRUCySvql",
        "outputId": "b9002029-88ab-4ddf-e241-8fb3fc0c959d"
      },
      "source": [
        "batch_size = 128\n",
        "unsoup = Unsoupdata()\n",
        "unsouploader = DataLoader(dataset=unsoup,batch_size = 128,shuffle = True)\n",
        "soup =  Soupdata()\n",
        "train_data,test_val = random_split(soup,[len(soup)-500,500])\n",
        "test_data = torchvision.datasets.STL10(root = '/',split = 'test',download = True,transform = transforms.ToTensor())\n",
        "souptrain = DataLoader(dataset=train_data,batch_size = batch_size,shuffle = True)\n",
        "soupval = DataLoader(dataset=test_val,batch_size = len(test_val),shuffle = True)\n",
        "souptest = DataLoader(dataset=test_data,batch_size = len(test_data),shuffle = True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to /stl10_binary.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82782acb0c6140daa839d58ccb8201b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2640397119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /stl10_binary.tar.gz to /\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1a9xI6416Bn"
      },
      "source": [
        "coste = nn.TripletMarginLoss()\n",
        "costc = nn.CrossEntropyLoss()\n",
        "sim = []\n",
        "def loss(y1,y2,y3,y4,t = 1,eps = 1e-8):\n",
        "  siml = nn.CosineSimilarity(dim = 1)\n",
        "  positive = siml(y1,y2)*siml(y3,y4)\n",
        "  negative = siml(y1,y3)+siml(y1,y4)+siml(y2,y3)+siml(y2,y4)\n",
        "  negative = negative**2\n",
        "  positive = torch.exp(positive)/t\n",
        "  negative = torch.exp(negative)/t\n",
        "  los = -1*torch.log(positive/negative)\n",
        "  los = los.mean()\n",
        "  return los"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIinezqgc6Ga"
      },
      "source": [
        "optimizerc = torch.optim.Adam(model.c.parameters(),lr = 0.001)\n",
        "optimizere = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "def test(data):\n",
        "    c = 0\n",
        "    s = 0\n",
        "    for i,(x,y) in enumerate(data):\n",
        "        with torch.no_grad():\n",
        "            x =x.to(device)\n",
        "            y = y.to(device,dtype = torch.int64)\n",
        "            yt = model(x,False)\n",
        "            yt = torch.argmax(yt, dim= 1)\n",
        "            c = (y == yt).sum()\n",
        "            s = y.shape[0]\n",
        "        break\n",
        "    return (100*c/s).item()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kCT1lYNNfD2w",
        "outputId": "b8f44ecf-d4d2-4dd0-889f-80854b636d2a"
      },
      "source": [
        "epochs = 10\n",
        "for j in range(epochs):\n",
        "  for i,(x1,x2,x3,x4) in enumerate(unsouploader) :\n",
        "    x1 = x1.to(device)\n",
        "    x2 = x2.to(device)\n",
        "    x3 = x3.to(device)\n",
        "    x4 = x4.to(device)\n",
        "    y1 = model(x1,True)\n",
        "    y2 = model(x2,True)\n",
        "    y3 = model(x3,True)\n",
        "    y4 = model(x4,True)\n",
        "    losse = loss(y1,y2,y3,y4)          \n",
        "    optimizere.zero_grad()\n",
        "    losse.backward()\n",
        "    optimizere.step()\n",
        "    print(f'epoch {j+1} step {i} loss {losse}')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch 1 step 389 loss 0.46476173400878906\n",
            "epoch 1 step 390 loss 0.612053394317627\n",
            "epoch 1 step 391 loss 0.6649968028068542\n",
            "epoch 1 step 392 loss 0.447976678609848\n",
            "epoch 1 step 393 loss 0.6037282943725586\n",
            "epoch 1 step 394 loss 0.9113625288009644\n",
            "epoch 1 step 395 loss 0.5033005475997925\n",
            "epoch 1 step 396 loss 0.5262120962142944\n",
            "epoch 1 step 397 loss 0.5894442796707153\n",
            "epoch 1 step 398 loss 0.5562207698822021\n",
            "epoch 1 step 399 loss 0.48075544834136963\n",
            "epoch 1 step 400 loss 0.5172708630561829\n",
            "epoch 1 step 401 loss 0.5605555772781372\n",
            "epoch 1 step 402 loss 0.4943723678588867\n",
            "epoch 1 step 403 loss 0.486528217792511\n",
            "epoch 1 step 404 loss 0.6501578092575073\n",
            "epoch 1 step 405 loss 0.3585273027420044\n",
            "epoch 1 step 406 loss 0.5527539849281311\n",
            "epoch 1 step 407 loss 0.5702358484268188\n",
            "epoch 1 step 408 loss 0.4440802037715912\n",
            "epoch 1 step 409 loss 0.5926536917686462\n",
            "epoch 1 step 410 loss 0.6106722950935364\n",
            "epoch 1 step 411 loss 0.5288556814193726\n",
            "epoch 1 step 412 loss 0.4498513638973236\n",
            "epoch 1 step 413 loss 0.5963495969772339\n",
            "epoch 1 step 414 loss 0.4989657998085022\n",
            "epoch 1 step 415 loss 0.5908526182174683\n",
            "epoch 1 step 416 loss 0.5660568475723267\n",
            "epoch 1 step 417 loss 0.5517035722732544\n",
            "epoch 1 step 418 loss 0.4445658326148987\n",
            "epoch 1 step 419 loss 0.3863821029663086\n",
            "epoch 1 step 420 loss 0.4972836971282959\n",
            "epoch 1 step 421 loss 0.5850299000740051\n",
            "epoch 1 step 422 loss 0.4517684280872345\n",
            "epoch 1 step 423 loss 0.481754332780838\n",
            "epoch 1 step 424 loss 0.48909515142440796\n",
            "epoch 1 step 425 loss 0.35675352811813354\n",
            "epoch 1 step 426 loss 0.4152752161026001\n",
            "epoch 1 step 427 loss 0.5387085676193237\n",
            "epoch 1 step 428 loss 0.6205573081970215\n",
            "epoch 1 step 429 loss 0.46852293610572815\n",
            "epoch 1 step 430 loss 0.6627794504165649\n",
            "epoch 1 step 431 loss 0.6060527563095093\n",
            "epoch 1 step 432 loss 0.4358636736869812\n",
            "epoch 1 step 433 loss 0.4427770972251892\n",
            "epoch 1 step 434 loss 0.46159833669662476\n",
            "epoch 1 step 435 loss 0.5084863901138306\n",
            "epoch 1 step 436 loss 0.5462188124656677\n",
            "epoch 1 step 437 loss 0.44351011514663696\n",
            "epoch 1 step 438 loss 0.40148866176605225\n",
            "epoch 1 step 439 loss 0.6069912910461426\n",
            "epoch 1 step 440 loss 0.45419856905937195\n",
            "epoch 1 step 441 loss 0.5588159561157227\n",
            "epoch 1 step 442 loss 0.4762422442436218\n",
            "epoch 1 step 443 loss 0.451316773891449\n",
            "epoch 1 step 444 loss 0.45560410618782043\n",
            "epoch 1 step 445 loss 0.3759310245513916\n",
            "epoch 1 step 446 loss 0.5886397361755371\n",
            "epoch 1 step 447 loss 0.5141393542289734\n",
            "epoch 1 step 448 loss 0.6112419366836548\n",
            "epoch 1 step 449 loss 0.4652753174304962\n",
            "epoch 1 step 450 loss 0.44548001885414124\n",
            "epoch 1 step 451 loss 0.6917534470558167\n",
            "epoch 1 step 452 loss 0.4385000765323639\n",
            "epoch 1 step 453 loss 0.46400177478790283\n",
            "epoch 1 step 454 loss 0.5205888748168945\n",
            "epoch 1 step 455 loss 0.5042586326599121\n",
            "epoch 1 step 456 loss 0.5434077382087708\n",
            "epoch 1 step 457 loss 0.36628296971321106\n",
            "epoch 1 step 458 loss 0.33075541257858276\n",
            "epoch 1 step 459 loss 0.5128051042556763\n",
            "epoch 1 step 460 loss 0.3955817222595215\n",
            "epoch 1 step 461 loss 0.4300837516784668\n",
            "epoch 1 step 462 loss 0.473839670419693\n",
            "epoch 1 step 463 loss 0.39184021949768066\n",
            "epoch 1 step 464 loss 0.4803377389907837\n",
            "epoch 1 step 465 loss 0.4667821526527405\n",
            "epoch 1 step 466 loss 0.40783801674842834\n",
            "epoch 1 step 467 loss 0.4637066125869751\n",
            "epoch 1 step 468 loss 0.3958868980407715\n",
            "epoch 1 step 469 loss 0.33246487379074097\n",
            "epoch 1 step 470 loss 0.41864287853240967\n",
            "epoch 1 step 471 loss 0.3323076069355011\n",
            "epoch 1 step 472 loss 0.4406451880931854\n",
            "epoch 1 step 473 loss 0.5055832862854004\n",
            "epoch 1 step 474 loss 0.36537155508995056\n",
            "epoch 1 step 475 loss 0.5372672080993652\n",
            "epoch 1 step 476 loss 0.5138691663742065\n",
            "epoch 1 step 477 loss 0.3237419128417969\n",
            "epoch 1 step 478 loss 0.39522892236709595\n",
            "epoch 1 step 479 loss 0.38008302450180054\n",
            "epoch 1 step 480 loss 0.5167430639266968\n",
            "epoch 1 step 481 loss 0.39720451831817627\n",
            "epoch 1 step 482 loss 0.4093358516693115\n",
            "epoch 1 step 483 loss 0.29799407720565796\n",
            "epoch 1 step 484 loss 0.4469187259674072\n",
            "epoch 1 step 485 loss 0.3779202401638031\n",
            "epoch 1 step 486 loss 0.36499327421188354\n",
            "epoch 1 step 487 loss 0.4443197250366211\n",
            "epoch 1 step 488 loss 0.3696194291114807\n",
            "epoch 1 step 489 loss 0.2877838611602783\n",
            "epoch 1 step 490 loss 0.39226680994033813\n",
            "epoch 1 step 491 loss 0.5039140582084656\n",
            "epoch 1 step 492 loss 0.389506459236145\n",
            "epoch 1 step 493 loss 0.32085272669792175\n",
            "epoch 1 step 494 loss 0.3312531113624573\n",
            "epoch 1 step 495 loss 0.2684327960014343\n",
            "epoch 1 step 496 loss 0.3235381841659546\n",
            "epoch 1 step 497 loss 0.28075942397117615\n",
            "epoch 1 step 498 loss 0.3607632517814636\n",
            "epoch 1 step 499 loss 0.37606024742126465\n",
            "epoch 1 step 500 loss 0.32911890745162964\n",
            "epoch 1 step 501 loss 0.3707371652126312\n",
            "epoch 1 step 502 loss 0.39335572719573975\n",
            "epoch 1 step 503 loss 0.39913612604141235\n",
            "epoch 1 step 504 loss 0.32426756620407104\n",
            "epoch 1 step 505 loss 0.358173131942749\n",
            "epoch 1 step 506 loss 0.2979557514190674\n",
            "epoch 1 step 507 loss 0.33921462297439575\n",
            "epoch 1 step 508 loss 0.2914324998855591\n",
            "epoch 1 step 509 loss 0.40523362159729004\n",
            "epoch 1 step 510 loss 0.3372529149055481\n",
            "epoch 1 step 511 loss 0.22221505641937256\n",
            "epoch 1 step 512 loss 0.35703393816947937\n",
            "epoch 1 step 513 loss 0.32528746128082275\n",
            "epoch 1 step 514 loss 0.2872454524040222\n",
            "epoch 1 step 515 loss 0.30626872181892395\n",
            "epoch 1 step 516 loss 0.38221389055252075\n",
            "epoch 1 step 517 loss 0.39305537939071655\n",
            "epoch 1 step 518 loss 0.5143599510192871\n",
            "epoch 1 step 519 loss 0.3009743392467499\n",
            "epoch 1 step 520 loss 0.2646141052246094\n",
            "epoch 1 step 521 loss 0.36120015382766724\n",
            "epoch 1 step 522 loss 0.39087963104248047\n",
            "epoch 1 step 523 loss 0.32145023345947266\n",
            "epoch 1 step 524 loss 0.3116432726383209\n",
            "epoch 1 step 525 loss 0.3163771629333496\n",
            "epoch 1 step 526 loss 0.34621351957321167\n",
            "epoch 1 step 527 loss 0.3197280168533325\n",
            "epoch 1 step 528 loss 0.3130686283111572\n",
            "epoch 1 step 529 loss 0.2769163250923157\n",
            "epoch 1 step 530 loss 0.365309476852417\n",
            "epoch 1 step 531 loss 0.30864453315734863\n",
            "epoch 1 step 532 loss 0.30880916118621826\n",
            "epoch 1 step 533 loss 0.4484831690788269\n",
            "epoch 1 step 534 loss 0.2576565742492676\n",
            "epoch 1 step 535 loss 0.31316065788269043\n",
            "epoch 1 step 536 loss 0.19768601655960083\n",
            "epoch 1 step 537 loss 0.24357618391513824\n",
            "epoch 1 step 538 loss 0.2808323800563812\n",
            "epoch 1 step 539 loss 0.28806042671203613\n",
            "epoch 1 step 540 loss 0.21791291236877441\n",
            "epoch 1 step 541 loss 0.2707393169403076\n",
            "epoch 1 step 542 loss 0.2856300175189972\n",
            "epoch 1 step 543 loss 0.30069947242736816\n",
            "epoch 1 step 544 loss 0.2372419536113739\n",
            "epoch 1 step 545 loss 0.2638269066810608\n",
            "epoch 1 step 546 loss 0.2854176461696625\n",
            "epoch 1 step 547 loss 0.23676109313964844\n",
            "epoch 1 step 548 loss 0.2827971577644348\n",
            "epoch 1 step 549 loss 0.3345864415168762\n",
            "epoch 1 step 550 loss 0.3263295292854309\n",
            "epoch 1 step 551 loss 0.29812633991241455\n",
            "epoch 1 step 552 loss 0.2621679902076721\n",
            "epoch 1 step 553 loss 0.19136309623718262\n",
            "epoch 1 step 554 loss 0.20952096581459045\n",
            "epoch 1 step 555 loss 0.2994304895401001\n",
            "epoch 1 step 556 loss 0.2444276511669159\n",
            "epoch 1 step 557 loss 0.2574663460254669\n",
            "epoch 1 step 558 loss 0.2720358073711395\n",
            "epoch 1 step 559 loss 0.274826318025589\n",
            "epoch 1 step 560 loss 0.304188072681427\n",
            "epoch 1 step 561 loss 0.22583580017089844\n",
            "epoch 1 step 562 loss 0.31033632159233093\n",
            "epoch 1 step 563 loss 0.23028098046779633\n",
            "epoch 1 step 564 loss 0.1710970401763916\n",
            "epoch 1 step 565 loss 0.25050482153892517\n",
            "epoch 1 step 566 loss 0.2688884735107422\n",
            "epoch 1 step 567 loss 0.27887290716171265\n",
            "epoch 1 step 568 loss 0.28980928659439087\n",
            "epoch 1 step 569 loss 0.21984466910362244\n",
            "epoch 1 step 570 loss 0.23719990253448486\n",
            "epoch 1 step 571 loss 0.25180959701538086\n",
            "epoch 1 step 572 loss 0.17559567093849182\n",
            "epoch 1 step 573 loss 0.20482784509658813\n",
            "epoch 1 step 574 loss 0.18460437655448914\n",
            "epoch 1 step 575 loss 0.19626358151435852\n",
            "epoch 1 step 576 loss 0.3025597035884857\n",
            "epoch 1 step 577 loss 0.3411571979522705\n",
            "epoch 1 step 578 loss 0.14063389599323273\n",
            "epoch 1 step 579 loss 0.22449806332588196\n",
            "epoch 1 step 580 loss 0.18834200501441956\n",
            "epoch 1 step 581 loss 0.3048395812511444\n",
            "epoch 1 step 582 loss 0.2780701518058777\n",
            "epoch 1 step 583 loss 0.24083484709262848\n",
            "epoch 1 step 584 loss 0.2498658299446106\n",
            "epoch 1 step 585 loss 0.18296143412590027\n",
            "epoch 1 step 586 loss 0.1785443276166916\n",
            "epoch 1 step 587 loss 0.15170708298683167\n",
            "epoch 1 step 588 loss 0.274604856967926\n",
            "epoch 1 step 589 loss 0.19690394401550293\n",
            "epoch 1 step 590 loss 0.2218499779701233\n",
            "epoch 1 step 591 loss 0.2566281855106354\n",
            "epoch 1 step 592 loss 0.4261273741722107\n",
            "epoch 1 step 593 loss 0.21448004245758057\n",
            "epoch 1 step 594 loss 0.24895483255386353\n",
            "epoch 1 step 595 loss 0.1898757815361023\n",
            "epoch 1 step 596 loss 0.24157525599002838\n",
            "epoch 1 step 597 loss 0.2004213035106659\n",
            "epoch 1 step 598 loss 0.17315959930419922\n",
            "epoch 1 step 599 loss 0.1839529573917389\n",
            "epoch 1 step 600 loss 0.2130279839038849\n",
            "epoch 1 step 601 loss 0.23223964869976044\n",
            "epoch 1 step 602 loss 0.2260863482952118\n",
            "epoch 1 step 603 loss 0.22455742955207825\n",
            "epoch 1 step 604 loss 0.19619330763816833\n",
            "epoch 1 step 605 loss 0.1407032310962677\n",
            "epoch 1 step 606 loss 0.338044673204422\n",
            "epoch 1 step 607 loss 0.29322168231010437\n",
            "epoch 1 step 608 loss 0.14614447951316833\n",
            "epoch 1 step 609 loss 0.15991343557834625\n",
            "epoch 1 step 610 loss 0.2057431936264038\n",
            "epoch 1 step 611 loss 0.26217272877693176\n",
            "epoch 1 step 612 loss 0.22739438712596893\n",
            "epoch 1 step 613 loss 0.21175597608089447\n",
            "epoch 1 step 614 loss 0.13561026751995087\n",
            "epoch 1 step 615 loss 0.23936651647090912\n",
            "epoch 1 step 616 loss 0.16751015186309814\n",
            "epoch 1 step 617 loss 0.22660386562347412\n",
            "epoch 1 step 618 loss 0.19046339392662048\n",
            "epoch 1 step 619 loss 0.19139283895492554\n",
            "epoch 1 step 620 loss 0.2095891535282135\n",
            "epoch 1 step 621 loss 0.2615540623664856\n",
            "epoch 1 step 622 loss 0.24920445680618286\n",
            "epoch 1 step 623 loss 0.19813759624958038\n",
            "epoch 1 step 624 loss 0.212773859500885\n",
            "epoch 1 step 625 loss 0.21962061524391174\n",
            "epoch 1 step 626 loss 0.24569174647331238\n",
            "epoch 1 step 627 loss 0.37077412009239197\n",
            "epoch 1 step 628 loss 0.16810539364814758\n",
            "epoch 1 step 629 loss 0.33195167779922485\n",
            "epoch 1 step 630 loss 0.20277507603168488\n",
            "epoch 1 step 631 loss 0.17477746307849884\n",
            "epoch 1 step 632 loss 0.26911866664886475\n",
            "epoch 1 step 633 loss 0.2264050990343094\n",
            "epoch 1 step 634 loss 0.20338456332683563\n",
            "epoch 1 step 635 loss 0.26819247007369995\n",
            "epoch 1 step 636 loss 0.17436333000659943\n",
            "epoch 1 step 637 loss 0.156795471906662\n",
            "epoch 1 step 638 loss 0.21260878443717957\n",
            "epoch 1 step 639 loss 0.2090030163526535\n",
            "epoch 1 step 640 loss 0.2226068675518036\n",
            "epoch 1 step 641 loss 0.2094649374485016\n",
            "epoch 1 step 642 loss 0.2178010493516922\n",
            "epoch 1 step 643 loss 0.19794559478759766\n",
            "epoch 1 step 644 loss 0.20636315643787384\n",
            "epoch 1 step 645 loss 0.14792472124099731\n",
            "epoch 1 step 646 loss 0.164240762591362\n",
            "epoch 1 step 647 loss 0.3304908275604248\n",
            "epoch 1 step 648 loss 0.17702026665210724\n",
            "epoch 1 step 649 loss 0.19602951407432556\n",
            "epoch 1 step 650 loss 0.13946327567100525\n",
            "epoch 1 step 651 loss 0.14195773005485535\n",
            "epoch 1 step 652 loss 0.24076808989048004\n",
            "epoch 1 step 653 loss 0.24777516722679138\n",
            "epoch 1 step 654 loss 0.16043439507484436\n",
            "epoch 1 step 655 loss 0.18531815707683563\n",
            "epoch 1 step 656 loss 0.22041943669319153\n",
            "epoch 1 step 657 loss 0.15818482637405396\n",
            "epoch 1 step 658 loss 0.13902398943901062\n",
            "epoch 1 step 659 loss 0.1318783313035965\n",
            "epoch 1 step 660 loss 0.2117181271314621\n",
            "epoch 1 step 661 loss 0.2056640386581421\n",
            "epoch 1 step 662 loss 0.20532649755477905\n",
            "epoch 1 step 663 loss 0.28627872467041016\n",
            "epoch 1 step 664 loss 0.19211450219154358\n",
            "epoch 1 step 665 loss 0.16642440855503082\n",
            "epoch 1 step 666 loss 0.1555107831954956\n",
            "epoch 1 step 667 loss 0.19872771203517914\n",
            "epoch 1 step 668 loss 0.16531842947006226\n",
            "epoch 1 step 669 loss 0.18807175755500793\n",
            "epoch 1 step 670 loss 0.1231340691447258\n",
            "epoch 1 step 671 loss 0.25594669580459595\n",
            "epoch 1 step 672 loss 0.19818516075611115\n",
            "epoch 1 step 673 loss 0.254916787147522\n",
            "epoch 1 step 674 loss 0.18886859714984894\n",
            "epoch 1 step 675 loss 0.15846064686775208\n",
            "epoch 1 step 676 loss 0.16074472665786743\n",
            "epoch 1 step 677 loss 0.22176557779312134\n",
            "epoch 1 step 678 loss 0.16968032717704773\n",
            "epoch 1 step 679 loss 0.1530013233423233\n",
            "epoch 1 step 680 loss 0.2234458327293396\n",
            "epoch 1 step 681 loss 0.20880097150802612\n",
            "epoch 1 step 682 loss 0.204391747713089\n",
            "epoch 1 step 683 loss 0.15099678933620453\n",
            "epoch 1 step 684 loss 0.12281370908021927\n",
            "epoch 1 step 685 loss 0.1406860053539276\n",
            "epoch 1 step 686 loss 0.14264291524887085\n",
            "epoch 1 step 687 loss 0.1810949742794037\n",
            "epoch 1 step 688 loss 0.18444079160690308\n",
            "epoch 1 step 689 loss 0.1644349992275238\n",
            "epoch 1 step 690 loss 0.18522237241268158\n",
            "epoch 1 step 691 loss 0.11123022437095642\n",
            "epoch 1 step 692 loss 0.1665225625038147\n",
            "epoch 1 step 693 loss 0.19516561925411224\n",
            "epoch 1 step 694 loss 0.19856664538383484\n",
            "epoch 1 step 695 loss 0.19199053943157196\n",
            "epoch 1 step 696 loss 0.16281621158123016\n",
            "epoch 1 step 697 loss 0.15398181974887848\n",
            "epoch 1 step 698 loss 0.18345525860786438\n",
            "epoch 1 step 699 loss 0.1178167387843132\n",
            "epoch 1 step 700 loss 0.1612749695777893\n",
            "epoch 1 step 701 loss 0.17608672380447388\n",
            "epoch 1 step 702 loss 0.22998261451721191\n",
            "epoch 1 step 703 loss 0.13805615901947021\n",
            "epoch 1 step 704 loss 0.25779417157173157\n",
            "epoch 1 step 705 loss 0.19350384175777435\n",
            "epoch 1 step 706 loss 0.12539726495742798\n",
            "epoch 1 step 707 loss 0.14544683694839478\n",
            "epoch 1 step 708 loss 0.19145503640174866\n",
            "epoch 1 step 709 loss 0.18843260407447815\n",
            "epoch 1 step 710 loss 0.15805387496948242\n",
            "epoch 1 step 711 loss 0.23350384831428528\n",
            "epoch 1 step 712 loss 0.21385623514652252\n",
            "epoch 1 step 713 loss 0.12053345143795013\n",
            "epoch 1 step 714 loss 0.13597792387008667\n",
            "epoch 1 step 715 loss 0.13466328382492065\n",
            "epoch 1 step 716 loss 0.1177283301949501\n",
            "epoch 1 step 717 loss 0.12964175641536713\n",
            "epoch 1 step 718 loss 0.129850372672081\n",
            "epoch 1 step 719 loss 0.1393699198961258\n",
            "epoch 1 step 720 loss 0.1606777012348175\n",
            "epoch 1 step 721 loss 0.15128777921199799\n",
            "epoch 1 step 722 loss 0.14934301376342773\n",
            "epoch 1 step 723 loss 0.18965554237365723\n",
            "epoch 1 step 724 loss 0.18538376688957214\n",
            "epoch 1 step 725 loss 0.11843017488718033\n",
            "epoch 1 step 726 loss 0.15307143330574036\n",
            "epoch 1 step 727 loss 0.16640174388885498\n",
            "epoch 1 step 728 loss 0.14855164289474487\n",
            "epoch 1 step 729 loss 0.16272549331188202\n",
            "epoch 1 step 730 loss 0.12169955670833588\n",
            "epoch 1 step 731 loss 0.17671933770179749\n",
            "epoch 1 step 732 loss 0.17850017547607422\n",
            "epoch 1 step 733 loss 0.1424781084060669\n",
            "epoch 1 step 734 loss 0.11740096658468246\n",
            "epoch 1 step 735 loss 0.12467880547046661\n",
            "epoch 1 step 736 loss 0.1618463099002838\n",
            "epoch 1 step 737 loss 0.18006372451782227\n",
            "epoch 1 step 738 loss 0.13424116373062134\n",
            "epoch 1 step 739 loss 0.1512051522731781\n",
            "epoch 1 step 740 loss 0.1141359955072403\n",
            "epoch 1 step 741 loss 0.10743972659111023\n",
            "epoch 1 step 742 loss 0.14828437566757202\n",
            "epoch 1 step 743 loss 0.14622072875499725\n",
            "epoch 1 step 744 loss 0.1281537264585495\n",
            "epoch 1 step 745 loss 0.09602095186710358\n",
            "epoch 1 step 746 loss 0.13626866042613983\n",
            "epoch 1 step 747 loss 0.1580333560705185\n",
            "epoch 1 step 748 loss 0.1484312117099762\n",
            "epoch 1 step 749 loss 0.11822009086608887\n",
            "epoch 1 step 750 loss 0.09806062281131744\n",
            "epoch 1 step 751 loss 0.17107483744621277\n",
            "epoch 1 step 752 loss 0.18093475699424744\n",
            "epoch 1 step 753 loss 0.16041222214698792\n",
            "epoch 1 step 754 loss 0.1466512531042099\n",
            "epoch 1 step 755 loss 0.12883451581001282\n",
            "epoch 1 step 756 loss 0.166843444108963\n",
            "epoch 1 step 757 loss 0.15445172786712646\n",
            "epoch 1 step 758 loss 0.13625934720039368\n",
            "epoch 1 step 759 loss 0.1832057684659958\n",
            "epoch 1 step 760 loss 0.0909954383969307\n",
            "epoch 1 step 761 loss 0.16309854388237\n",
            "epoch 1 step 762 loss 0.13192307949066162\n",
            "epoch 1 step 763 loss 0.1796780526638031\n",
            "epoch 1 step 764 loss 0.13621237874031067\n",
            "epoch 1 step 765 loss 0.13524645566940308\n",
            "epoch 1 step 766 loss 0.10579735040664673\n",
            "epoch 1 step 767 loss 0.1863386482000351\n",
            "epoch 1 step 768 loss 0.17045682668685913\n",
            "epoch 1 step 769 loss 0.18054607510566711\n",
            "epoch 1 step 770 loss 0.1119871586561203\n",
            "epoch 1 step 771 loss 0.17956507205963135\n",
            "epoch 1 step 772 loss 0.12183576822280884\n",
            "epoch 1 step 773 loss 0.12573577463626862\n",
            "epoch 1 step 774 loss 0.12877587974071503\n",
            "epoch 1 step 775 loss 0.12049317359924316\n",
            "epoch 1 step 776 loss 0.1039866954088211\n",
            "epoch 1 step 777 loss 0.0924062579870224\n",
            "epoch 1 step 778 loss 0.10202620923519135\n",
            "epoch 1 step 779 loss 0.15093904733657837\n",
            "epoch 1 step 780 loss 0.12986142933368683\n",
            "epoch 1 step 781 loss 0.16428864002227783\n",
            "epoch 2 step 0 loss 0.10760412365198135\n",
            "epoch 2 step 1 loss 0.19071707129478455\n",
            "epoch 2 step 2 loss 0.10983790457248688\n",
            "epoch 2 step 3 loss 0.11546748876571655\n",
            "epoch 2 step 4 loss 0.17753344774246216\n",
            "epoch 2 step 5 loss 0.1121230274438858\n",
            "epoch 2 step 6 loss 0.1290912628173828\n",
            "epoch 2 step 7 loss 0.16476431488990784\n",
            "epoch 2 step 8 loss 0.13875868916511536\n",
            "epoch 2 step 9 loss 0.14228731393814087\n",
            "epoch 2 step 10 loss 0.07106330245733261\n",
            "epoch 2 step 11 loss 0.17706598341464996\n",
            "epoch 2 step 12 loss 0.13384418189525604\n",
            "epoch 2 step 13 loss 0.1262052059173584\n",
            "epoch 2 step 14 loss 0.12212918698787689\n",
            "epoch 2 step 15 loss 0.16425107419490814\n",
            "epoch 2 step 16 loss 0.11542154103517532\n",
            "epoch 2 step 17 loss 0.09630454331636429\n",
            "epoch 2 step 18 loss 0.10760369151830673\n",
            "epoch 2 step 19 loss 0.10918566584587097\n",
            "epoch 2 step 20 loss 0.07849183678627014\n",
            "epoch 2 step 21 loss 0.10329248011112213\n",
            "epoch 2 step 22 loss 0.10661883652210236\n",
            "epoch 2 step 23 loss 0.10664252936840057\n",
            "epoch 2 step 24 loss 0.08301141113042831\n",
            "epoch 2 step 25 loss 0.14128990471363068\n",
            "epoch 2 step 26 loss 0.12851795554161072\n",
            "epoch 2 step 27 loss 0.10960111767053604\n",
            "epoch 2 step 28 loss 0.08362708985805511\n",
            "epoch 2 step 29 loss 0.13292083144187927\n",
            "epoch 2 step 30 loss 0.08640159666538239\n",
            "epoch 2 step 31 loss 0.13874171674251556\n",
            "epoch 2 step 32 loss 0.07880106568336487\n",
            "epoch 2 step 33 loss 0.09267684072256088\n",
            "epoch 2 step 34 loss 0.08800412714481354\n",
            "epoch 2 step 35 loss 0.10201279819011688\n",
            "epoch 2 step 36 loss 0.09959587454795837\n",
            "epoch 2 step 37 loss 0.12149327993392944\n",
            "epoch 2 step 38 loss 0.0970991924405098\n",
            "epoch 2 step 39 loss 0.10653642565011978\n",
            "epoch 2 step 40 loss 0.09913693368434906\n",
            "epoch 2 step 41 loss 0.1445036083459854\n",
            "epoch 2 step 42 loss 0.09012822806835175\n",
            "epoch 2 step 43 loss 0.09396781027317047\n",
            "epoch 2 step 44 loss 0.07577542960643768\n",
            "epoch 2 step 45 loss 0.2203049659729004\n",
            "epoch 2 step 46 loss 0.0995180532336235\n",
            "epoch 2 step 47 loss 0.09330928325653076\n",
            "epoch 2 step 48 loss 0.11387422680854797\n",
            "epoch 2 step 49 loss 0.08930635452270508\n",
            "epoch 2 step 50 loss 0.10638773441314697\n",
            "epoch 2 step 51 loss 0.1381850242614746\n",
            "epoch 2 step 52 loss 0.12579995393753052\n",
            "epoch 2 step 53 loss 0.12511152029037476\n",
            "epoch 2 step 54 loss 0.1300356686115265\n",
            "epoch 2 step 55 loss 0.10914528369903564\n",
            "epoch 2 step 56 loss 0.08870988339185715\n",
            "epoch 2 step 57 loss 0.1377374529838562\n",
            "epoch 2 step 58 loss 0.13336706161499023\n",
            "epoch 2 step 59 loss 0.08957009762525558\n",
            "epoch 2 step 60 loss 0.12478303909301758\n",
            "epoch 2 step 61 loss 0.10433816909790039\n",
            "epoch 2 step 62 loss 0.12380099296569824\n",
            "epoch 2 step 63 loss 0.10531804710626602\n",
            "epoch 2 step 64 loss 0.08854949474334717\n",
            "epoch 2 step 65 loss 0.1120864674448967\n",
            "epoch 2 step 66 loss 0.11422724276781082\n",
            "epoch 2 step 67 loss 0.11955694854259491\n",
            "epoch 2 step 68 loss 0.10087534785270691\n",
            "epoch 2 step 69 loss 0.1151803731918335\n",
            "epoch 2 step 70 loss 0.0931851714849472\n",
            "epoch 2 step 71 loss 0.09342430531978607\n",
            "epoch 2 step 72 loss 0.11608177423477173\n",
            "epoch 2 step 73 loss 0.09370894730091095\n",
            "epoch 2 step 74 loss 0.08706642687320709\n",
            "epoch 2 step 75 loss 0.12643778324127197\n",
            "epoch 2 step 76 loss 0.10455851256847382\n",
            "epoch 2 step 77 loss 0.14189492166042328\n",
            "epoch 2 step 78 loss 0.10251499712467194\n",
            "epoch 2 step 79 loss 0.15540869534015656\n",
            "epoch 2 step 80 loss 0.10755665600299835\n",
            "epoch 2 step 81 loss 0.14505869150161743\n",
            "epoch 2 step 82 loss 0.08279432356357574\n",
            "epoch 2 step 83 loss 0.1506296545267105\n",
            "epoch 2 step 84 loss 0.11829008162021637\n",
            "epoch 2 step 85 loss 0.1301289200782776\n",
            "epoch 2 step 86 loss 0.09749028086662292\n",
            "epoch 2 step 87 loss 0.0928712785243988\n",
            "epoch 2 step 88 loss 0.11515588313341141\n",
            "epoch 2 step 89 loss 0.11214768141508102\n",
            "epoch 2 step 90 loss 0.12093329429626465\n",
            "epoch 2 step 91 loss 0.14372555911540985\n",
            "epoch 2 step 92 loss 0.1461828649044037\n",
            "epoch 2 step 93 loss 0.08056530356407166\n",
            "epoch 2 step 94 loss 0.12957465648651123\n",
            "epoch 2 step 95 loss 0.06854767352342606\n",
            "epoch 2 step 96 loss 0.0823611468076706\n",
            "epoch 2 step 97 loss 0.09378807246685028\n",
            "epoch 2 step 98 loss 0.07336437702178955\n",
            "epoch 2 step 99 loss 0.1224302127957344\n",
            "epoch 2 step 100 loss 0.13627654314041138\n",
            "epoch 2 step 101 loss 0.08894145488739014\n",
            "epoch 2 step 102 loss 0.1326788067817688\n",
            "epoch 2 step 103 loss 0.09993655979633331\n",
            "epoch 2 step 104 loss 0.09998659044504166\n",
            "epoch 2 step 105 loss 0.15130439400672913\n",
            "epoch 2 step 106 loss 0.1338093876838684\n",
            "epoch 2 step 107 loss 0.10347618162631989\n",
            "epoch 2 step 108 loss 0.09591935575008392\n",
            "epoch 2 step 109 loss 0.12020072340965271\n",
            "epoch 2 step 110 loss 0.09269647300243378\n",
            "epoch 2 step 111 loss 0.11765572428703308\n",
            "epoch 2 step 112 loss 0.13027507066726685\n",
            "epoch 2 step 113 loss 0.06735675781965256\n",
            "epoch 2 step 114 loss 0.11132004857063293\n",
            "epoch 2 step 115 loss 0.0984949916601181\n",
            "epoch 2 step 116 loss 0.12002827972173691\n",
            "epoch 2 step 117 loss 0.12996533513069153\n",
            "epoch 2 step 118 loss 0.10015280544757843\n",
            "epoch 2 step 119 loss 0.0867108553647995\n",
            "epoch 2 step 120 loss 0.09717704355716705\n",
            "epoch 2 step 121 loss 0.13234657049179077\n",
            "epoch 2 step 122 loss 0.17094701528549194\n",
            "epoch 2 step 123 loss 0.10639836639165878\n",
            "epoch 2 step 124 loss 0.09071250259876251\n",
            "epoch 2 step 125 loss 0.17335893213748932\n",
            "epoch 2 step 126 loss 0.13985693454742432\n",
            "epoch 2 step 127 loss 0.08686317503452301\n",
            "epoch 2 step 128 loss 0.0890248715877533\n",
            "epoch 2 step 129 loss 0.1044580414891243\n",
            "epoch 2 step 130 loss 0.10904471576213837\n",
            "epoch 2 step 131 loss 0.06500720977783203\n",
            "epoch 2 step 132 loss 0.12458023428916931\n",
            "epoch 2 step 133 loss 0.1369461566209793\n",
            "epoch 2 step 134 loss 0.10752281546592712\n",
            "epoch 2 step 135 loss 0.09194550663232803\n",
            "epoch 2 step 136 loss 0.08125217258930206\n",
            "epoch 2 step 137 loss 0.10711032152175903\n",
            "epoch 2 step 138 loss 0.09470811486244202\n",
            "epoch 2 step 139 loss 0.13775892555713654\n",
            "epoch 2 step 140 loss 0.13851095736026764\n",
            "epoch 2 step 141 loss 0.09005472809076309\n",
            "epoch 2 step 142 loss 0.10481663793325424\n",
            "epoch 2 step 143 loss 0.10265409201383591\n",
            "epoch 2 step 144 loss 0.07914423197507858\n",
            "epoch 2 step 145 loss 0.06733059883117676\n",
            "epoch 2 step 146 loss 0.12525498867034912\n",
            "epoch 2 step 147 loss 0.09374770522117615\n",
            "epoch 2 step 148 loss 0.07769474387168884\n",
            "epoch 2 step 149 loss 0.11700397729873657\n",
            "epoch 2 step 150 loss 0.09759239852428436\n",
            "epoch 2 step 151 loss 0.09841518104076385\n",
            "epoch 2 step 152 loss 0.13326632976531982\n",
            "epoch 2 step 153 loss 0.12169697880744934\n",
            "epoch 2 step 154 loss 0.12351657450199127\n",
            "epoch 2 step 155 loss 0.08837585151195526\n",
            "epoch 2 step 156 loss 0.1531534045934677\n",
            "epoch 2 step 157 loss 0.12098032236099243\n",
            "epoch 2 step 158 loss 0.0861826241016388\n",
            "epoch 2 step 159 loss 0.07924286276102066\n",
            "epoch 2 step 160 loss 0.06250891834497452\n",
            "epoch 2 step 161 loss 0.10596823692321777\n",
            "epoch 2 step 162 loss 0.14250832796096802\n",
            "epoch 2 step 163 loss 0.11555958539247513\n",
            "epoch 2 step 164 loss 0.07485483586788177\n",
            "epoch 2 step 165 loss 0.06283741444349289\n",
            "epoch 2 step 166 loss 0.10650967061519623\n",
            "epoch 2 step 167 loss 0.1297968327999115\n",
            "epoch 2 step 168 loss 0.05836673825979233\n",
            "epoch 2 step 169 loss 0.09625095129013062\n",
            "epoch 2 step 170 loss 0.08829125761985779\n",
            "epoch 2 step 171 loss 0.07771176099777222\n",
            "epoch 2 step 172 loss 0.110478475689888\n",
            "epoch 2 step 173 loss 0.08172266185283661\n",
            "epoch 2 step 174 loss 0.10857364535331726\n",
            "epoch 2 step 175 loss 0.11386165022850037\n",
            "epoch 2 step 176 loss 0.08491379022598267\n",
            "epoch 2 step 177 loss 0.07072798907756805\n",
            "epoch 2 step 178 loss 0.12948451936244965\n",
            "epoch 2 step 179 loss 0.06392852216959\n",
            "epoch 2 step 180 loss 0.09426999092102051\n",
            "epoch 2 step 181 loss 0.06047634035348892\n",
            "epoch 2 step 182 loss 0.10508528351783752\n",
            "epoch 2 step 183 loss 0.07007987797260284\n",
            "epoch 2 step 184 loss 0.09271478652954102\n",
            "epoch 2 step 185 loss 0.11895179003477097\n",
            "epoch 2 step 186 loss 0.05723648518323898\n",
            "epoch 2 step 187 loss 0.07671786844730377\n",
            "epoch 2 step 188 loss 0.10290994495153427\n",
            "epoch 2 step 189 loss 0.10761416703462601\n",
            "epoch 2 step 190 loss 0.09997057914733887\n",
            "epoch 2 step 191 loss 0.0899008959531784\n",
            "epoch 2 step 192 loss 0.12464747577905655\n",
            "epoch 2 step 193 loss 0.08421072363853455\n",
            "epoch 2 step 194 loss 0.12726522982120514\n",
            "epoch 2 step 195 loss 0.14689432084560394\n",
            "epoch 2 step 196 loss 0.11477594822645187\n",
            "epoch 2 step 197 loss 0.1348821073770523\n",
            "epoch 2 step 198 loss 0.10234102606773376\n",
            "epoch 2 step 199 loss 0.074109748005867\n",
            "epoch 2 step 200 loss 0.07882953435182571\n",
            "epoch 2 step 201 loss 0.0993422269821167\n",
            "epoch 2 step 202 loss 0.10491567850112915\n",
            "epoch 2 step 203 loss 0.11047005653381348\n",
            "epoch 2 step 204 loss 0.10737169533967972\n",
            "epoch 2 step 205 loss 0.08260123431682587\n",
            "epoch 2 step 206 loss 0.09977574646472931\n",
            "epoch 2 step 207 loss 0.07974317669868469\n",
            "epoch 2 step 208 loss 0.1459227204322815\n",
            "epoch 2 step 209 loss 0.10619718581438065\n",
            "epoch 2 step 210 loss 0.14078748226165771\n",
            "epoch 2 step 211 loss 0.10177876055240631\n",
            "epoch 2 step 212 loss 0.09459526836872101\n",
            "epoch 2 step 213 loss 0.10133254528045654\n",
            "epoch 2 step 214 loss 0.1021106094121933\n",
            "epoch 2 step 215 loss 0.12195128202438354\n",
            "epoch 2 step 216 loss 0.11790996789932251\n",
            "epoch 2 step 217 loss 0.1509139984846115\n",
            "epoch 2 step 218 loss 0.11588089168071747\n",
            "epoch 2 step 219 loss 0.10280212759971619\n",
            "epoch 2 step 220 loss 0.14935195446014404\n",
            "epoch 2 step 221 loss 0.12121622264385223\n",
            "epoch 2 step 222 loss 0.11647264659404755\n",
            "epoch 2 step 223 loss 0.11100713908672333\n",
            "epoch 2 step 224 loss 0.09246466308832169\n",
            "epoch 2 step 225 loss 0.08216096460819244\n",
            "epoch 2 step 226 loss 0.123051717877388\n",
            "epoch 2 step 227 loss 0.11301736533641815\n",
            "epoch 2 step 228 loss 0.06832966208457947\n",
            "epoch 2 step 229 loss 0.12828122079372406\n",
            "epoch 2 step 230 loss 0.06696236878633499\n",
            "epoch 2 step 231 loss 0.13398377597332\n",
            "epoch 2 step 232 loss 0.10947470366954803\n",
            "epoch 2 step 233 loss 0.08190969377756119\n",
            "epoch 2 step 234 loss 0.13269081711769104\n",
            "epoch 2 step 235 loss 0.08901871740818024\n",
            "epoch 2 step 236 loss 0.09382849186658859\n",
            "epoch 2 step 237 loss 0.05364079773426056\n",
            "epoch 2 step 238 loss 0.10696905106306076\n",
            "epoch 2 step 239 loss 0.10538561642169952\n",
            "epoch 2 step 240 loss 0.07932063192129135\n",
            "epoch 2 step 241 loss 0.07542112469673157\n",
            "epoch 2 step 242 loss 0.14859256148338318\n",
            "epoch 2 step 243 loss 0.10042230784893036\n",
            "epoch 2 step 244 loss 0.08168791979551315\n",
            "epoch 2 step 245 loss 0.10988172888755798\n",
            "epoch 2 step 246 loss 0.10024778544902802\n",
            "epoch 2 step 247 loss 0.13378585875034332\n",
            "epoch 2 step 248 loss 0.0918983519077301\n",
            "epoch 2 step 249 loss 0.08969709277153015\n",
            "epoch 2 step 250 loss 0.13731113076210022\n",
            "epoch 2 step 251 loss 0.12242360413074493\n",
            "epoch 2 step 252 loss 0.09136223793029785\n",
            "epoch 2 step 253 loss 0.07967908680438995\n",
            "epoch 2 step 254 loss 0.15749019384384155\n",
            "epoch 2 step 255 loss 0.09986426681280136\n",
            "epoch 2 step 256 loss 0.10219264775514603\n",
            "epoch 2 step 257 loss 0.11585503071546555\n",
            "epoch 2 step 258 loss 0.08916480839252472\n",
            "epoch 2 step 259 loss 0.1081840991973877\n",
            "epoch 2 step 260 loss 0.10283856093883514\n",
            "epoch 2 step 261 loss 0.08239709585905075\n",
            "epoch 2 step 262 loss 0.11636027693748474\n",
            "epoch 2 step 263 loss 0.10494698584079742\n",
            "epoch 2 step 264 loss 0.0838659405708313\n",
            "epoch 2 step 265 loss 0.08614456653594971\n",
            "epoch 2 step 266 loss 0.08680495619773865\n",
            "epoch 2 step 267 loss 0.07068562507629395\n",
            "epoch 2 step 268 loss 0.08785593509674072\n",
            "epoch 2 step 269 loss 0.11215716600418091\n",
            "epoch 2 step 270 loss 0.15020063519477844\n",
            "epoch 2 step 271 loss 0.09766984730958939\n",
            "epoch 2 step 272 loss 0.10353721678256989\n",
            "epoch 2 step 273 loss 0.06471012532711029\n",
            "epoch 2 step 274 loss 0.1087912917137146\n",
            "epoch 2 step 275 loss 0.08720650523900986\n",
            "epoch 2 step 276 loss 0.08553905785083771\n",
            "epoch 2 step 277 loss 0.08512111753225327\n",
            "epoch 2 step 278 loss 0.09050102531909943\n",
            "epoch 2 step 279 loss 0.10067445039749146\n",
            "epoch 2 step 280 loss 0.07511436939239502\n",
            "epoch 2 step 281 loss 0.11095576733350754\n",
            "epoch 2 step 282 loss 0.066815584897995\n",
            "epoch 2 step 283 loss 0.10451796650886536\n",
            "epoch 2 step 284 loss 0.09959002584218979\n",
            "epoch 2 step 285 loss 0.06467397511005402\n",
            "epoch 2 step 286 loss 0.0775347352027893\n",
            "epoch 2 step 287 loss 0.06199803575873375\n",
            "epoch 2 step 288 loss 0.16758058965206146\n",
            "epoch 2 step 289 loss 0.11978134512901306\n",
            "epoch 2 step 290 loss 0.11307133734226227\n",
            "epoch 2 step 291 loss 0.09744853526353836\n",
            "epoch 2 step 292 loss 0.10736909508705139\n",
            "epoch 2 step 293 loss 0.07932037115097046\n",
            "epoch 2 step 294 loss 0.06521904468536377\n",
            "epoch 2 step 295 loss 0.06505799293518066\n",
            "epoch 2 step 296 loss 0.08823101222515106\n",
            "epoch 2 step 297 loss 0.08167107403278351\n",
            "epoch 2 step 298 loss 0.10450901091098785\n",
            "epoch 2 step 299 loss 0.06450408697128296\n",
            "epoch 2 step 300 loss 0.08559757471084595\n",
            "epoch 2 step 301 loss 0.11626023054122925\n",
            "epoch 2 step 302 loss 0.133467435836792\n",
            "epoch 2 step 303 loss 0.0985405445098877\n",
            "epoch 2 step 304 loss 0.08855562657117844\n",
            "epoch 2 step 305 loss 0.08448797464370728\n",
            "epoch 2 step 306 loss 0.08841677755117416\n",
            "epoch 2 step 307 loss 0.12773612141609192\n",
            "epoch 2 step 308 loss 0.08426065742969513\n",
            "epoch 2 step 309 loss 0.11697650700807571\n",
            "epoch 2 step 310 loss 0.16776494681835175\n",
            "epoch 2 step 311 loss 0.12157822400331497\n",
            "epoch 2 step 312 loss 0.09240502119064331\n",
            "epoch 2 step 313 loss 0.09890808165073395\n",
            "epoch 2 step 314 loss 0.08193521201610565\n",
            "epoch 2 step 315 loss 0.12627510726451874\n",
            "epoch 2 step 316 loss 0.07732900232076645\n",
            "epoch 2 step 317 loss 0.10146688669919968\n",
            "epoch 2 step 318 loss 0.06077037751674652\n",
            "epoch 2 step 319 loss 0.12005309015512466\n",
            "epoch 2 step 320 loss 0.1249210461974144\n",
            "epoch 2 step 321 loss 0.08643880486488342\n",
            "epoch 2 step 322 loss 0.07374206930398941\n",
            "epoch 2 step 323 loss 0.08660914748907089\n",
            "epoch 2 step 324 loss 0.11427171528339386\n",
            "epoch 2 step 325 loss 0.0836237370967865\n",
            "epoch 2 step 326 loss 0.10581568628549576\n",
            "epoch 2 step 327 loss 0.11038234829902649\n",
            "epoch 2 step 328 loss 0.11499690264463425\n",
            "epoch 2 step 329 loss 0.10578043758869171\n",
            "epoch 2 step 330 loss 0.08172528445720673\n",
            "epoch 2 step 331 loss 0.06054272502660751\n",
            "epoch 2 step 332 loss 0.10643383860588074\n",
            "epoch 2 step 333 loss 0.11733938753604889\n",
            "epoch 2 step 334 loss 0.09228231757879257\n",
            "epoch 2 step 335 loss 0.10956668108701706\n",
            "epoch 2 step 336 loss 0.08319250494241714\n",
            "epoch 2 step 337 loss 0.07382427155971527\n",
            "epoch 2 step 338 loss 0.06140877306461334\n",
            "epoch 2 step 339 loss 0.06081023067235947\n",
            "epoch 2 step 340 loss 0.08360922336578369\n",
            "epoch 2 step 341 loss 0.09614101052284241\n",
            "epoch 2 step 342 loss 0.0750870555639267\n",
            "epoch 2 step 343 loss 0.13165655732154846\n",
            "epoch 2 step 344 loss 0.08196539431810379\n",
            "epoch 2 step 345 loss 0.14504116773605347\n",
            "epoch 2 step 346 loss 0.06947611272335052\n",
            "epoch 2 step 347 loss 0.0544716939330101\n",
            "epoch 2 step 348 loss 0.0826663225889206\n",
            "epoch 2 step 349 loss 0.09586858004331589\n",
            "epoch 2 step 350 loss 0.0694446712732315\n",
            "epoch 2 step 351 loss 0.0546974316239357\n",
            "epoch 2 step 352 loss 0.06505627930164337\n",
            "epoch 2 step 353 loss 0.05530925095081329\n",
            "epoch 2 step 354 loss 0.09463734924793243\n",
            "epoch 2 step 355 loss 0.09504489600658417\n",
            "epoch 2 step 356 loss 0.08955085277557373\n",
            "epoch 2 step 357 loss 0.10152068734169006\n",
            "epoch 2 step 358 loss 0.08984380960464478\n",
            "epoch 2 step 359 loss 0.10916637629270554\n",
            "epoch 2 step 360 loss 0.08494341373443604\n",
            "epoch 2 step 361 loss 0.09426957368850708\n",
            "epoch 2 step 362 loss 0.0788087248802185\n",
            "epoch 2 step 363 loss 0.09299183636903763\n",
            "epoch 2 step 364 loss 0.07716239988803864\n",
            "epoch 2 step 365 loss 0.10258883237838745\n",
            "epoch 2 step 366 loss 0.07416732609272003\n",
            "epoch 2 step 367 loss 0.1196211650967598\n",
            "epoch 2 step 368 loss 0.15794382989406586\n",
            "epoch 2 step 369 loss 0.06046242266893387\n",
            "epoch 2 step 370 loss 0.09872317314147949\n",
            "epoch 2 step 371 loss 0.17022348940372467\n",
            "epoch 2 step 372 loss 0.07074229419231415\n",
            "epoch 2 step 373 loss 0.09836381673812866\n",
            "epoch 2 step 374 loss 0.09036835283041\n",
            "epoch 2 step 375 loss 0.09655694663524628\n",
            "epoch 2 step 376 loss 0.07213684916496277\n",
            "epoch 2 step 377 loss 0.08290344476699829\n",
            "epoch 2 step 378 loss 0.09268788993358612\n",
            "epoch 2 step 379 loss 0.07389402389526367\n",
            "epoch 2 step 380 loss 0.12752729654312134\n",
            "epoch 2 step 381 loss 0.08844786882400513\n",
            "epoch 2 step 382 loss 0.09331335872411728\n",
            "epoch 2 step 383 loss 0.06771955639123917\n",
            "epoch 2 step 384 loss 0.06251081079244614\n",
            "epoch 2 step 385 loss 0.09212923049926758\n",
            "epoch 2 step 386 loss 0.09186190366744995\n",
            "epoch 2 step 387 loss 0.09030388295650482\n",
            "epoch 2 step 388 loss 0.09725797176361084\n",
            "epoch 2 step 389 loss 0.1019965261220932\n",
            "epoch 2 step 390 loss 0.10481531172990799\n",
            "epoch 2 step 391 loss 0.09218670427799225\n",
            "epoch 2 step 392 loss 0.09029172360897064\n",
            "epoch 2 step 393 loss 0.15748272836208344\n",
            "epoch 2 step 394 loss 0.09046868234872818\n",
            "epoch 2 step 395 loss 0.14119884371757507\n",
            "epoch 2 step 396 loss 0.07724285125732422\n",
            "epoch 2 step 397 loss 0.0726398229598999\n",
            "epoch 2 step 398 loss 0.1568872332572937\n",
            "epoch 2 step 399 loss 0.0665978416800499\n",
            "epoch 2 step 400 loss 0.0904720351099968\n",
            "epoch 2 step 401 loss 0.10600326955318451\n",
            "epoch 2 step 402 loss 0.14084070920944214\n",
            "epoch 2 step 403 loss 0.05819365009665489\n",
            "epoch 2 step 404 loss 0.11777640879154205\n",
            "epoch 2 step 405 loss 0.1303371787071228\n",
            "epoch 2 step 406 loss 0.10949496924877167\n",
            "epoch 2 step 407 loss 0.07038509100675583\n",
            "epoch 2 step 408 loss 0.11064882576465607\n",
            "epoch 2 step 409 loss 0.10589069873094559\n",
            "epoch 2 step 410 loss 0.09033431112766266\n",
            "epoch 2 step 411 loss 0.05016385018825531\n",
            "epoch 2 step 412 loss 0.11558135598897934\n",
            "epoch 2 step 413 loss 0.07748129963874817\n",
            "epoch 2 step 414 loss 0.09818239510059357\n",
            "epoch 2 step 415 loss 0.11115491390228271\n",
            "epoch 2 step 416 loss 0.06593924760818481\n",
            "epoch 2 step 417 loss 0.08399729430675507\n",
            "epoch 2 step 418 loss 0.09195975959300995\n",
            "epoch 2 step 419 loss 0.049121588468551636\n",
            "epoch 2 step 420 loss 0.10171647369861603\n",
            "epoch 2 step 421 loss 0.04966844618320465\n",
            "epoch 2 step 422 loss 0.08316609263420105\n",
            "epoch 2 step 423 loss 0.07690826058387756\n",
            "epoch 2 step 424 loss 0.07922683656215668\n",
            "epoch 2 step 425 loss 0.06999504566192627\n",
            "epoch 2 step 426 loss 0.06525339186191559\n",
            "epoch 2 step 427 loss 0.057841040194034576\n",
            "epoch 2 step 428 loss 0.07276251912117004\n",
            "epoch 2 step 429 loss 0.09128005057573318\n",
            "epoch 2 step 430 loss 0.08419378101825714\n",
            "epoch 2 step 431 loss 0.0761156976222992\n",
            "epoch 2 step 432 loss 0.08256885409355164\n",
            "epoch 2 step 433 loss 0.06965371966362\n",
            "epoch 2 step 434 loss 0.0925440862774849\n",
            "epoch 2 step 435 loss 0.08771377801895142\n",
            "epoch 2 step 436 loss 0.10788323730230331\n",
            "epoch 2 step 437 loss 0.0829494446516037\n",
            "epoch 2 step 438 loss 0.07967838644981384\n",
            "epoch 2 step 439 loss 0.09724405407905579\n",
            "epoch 2 step 440 loss 0.063844233751297\n",
            "epoch 2 step 441 loss 0.08407700061798096\n",
            "epoch 2 step 442 loss 0.12367638945579529\n",
            "epoch 2 step 443 loss 0.059897180646657944\n",
            "epoch 2 step 444 loss 0.06836096942424774\n",
            "epoch 2 step 445 loss 0.07380220293998718\n",
            "epoch 2 step 446 loss 0.07740629464387894\n",
            "epoch 2 step 447 loss 0.09200500696897507\n",
            "epoch 2 step 448 loss 0.11099142581224442\n",
            "epoch 2 step 449 loss 0.07697397470474243\n",
            "epoch 2 step 450 loss 0.06626540422439575\n",
            "epoch 2 step 451 loss 0.0653935894370079\n",
            "epoch 2 step 452 loss 0.08101394772529602\n",
            "epoch 2 step 453 loss 0.07632891833782196\n",
            "epoch 2 step 454 loss 0.10435004532337189\n",
            "epoch 2 step 455 loss 0.06099247187376022\n",
            "epoch 2 step 456 loss 0.0788719579577446\n",
            "epoch 2 step 457 loss 0.08777876943349838\n",
            "epoch 2 step 458 loss 0.07051413506269455\n",
            "epoch 2 step 459 loss 0.09214595705270767\n",
            "epoch 2 step 460 loss 0.07797642052173615\n",
            "epoch 2 step 461 loss 0.08615794032812119\n",
            "epoch 2 step 462 loss 0.0836886465549469\n",
            "epoch 2 step 463 loss 0.07273130863904953\n",
            "epoch 2 step 464 loss 0.09357917308807373\n",
            "epoch 2 step 465 loss 0.09996075928211212\n",
            "epoch 2 step 466 loss 0.07702212780714035\n",
            "epoch 2 step 467 loss 0.06507444381713867\n",
            "epoch 2 step 468 loss 0.08294785767793655\n",
            "epoch 2 step 469 loss 0.1010381430387497\n",
            "epoch 2 step 470 loss 0.0480320006608963\n",
            "epoch 2 step 471 loss 0.06527484953403473\n",
            "epoch 2 step 472 loss 0.0756634771823883\n",
            "epoch 2 step 473 loss 0.05601499602198601\n",
            "epoch 2 step 474 loss 0.08422234654426575\n",
            "epoch 2 step 475 loss 0.0983651727437973\n",
            "epoch 2 step 476 loss 0.08713915944099426\n",
            "epoch 2 step 477 loss 0.09862291067838669\n",
            "epoch 2 step 478 loss 0.08067493885755539\n",
            "epoch 2 step 479 loss 0.10766316205263138\n",
            "epoch 2 step 480 loss 0.09455642849206924\n",
            "epoch 2 step 481 loss 0.08296936750411987\n",
            "epoch 2 step 482 loss 0.07418254017829895\n",
            "epoch 2 step 483 loss 0.12062270939350128\n",
            "epoch 2 step 484 loss 0.09356270730495453\n",
            "epoch 2 step 485 loss 0.10018247365951538\n",
            "epoch 2 step 486 loss 0.08509020507335663\n",
            "epoch 2 step 487 loss 0.10297331213951111\n",
            "epoch 2 step 488 loss 0.059708207845687866\n",
            "epoch 2 step 489 loss 0.05991043150424957\n",
            "epoch 2 step 490 loss 0.0899314284324646\n",
            "epoch 2 step 491 loss 0.06901521235704422\n",
            "epoch 2 step 492 loss 0.09084011614322662\n",
            "epoch 2 step 493 loss 0.05064964294433594\n",
            "epoch 2 step 494 loss 0.05983928591012955\n",
            "epoch 2 step 495 loss 0.11226797103881836\n",
            "epoch 2 step 496 loss 0.12723365426063538\n",
            "epoch 2 step 497 loss 0.16591310501098633\n",
            "epoch 2 step 498 loss 0.07312016934156418\n",
            "epoch 2 step 499 loss 0.06611335277557373\n",
            "epoch 2 step 500 loss 0.1295069456100464\n",
            "epoch 2 step 501 loss 0.10834401100873947\n",
            "epoch 2 step 502 loss 0.09967769682407379\n",
            "epoch 2 step 503 loss 0.1087414026260376\n",
            "epoch 2 step 504 loss 0.09104390442371368\n",
            "epoch 2 step 505 loss 0.07923934608697891\n",
            "epoch 2 step 506 loss 0.07378041744232178\n",
            "epoch 2 step 507 loss 0.07960424572229385\n",
            "epoch 2 step 508 loss 0.13577254116535187\n",
            "epoch 2 step 509 loss 0.10456222295761108\n",
            "epoch 2 step 510 loss 0.06375493854284286\n",
            "epoch 2 step 511 loss 0.0830056294798851\n",
            "epoch 2 step 512 loss 0.13379937410354614\n",
            "epoch 2 step 513 loss 0.08141423016786575\n",
            "epoch 2 step 514 loss 0.11279164999723434\n",
            "epoch 2 step 515 loss 0.09038174152374268\n",
            "epoch 2 step 516 loss 0.0671064481139183\n",
            "epoch 2 step 517 loss 0.0977543443441391\n",
            "epoch 2 step 518 loss 0.15926378965377808\n",
            "epoch 2 step 519 loss 0.10967634618282318\n",
            "epoch 2 step 520 loss 0.0775521844625473\n",
            "epoch 2 step 521 loss 0.08438834547996521\n",
            "epoch 2 step 522 loss 0.20421940088272095\n",
            "epoch 2 step 523 loss 0.08229002356529236\n",
            "epoch 2 step 524 loss 0.15740302205085754\n",
            "epoch 2 step 525 loss 0.12772852182388306\n",
            "epoch 2 step 526 loss 0.14232301712036133\n",
            "epoch 2 step 527 loss 0.10790322721004486\n",
            "epoch 2 step 528 loss 0.12550362944602966\n",
            "epoch 2 step 529 loss 0.06398800015449524\n",
            "epoch 2 step 530 loss 0.08279059827327728\n",
            "epoch 2 step 531 loss 0.06971818208694458\n",
            "epoch 2 step 532 loss 0.1325637400150299\n",
            "epoch 2 step 533 loss 0.1350257843732834\n",
            "epoch 2 step 534 loss 0.08861629664897919\n",
            "epoch 2 step 535 loss 0.10301147401332855\n",
            "epoch 2 step 536 loss 0.11185070872306824\n",
            "epoch 2 step 537 loss 0.1280103325843811\n",
            "epoch 2 step 538 loss 0.12881198525428772\n",
            "epoch 2 step 539 loss 0.1374453604221344\n",
            "epoch 2 step 540 loss 0.0786353349685669\n",
            "epoch 2 step 541 loss 0.11739124357700348\n",
            "epoch 2 step 542 loss 0.0957600548863411\n",
            "epoch 2 step 543 loss 0.11113258451223373\n",
            "epoch 2 step 544 loss 0.09299654513597488\n",
            "epoch 2 step 545 loss 0.08851240575313568\n",
            "epoch 2 step 546 loss 0.14261139929294586\n",
            "epoch 2 step 547 loss 0.0638372153043747\n",
            "epoch 2 step 548 loss 0.10199782997369766\n",
            "epoch 2 step 549 loss 0.09582357108592987\n",
            "epoch 2 step 550 loss 0.10732076317071915\n",
            "epoch 2 step 551 loss 0.08811597526073456\n",
            "epoch 2 step 552 loss 0.06690986454486847\n",
            "epoch 2 step 553 loss 0.05564697086811066\n",
            "epoch 2 step 554 loss 0.05753094702959061\n",
            "epoch 2 step 555 loss 0.06860049813985825\n",
            "epoch 2 step 556 loss 0.09889039397239685\n",
            "epoch 2 step 557 loss 0.09930113703012466\n",
            "epoch 2 step 558 loss 0.11399325728416443\n",
            "epoch 2 step 559 loss 0.11323799937963486\n",
            "epoch 2 step 560 loss 0.11901232600212097\n",
            "epoch 2 step 561 loss 0.0740353912115097\n",
            "epoch 2 step 562 loss 0.08315568417310715\n",
            "epoch 2 step 563 loss 0.10991926491260529\n",
            "epoch 2 step 564 loss 0.07881386578083038\n",
            "epoch 2 step 565 loss 0.10864870250225067\n",
            "epoch 2 step 566 loss 0.15425580739974976\n",
            "epoch 2 step 567 loss 0.12889990210533142\n",
            "epoch 2 step 568 loss 0.07174181193113327\n",
            "epoch 2 step 569 loss 0.0708901584148407\n",
            "epoch 2 step 570 loss 0.0906432569026947\n",
            "epoch 2 step 571 loss 0.12563318014144897\n",
            "epoch 2 step 572 loss 0.08193349093198776\n",
            "epoch 2 step 573 loss 0.07552362978458405\n",
            "epoch 2 step 574 loss 0.08627257496118546\n",
            "epoch 2 step 575 loss 0.06658060848712921\n",
            "epoch 2 step 576 loss 0.11688867211341858\n",
            "epoch 2 step 577 loss 0.09511483460664749\n",
            "epoch 2 step 578 loss 0.07103455066680908\n",
            "epoch 2 step 579 loss 0.10325191169977188\n",
            "epoch 2 step 580 loss 0.09584522992372513\n",
            "epoch 2 step 581 loss 0.1363358497619629\n",
            "epoch 2 step 582 loss 0.10847204923629761\n",
            "epoch 2 step 583 loss 0.08929242193698883\n",
            "epoch 2 step 584 loss 0.12142190337181091\n",
            "epoch 2 step 585 loss 0.1672828495502472\n",
            "epoch 2 step 586 loss 0.1600050926208496\n",
            "epoch 2 step 587 loss 0.20292457938194275\n",
            "epoch 2 step 588 loss 0.1152908056974411\n",
            "epoch 2 step 589 loss 0.08770839869976044\n",
            "epoch 2 step 590 loss 0.1700390875339508\n",
            "epoch 2 step 591 loss 0.08101043105125427\n",
            "epoch 2 step 592 loss 0.16596344113349915\n",
            "epoch 2 step 593 loss 0.08245565742254257\n",
            "epoch 2 step 594 loss 0.10418139398097992\n",
            "epoch 2 step 595 loss 0.10327953845262527\n",
            "epoch 2 step 596 loss 0.14644311368465424\n",
            "epoch 2 step 597 loss 0.1644209325313568\n",
            "epoch 2 step 598 loss 0.08917064964771271\n",
            "epoch 2 step 599 loss 0.11159857362508774\n",
            "epoch 2 step 600 loss 0.08965715765953064\n",
            "epoch 2 step 601 loss 0.1064520999789238\n",
            "epoch 2 step 602 loss 0.10647664219141006\n",
            "epoch 2 step 603 loss 0.10561619699001312\n",
            "epoch 2 step 604 loss 0.10751791298389435\n",
            "epoch 2 step 605 loss 0.08688864856958389\n",
            "epoch 2 step 606 loss 0.08184711635112762\n",
            "epoch 2 step 607 loss 0.08154815435409546\n",
            "epoch 2 step 608 loss 0.0703841894865036\n",
            "epoch 2 step 609 loss 0.10599485784769058\n",
            "epoch 2 step 610 loss 0.08985045552253723\n",
            "epoch 2 step 611 loss 0.08642178773880005\n",
            "epoch 2 step 612 loss 0.1031857579946518\n",
            "epoch 2 step 613 loss 0.06863169372081757\n",
            "epoch 2 step 614 loss 0.08208435773849487\n",
            "epoch 2 step 615 loss 0.079022616147995\n",
            "epoch 2 step 616 loss 0.15328849852085114\n",
            "epoch 2 step 617 loss 0.13625448942184448\n",
            "epoch 2 step 618 loss 0.1299968957901001\n",
            "epoch 2 step 619 loss 0.07174757122993469\n",
            "epoch 2 step 620 loss 0.05529516190290451\n",
            "epoch 2 step 621 loss 0.10419797897338867\n",
            "epoch 2 step 622 loss 0.10625475645065308\n",
            "epoch 2 step 623 loss 0.06168987601995468\n",
            "epoch 2 step 624 loss 0.0739017203450203\n",
            "epoch 2 step 625 loss 0.10753633081912994\n",
            "epoch 2 step 626 loss 0.1150856465101242\n",
            "epoch 2 step 627 loss 0.12094878405332565\n",
            "epoch 2 step 628 loss 0.08792035281658173\n",
            "epoch 2 step 629 loss 0.08888666331768036\n",
            "epoch 2 step 630 loss 0.11737275868654251\n",
            "epoch 2 step 631 loss 0.10245555639266968\n",
            "epoch 2 step 632 loss 0.06394276022911072\n",
            "epoch 2 step 633 loss 0.11008667945861816\n",
            "epoch 2 step 634 loss 0.05008843168616295\n",
            "epoch 2 step 635 loss 0.06592515110969543\n",
            "epoch 2 step 636 loss 0.062073491513729095\n",
            "epoch 2 step 637 loss 0.09047021716833115\n",
            "epoch 2 step 638 loss 0.09052372723817825\n",
            "epoch 2 step 639 loss 0.05869744345545769\n",
            "epoch 2 step 640 loss 0.12807519733905792\n",
            "epoch 2 step 641 loss 0.07369036972522736\n",
            "epoch 2 step 642 loss 0.07346609234809875\n",
            "epoch 2 step 643 loss 0.0717184841632843\n",
            "epoch 2 step 644 loss 0.08897995203733444\n",
            "epoch 2 step 645 loss 0.06802061200141907\n",
            "epoch 2 step 646 loss 0.1050790324807167\n",
            "epoch 2 step 647 loss 0.033392034471035004\n",
            "epoch 2 step 648 loss 0.10334517061710358\n",
            "epoch 2 step 649 loss 0.06675087660551071\n",
            "epoch 2 step 650 loss 0.057810597121715546\n",
            "epoch 2 step 651 loss 0.07351219654083252\n",
            "epoch 2 step 652 loss 0.0607011616230011\n",
            "epoch 2 step 653 loss 0.09670023620128632\n",
            "epoch 2 step 654 loss 0.04829045385122299\n",
            "epoch 2 step 655 loss 0.07414968311786652\n",
            "epoch 2 step 656 loss 0.09480483829975128\n",
            "epoch 2 step 657 loss 0.07308773696422577\n",
            "epoch 2 step 658 loss 0.08821210265159607\n",
            "epoch 2 step 659 loss 0.1586177498102188\n",
            "epoch 2 step 660 loss 0.07096445560455322\n",
            "epoch 2 step 661 loss 0.10296432673931122\n",
            "epoch 2 step 662 loss 0.08607538044452667\n",
            "epoch 2 step 663 loss 0.08461011946201324\n",
            "epoch 2 step 664 loss 0.07368672639131546\n",
            "epoch 2 step 665 loss 0.07964769005775452\n",
            "epoch 2 step 666 loss 0.040580011904239655\n",
            "epoch 2 step 667 loss 0.08189552277326584\n",
            "epoch 2 step 668 loss 0.08227280527353287\n",
            "epoch 2 step 669 loss 0.06654022634029388\n",
            "epoch 2 step 670 loss 0.08071836829185486\n",
            "epoch 2 step 671 loss 0.18412721157073975\n",
            "epoch 2 step 672 loss 0.11597020924091339\n",
            "epoch 2 step 673 loss 0.07011295855045319\n",
            "epoch 2 step 674 loss 0.10883563756942749\n",
            "epoch 2 step 675 loss 0.12404095381498337\n",
            "epoch 2 step 676 loss 0.0586153119802475\n",
            "epoch 2 step 677 loss 0.06917660683393478\n",
            "epoch 2 step 678 loss 0.07279688119888306\n",
            "epoch 2 step 679 loss 0.11175398528575897\n",
            "epoch 2 step 680 loss 0.1103711649775505\n",
            "epoch 2 step 681 loss 0.0833057165145874\n",
            "epoch 2 step 682 loss 0.08694782853126526\n",
            "epoch 2 step 683 loss 0.16481517255306244\n",
            "epoch 2 step 684 loss 0.08558700233697891\n",
            "epoch 2 step 685 loss 0.09192170947790146\n",
            "epoch 2 step 686 loss 0.0711413100361824\n",
            "epoch 2 step 687 loss 0.08996100723743439\n",
            "epoch 2 step 688 loss 0.07697834819555283\n",
            "epoch 2 step 689 loss 0.05880521982908249\n",
            "epoch 2 step 690 loss 0.0827048197388649\n",
            "epoch 2 step 691 loss 0.12988190352916718\n",
            "epoch 2 step 692 loss 0.11986047029495239\n",
            "epoch 2 step 693 loss 0.13891631364822388\n",
            "epoch 2 step 694 loss 0.12028070539236069\n",
            "epoch 2 step 695 loss 0.11062072962522507\n",
            "epoch 2 step 696 loss 0.05214995518326759\n",
            "epoch 2 step 697 loss 0.05707583576440811\n",
            "epoch 2 step 698 loss 0.07406307756900787\n",
            "epoch 2 step 699 loss 0.1061616763472557\n",
            "epoch 2 step 700 loss 0.09261509031057358\n",
            "epoch 2 step 701 loss 0.08213073015213013\n",
            "epoch 2 step 702 loss 0.12519490718841553\n",
            "epoch 2 step 703 loss 0.11029775440692902\n",
            "epoch 2 step 704 loss 0.10375892370939255\n",
            "epoch 2 step 705 loss 0.1331596076488495\n",
            "epoch 2 step 706 loss 0.08327373117208481\n",
            "epoch 2 step 707 loss 0.09242671728134155\n",
            "epoch 2 step 708 loss 0.0844980925321579\n",
            "epoch 2 step 709 loss 0.08456465601921082\n",
            "epoch 2 step 710 loss 0.06665372848510742\n",
            "epoch 2 step 711 loss 0.11613909900188446\n",
            "epoch 2 step 712 loss 0.12253901362419128\n",
            "epoch 2 step 713 loss 0.06924961507320404\n",
            "epoch 2 step 714 loss 0.10098496824502945\n",
            "epoch 2 step 715 loss 0.10724647343158722\n",
            "epoch 2 step 716 loss 0.0954161286354065\n",
            "epoch 2 step 717 loss 0.06959807872772217\n",
            "epoch 2 step 718 loss 0.08436398208141327\n",
            "epoch 2 step 719 loss 0.10238023102283478\n",
            "epoch 2 step 720 loss 0.102736696600914\n",
            "epoch 2 step 721 loss 0.07767477631568909\n",
            "epoch 2 step 722 loss 0.1268339604139328\n",
            "epoch 2 step 723 loss 0.08979614078998566\n",
            "epoch 2 step 724 loss 0.10632999241352081\n",
            "epoch 2 step 725 loss 0.09708644449710846\n",
            "epoch 2 step 726 loss 0.1265660524368286\n",
            "epoch 2 step 727 loss 0.08123714476823807\n",
            "epoch 2 step 728 loss 0.10873810201883316\n",
            "epoch 2 step 729 loss 0.10196506977081299\n",
            "epoch 2 step 730 loss 0.07332583516836166\n",
            "epoch 2 step 731 loss 0.08537328243255615\n",
            "epoch 2 step 732 loss 0.10747440159320831\n",
            "epoch 2 step 733 loss 0.06459309160709381\n",
            "epoch 2 step 734 loss 0.07153300940990448\n",
            "epoch 2 step 735 loss 0.07569767534732819\n",
            "epoch 2 step 736 loss 0.08276471495628357\n",
            "epoch 2 step 737 loss 0.09846214950084686\n",
            "epoch 2 step 738 loss 0.04924728721380234\n",
            "epoch 2 step 739 loss 0.08477559685707092\n",
            "epoch 2 step 740 loss 0.11314959824085236\n",
            "epoch 2 step 741 loss 0.04716583713889122\n",
            "epoch 2 step 742 loss 0.06861995160579681\n",
            "epoch 2 step 743 loss 0.09217537939548492\n",
            "epoch 2 step 744 loss 0.08330165594816208\n",
            "epoch 2 step 745 loss 0.06522222608327866\n",
            "epoch 2 step 746 loss 0.11371113359928131\n",
            "epoch 2 step 747 loss 0.07598643004894257\n",
            "epoch 2 step 748 loss 0.0847187265753746\n",
            "epoch 2 step 749 loss 0.07866202294826508\n",
            "epoch 2 step 750 loss 0.09625758230686188\n",
            "epoch 2 step 751 loss 0.09297189861536026\n",
            "epoch 2 step 752 loss 0.1092464029788971\n",
            "epoch 2 step 753 loss 0.13287124037742615\n",
            "epoch 2 step 754 loss 0.08295033127069473\n",
            "epoch 2 step 755 loss 0.07615485042333603\n",
            "epoch 2 step 756 loss 0.03803861513733864\n",
            "epoch 2 step 757 loss 0.06729742884635925\n",
            "epoch 2 step 758 loss 0.07741673290729523\n",
            "epoch 2 step 759 loss 0.09149159491062164\n",
            "epoch 2 step 760 loss 0.1330009400844574\n",
            "epoch 2 step 761 loss 0.08770383894443512\n",
            "epoch 2 step 762 loss 0.07774466276168823\n",
            "epoch 2 step 763 loss 0.13098423182964325\n",
            "epoch 2 step 764 loss 0.08760946989059448\n",
            "epoch 2 step 765 loss 0.06785287708044052\n",
            "epoch 2 step 766 loss 0.10366297513246536\n",
            "epoch 2 step 767 loss 0.07871412485837936\n",
            "epoch 2 step 768 loss 0.06672413647174835\n",
            "epoch 2 step 769 loss 0.1011705994606018\n",
            "epoch 2 step 770 loss 0.08820738643407822\n",
            "epoch 2 step 771 loss 0.052632734179496765\n",
            "epoch 2 step 772 loss 0.13422037661075592\n",
            "epoch 2 step 773 loss 0.05220118165016174\n",
            "epoch 2 step 774 loss 0.04986605793237686\n",
            "epoch 2 step 775 loss 0.08091600239276886\n",
            "epoch 2 step 776 loss 0.0594136081635952\n",
            "epoch 2 step 777 loss 0.08860516548156738\n",
            "epoch 2 step 778 loss 0.10174186527729034\n",
            "epoch 2 step 779 loss 0.07206912338733673\n",
            "epoch 2 step 780 loss 0.08011148869991302\n",
            "epoch 2 step 781 loss 0.05206890404224396\n",
            "epoch 3 step 0 loss 0.07871118932962418\n",
            "epoch 3 step 1 loss 0.057437021285295486\n",
            "epoch 3 step 2 loss 0.11838795989751816\n",
            "epoch 3 step 3 loss 0.06891681998968124\n",
            "epoch 3 step 4 loss 0.09141955524682999\n",
            "epoch 3 step 5 loss 0.08481153845787048\n",
            "epoch 3 step 6 loss 0.06789474189281464\n",
            "epoch 3 step 7 loss 0.10512819141149521\n",
            "epoch 3 step 8 loss 0.0987621545791626\n",
            "epoch 3 step 9 loss 0.07966793328523636\n",
            "epoch 3 step 10 loss 0.10016830265522003\n",
            "epoch 3 step 11 loss 0.07828071713447571\n",
            "epoch 3 step 12 loss 0.07454103976488113\n",
            "epoch 3 step 13 loss 0.05828572064638138\n",
            "epoch 3 step 14 loss 0.08236657083034515\n",
            "epoch 3 step 15 loss 0.07927364110946655\n",
            "epoch 3 step 16 loss 0.0635673999786377\n",
            "epoch 3 step 17 loss 0.08387330919504166\n",
            "epoch 3 step 18 loss 0.13585339486598969\n",
            "epoch 3 step 19 loss 0.07805737853050232\n",
            "epoch 3 step 20 loss 0.15128998458385468\n",
            "epoch 3 step 21 loss 0.08784142881631851\n",
            "epoch 3 step 22 loss 0.1214199811220169\n",
            "epoch 3 step 23 loss 0.06619749218225479\n",
            "epoch 3 step 24 loss 0.11679191887378693\n",
            "epoch 3 step 25 loss 0.11207135021686554\n",
            "epoch 3 step 26 loss 0.08074601739645004\n",
            "epoch 3 step 27 loss 0.09300670772790909\n",
            "epoch 3 step 28 loss 0.06436584144830704\n",
            "epoch 3 step 29 loss 0.09898313134908676\n",
            "epoch 3 step 30 loss 0.08908292651176453\n",
            "epoch 3 step 31 loss 0.05489993840456009\n",
            "epoch 3 step 32 loss 0.11673559993505478\n",
            "epoch 3 step 33 loss 0.16808508336544037\n",
            "epoch 3 step 34 loss 0.06058173626661301\n",
            "epoch 3 step 35 loss 0.08502192795276642\n",
            "epoch 3 step 36 loss 0.09462130069732666\n",
            "epoch 3 step 37 loss 0.06853701919317245\n",
            "epoch 3 step 38 loss 0.09261929243803024\n",
            "epoch 3 step 39 loss 0.1041431576013565\n",
            "epoch 3 step 40 loss 0.10385185480117798\n",
            "epoch 3 step 41 loss 0.05520278587937355\n",
            "epoch 3 step 42 loss 0.10157895088195801\n",
            "epoch 3 step 43 loss 0.0680544376373291\n",
            "epoch 3 step 44 loss 0.14598752558231354\n",
            "epoch 3 step 45 loss 0.0816115140914917\n",
            "epoch 3 step 46 loss 0.07976474612951279\n",
            "epoch 3 step 47 loss 0.1163819432258606\n",
            "epoch 3 step 48 loss 0.09283636510372162\n",
            "epoch 3 step 49 loss 0.13664786517620087\n",
            "epoch 3 step 50 loss 0.1239963173866272\n",
            "epoch 3 step 51 loss 0.08888210356235504\n",
            "epoch 3 step 52 loss 0.08150840550661087\n",
            "epoch 3 step 53 loss 0.08434206992387772\n",
            "epoch 3 step 54 loss 0.09342128038406372\n",
            "epoch 3 step 55 loss 0.07187558710575104\n",
            "epoch 3 step 56 loss 0.06041261553764343\n",
            "epoch 3 step 57 loss 0.060996413230895996\n",
            "epoch 3 step 58 loss 0.07637487351894379\n",
            "epoch 3 step 59 loss 0.07461739331483841\n",
            "epoch 3 step 60 loss 0.06715022027492523\n",
            "epoch 3 step 61 loss 0.08611717075109482\n",
            "epoch 3 step 62 loss 0.07881003618240356\n",
            "epoch 3 step 63 loss 0.08273527026176453\n",
            "epoch 3 step 64 loss 0.06743302941322327\n",
            "epoch 3 step 65 loss 0.10393194854259491\n",
            "epoch 3 step 66 loss 0.06408066302537918\n",
            "epoch 3 step 67 loss 0.09474236518144608\n",
            "epoch 3 step 68 loss 0.09522533416748047\n",
            "epoch 3 step 69 loss 0.09958742558956146\n",
            "epoch 3 step 70 loss 0.07299843430519104\n",
            "epoch 3 step 71 loss 0.031961385160684586\n",
            "epoch 3 step 72 loss 0.07777796685695648\n",
            "epoch 3 step 73 loss 0.05662785470485687\n",
            "epoch 3 step 74 loss 0.06854899227619171\n",
            "epoch 3 step 75 loss 0.06102369353175163\n",
            "epoch 3 step 76 loss 0.08938200026750565\n",
            "epoch 3 step 77 loss 0.06524362415075302\n",
            "epoch 3 step 78 loss 0.05954351648688316\n",
            "epoch 3 step 79 loss 0.0858132615685463\n",
            "epoch 3 step 80 loss 0.07979948818683624\n",
            "epoch 3 step 81 loss 0.09428776055574417\n",
            "epoch 3 step 82 loss 0.06257061660289764\n",
            "epoch 3 step 83 loss 0.11021304130554199\n",
            "epoch 3 step 84 loss 0.04433387145400047\n",
            "epoch 3 step 85 loss 0.11427663266658783\n",
            "epoch 3 step 86 loss 0.09914842247962952\n",
            "epoch 3 step 87 loss 0.06553204357624054\n",
            "epoch 3 step 88 loss 0.1051977276802063\n",
            "epoch 3 step 89 loss 0.061367832124233246\n",
            "epoch 3 step 90 loss 0.08986003696918488\n",
            "epoch 3 step 91 loss 0.08012089133262634\n",
            "epoch 3 step 92 loss 0.07117226719856262\n",
            "epoch 3 step 93 loss 0.10389424115419388\n",
            "epoch 3 step 94 loss 0.06479522585868835\n",
            "epoch 3 step 95 loss 0.0907365009188652\n",
            "epoch 3 step 96 loss 0.06762152165174484\n",
            "epoch 3 step 97 loss 0.14150270819664001\n",
            "epoch 3 step 98 loss 0.07286084443330765\n",
            "epoch 3 step 99 loss 0.08159919083118439\n",
            "epoch 3 step 100 loss 0.08500546216964722\n",
            "epoch 3 step 101 loss 0.08837170898914337\n",
            "epoch 3 step 102 loss 0.07019923627376556\n",
            "epoch 3 step 103 loss 0.0826990082859993\n",
            "epoch 3 step 104 loss 0.08840615302324295\n",
            "epoch 3 step 105 loss 0.06018470227718353\n",
            "epoch 3 step 106 loss 0.10218685120344162\n",
            "epoch 3 step 107 loss 0.13082340359687805\n",
            "epoch 3 step 108 loss 0.07445818185806274\n",
            "epoch 3 step 109 loss 0.11582302302122116\n",
            "epoch 3 step 110 loss 0.08417841792106628\n",
            "epoch 3 step 111 loss 0.08460438251495361\n",
            "epoch 3 step 112 loss 0.09715049713850021\n",
            "epoch 3 step 113 loss 0.11329614371061325\n",
            "epoch 3 step 114 loss 0.06689467281103134\n",
            "epoch 3 step 115 loss 0.09248210489749908\n",
            "epoch 3 step 116 loss 0.08733139932155609\n",
            "epoch 3 step 117 loss 0.09594942629337311\n",
            "epoch 3 step 118 loss 0.06699652224779129\n",
            "epoch 3 step 119 loss 0.08346305787563324\n",
            "epoch 3 step 120 loss 0.081075020134449\n",
            "epoch 3 step 121 loss 0.08258777856826782\n",
            "epoch 3 step 122 loss 0.1302097737789154\n",
            "epoch 3 step 123 loss 0.0976441353559494\n",
            "epoch 3 step 124 loss 0.08288826048374176\n",
            "epoch 3 step 125 loss 0.07415080070495605\n",
            "epoch 3 step 126 loss 0.059966325759887695\n",
            "epoch 3 step 127 loss 0.061186134815216064\n",
            "epoch 3 step 128 loss 0.05692735314369202\n",
            "epoch 3 step 129 loss 0.06987745314836502\n",
            "epoch 3 step 130 loss 0.05374264717102051\n",
            "epoch 3 step 131 loss 0.09352417290210724\n",
            "epoch 3 step 132 loss 0.05285066366195679\n",
            "epoch 3 step 133 loss 0.05945007875561714\n",
            "epoch 3 step 134 loss 0.05926221236586571\n",
            "epoch 3 step 135 loss 0.07725775241851807\n",
            "epoch 3 step 136 loss 0.05654064565896988\n",
            "epoch 3 step 137 loss 0.12090107053518295\n",
            "epoch 3 step 138 loss 0.12913723289966583\n",
            "epoch 3 step 139 loss 0.09090688079595566\n",
            "epoch 3 step 140 loss 0.09206836670637131\n",
            "epoch 3 step 141 loss 0.0832175463438034\n",
            "epoch 3 step 142 loss 0.0442676767706871\n",
            "epoch 3 step 143 loss 0.04744734615087509\n",
            "epoch 3 step 144 loss 0.08508164435625076\n",
            "epoch 3 step 145 loss 0.09750483185052872\n",
            "epoch 3 step 146 loss 0.07620179653167725\n",
            "epoch 3 step 147 loss 0.09120988845825195\n",
            "epoch 3 step 148 loss 0.07538177073001862\n",
            "epoch 3 step 149 loss 0.052899070084095\n",
            "epoch 3 step 150 loss 0.10184389352798462\n",
            "epoch 3 step 151 loss 0.07240109890699387\n",
            "epoch 3 step 152 loss 0.0995156541466713\n",
            "epoch 3 step 153 loss 0.06725925207138062\n",
            "epoch 3 step 154 loss 0.09385217726230621\n",
            "epoch 3 step 155 loss 0.09084486961364746\n",
            "epoch 3 step 156 loss 0.0662284642457962\n",
            "epoch 3 step 157 loss 0.05686097964644432\n",
            "epoch 3 step 158 loss 0.050973568111658096\n",
            "epoch 3 step 159 loss 0.08416962623596191\n",
            "epoch 3 step 160 loss 0.07215101271867752\n",
            "epoch 3 step 161 loss 0.0889485776424408\n",
            "epoch 3 step 162 loss 0.1053929477930069\n",
            "epoch 3 step 163 loss 0.08255061507225037\n",
            "epoch 3 step 164 loss 0.08502286672592163\n",
            "epoch 3 step 165 loss 0.09169016778469086\n",
            "epoch 3 step 166 loss 0.08573663234710693\n",
            "epoch 3 step 167 loss 0.06028839573264122\n",
            "epoch 3 step 168 loss 0.05891554057598114\n",
            "epoch 3 step 169 loss 0.09205737709999084\n",
            "epoch 3 step 170 loss 0.07437387853860855\n",
            "epoch 3 step 171 loss 0.1033116951584816\n",
            "epoch 3 step 172 loss 0.05884166434407234\n",
            "epoch 3 step 173 loss 0.0660717710852623\n",
            "epoch 3 step 174 loss 0.05232030153274536\n",
            "epoch 3 step 175 loss 0.07933491468429565\n",
            "epoch 3 step 176 loss 0.0701824352145195\n",
            "epoch 3 step 177 loss 0.06869928538799286\n",
            "epoch 3 step 178 loss 0.04625208303332329\n",
            "epoch 3 step 179 loss 0.08308382332324982\n",
            "epoch 3 step 180 loss 0.0784885361790657\n",
            "epoch 3 step 181 loss 0.05932626873254776\n",
            "epoch 3 step 182 loss 0.07646395266056061\n",
            "epoch 3 step 183 loss 0.12064237147569656\n",
            "epoch 3 step 184 loss 0.06468284130096436\n",
            "epoch 3 step 185 loss 0.09259708225727081\n",
            "epoch 3 step 186 loss 0.1528725028038025\n",
            "epoch 3 step 187 loss 0.07909347116947174\n",
            "epoch 3 step 188 loss 0.07691536098718643\n",
            "epoch 3 step 189 loss 0.13522475957870483\n",
            "epoch 3 step 190 loss 0.10192450881004333\n",
            "epoch 3 step 191 loss 0.1078617051243782\n",
            "epoch 3 step 192 loss 0.1277112364768982\n",
            "epoch 3 step 193 loss 0.10921349376440048\n",
            "epoch 3 step 194 loss 0.06186854466795921\n",
            "epoch 3 step 195 loss 0.05109485238790512\n",
            "epoch 3 step 196 loss 0.09666187316179276\n",
            "epoch 3 step 197 loss 0.13545212149620056\n",
            "epoch 3 step 198 loss 0.07158041000366211\n",
            "epoch 3 step 199 loss 0.09787211567163467\n",
            "epoch 3 step 200 loss 0.0648624449968338\n",
            "epoch 3 step 201 loss 0.07343211770057678\n",
            "epoch 3 step 202 loss 0.10064034163951874\n",
            "epoch 3 step 203 loss 0.06377912312746048\n",
            "epoch 3 step 204 loss 0.09729976952075958\n",
            "epoch 3 step 205 loss 0.09721634536981583\n",
            "epoch 3 step 206 loss 0.07948818057775497\n",
            "epoch 3 step 207 loss 0.05923308804631233\n",
            "epoch 3 step 208 loss 0.05025266483426094\n",
            "epoch 3 step 209 loss 0.08089698106050491\n",
            "epoch 3 step 210 loss 0.08588355779647827\n",
            "epoch 3 step 211 loss 0.14682705700397491\n",
            "epoch 3 step 212 loss 0.07138168066740036\n",
            "epoch 3 step 213 loss 0.12175677716732025\n",
            "epoch 3 step 214 loss 0.11791129410266876\n",
            "epoch 3 step 215 loss 0.07113660126924515\n",
            "epoch 3 step 216 loss 0.10051310807466507\n",
            "epoch 3 step 217 loss 0.0800243467092514\n",
            "epoch 3 step 218 loss 0.08058249205350876\n",
            "epoch 3 step 219 loss 0.08684093505144119\n",
            "epoch 3 step 220 loss 0.071683369576931\n",
            "epoch 3 step 221 loss 0.07690548151731491\n",
            "epoch 3 step 222 loss 0.06709997355937958\n",
            "epoch 3 step 223 loss 0.13155736029148102\n",
            "epoch 3 step 224 loss 0.04059067368507385\n",
            "epoch 3 step 225 loss 0.11578284949064255\n",
            "epoch 3 step 226 loss 0.0884079709649086\n",
            "epoch 3 step 227 loss 0.050009191036224365\n",
            "epoch 3 step 228 loss 0.08678770810365677\n",
            "epoch 3 step 229 loss 0.1092686802148819\n",
            "epoch 3 step 230 loss 0.04290243238210678\n",
            "epoch 3 step 231 loss 0.10716941207647324\n",
            "epoch 3 step 232 loss 0.09825102984905243\n",
            "epoch 3 step 233 loss 0.09222434461116791\n",
            "epoch 3 step 234 loss 0.09329241514205933\n",
            "epoch 3 step 235 loss 0.09423010796308517\n",
            "epoch 3 step 236 loss 0.13230164349079132\n",
            "epoch 3 step 237 loss 0.09297209978103638\n",
            "epoch 3 step 238 loss 0.03483844920992851\n",
            "epoch 3 step 239 loss 0.08532780408859253\n",
            "epoch 3 step 240 loss 0.06353425234556198\n",
            "epoch 3 step 241 loss 0.05859607458114624\n",
            "epoch 3 step 242 loss 0.060707591474056244\n",
            "epoch 3 step 243 loss 0.07693677395582199\n",
            "epoch 3 step 244 loss 0.1327189803123474\n",
            "epoch 3 step 245 loss 0.08319757878780365\n",
            "epoch 3 step 246 loss 0.06216961145401001\n",
            "epoch 3 step 247 loss 0.07183022797107697\n",
            "epoch 3 step 248 loss 0.06915467977523804\n",
            "epoch 3 step 249 loss 0.043817710131406784\n",
            "epoch 3 step 250 loss 0.0525486096739769\n",
            "epoch 3 step 251 loss 0.06580980867147446\n",
            "epoch 3 step 252 loss 0.10118406265974045\n",
            "epoch 3 step 253 loss 0.09840066730976105\n",
            "epoch 3 step 254 loss 0.07446343451738358\n",
            "epoch 3 step 255 loss 0.06363420188426971\n",
            "epoch 3 step 256 loss 0.07682887464761734\n",
            "epoch 3 step 257 loss 0.11411844193935394\n",
            "epoch 3 step 258 loss 0.07992716133594513\n",
            "epoch 3 step 259 loss 0.06114450469613075\n",
            "epoch 3 step 260 loss 0.10121805220842361\n",
            "epoch 3 step 261 loss 0.13785655796527863\n",
            "epoch 3 step 262 loss 0.04139623045921326\n",
            "epoch 3 step 263 loss 0.08558502793312073\n",
            "epoch 3 step 264 loss 0.15263259410858154\n",
            "epoch 3 step 265 loss 0.11331146955490112\n",
            "epoch 3 step 266 loss 0.12446323037147522\n",
            "epoch 3 step 267 loss 0.10270504653453827\n",
            "epoch 3 step 268 loss 0.08703362941741943\n",
            "epoch 3 step 269 loss 0.10121966898441315\n",
            "epoch 3 step 270 loss 0.13741019368171692\n",
            "epoch 3 step 271 loss 0.06927099823951721\n",
            "epoch 3 step 272 loss 0.08513376116752625\n",
            "epoch 3 step 273 loss 0.06723635643720627\n",
            "epoch 3 step 274 loss 0.11492688953876495\n",
            "epoch 3 step 275 loss 0.0758303552865982\n",
            "epoch 3 step 276 loss 0.060662511736154556\n",
            "epoch 3 step 277 loss 0.057219840586185455\n",
            "epoch 3 step 278 loss 0.10564938187599182\n",
            "epoch 3 step 279 loss 0.08028465509414673\n",
            "epoch 3 step 280 loss 0.14420185983181\n",
            "epoch 3 step 281 loss 0.08594739437103271\n",
            "epoch 3 step 282 loss 0.06035152077674866\n",
            "epoch 3 step 283 loss 0.08150255680084229\n",
            "epoch 3 step 284 loss 0.07861532270908356\n",
            "epoch 3 step 285 loss 0.07190610468387604\n",
            "epoch 3 step 286 loss 0.08529920876026154\n",
            "epoch 3 step 287 loss 0.09561294317245483\n",
            "epoch 3 step 288 loss 0.08331184834241867\n",
            "epoch 3 step 289 loss 0.061147741973400116\n",
            "epoch 3 step 290 loss 0.08168698847293854\n",
            "epoch 3 step 291 loss 0.0730457603931427\n",
            "epoch 3 step 292 loss 0.11645328998565674\n",
            "epoch 3 step 293 loss 0.071995809674263\n",
            "epoch 3 step 294 loss 0.0992443785071373\n",
            "epoch 3 step 295 loss 0.07762470096349716\n",
            "epoch 3 step 296 loss 0.07591259479522705\n",
            "epoch 3 step 297 loss 0.07745029777288437\n",
            "epoch 3 step 298 loss 0.18734966218471527\n",
            "epoch 3 step 299 loss 0.07436908036470413\n",
            "epoch 3 step 300 loss 0.09238341450691223\n",
            "epoch 3 step 301 loss 0.09690581262111664\n",
            "epoch 3 step 302 loss 0.10064172744750977\n",
            "epoch 3 step 303 loss 0.0856105387210846\n",
            "epoch 3 step 304 loss 0.05583920329809189\n",
            "epoch 3 step 305 loss 0.03719531372189522\n",
            "epoch 3 step 306 loss 0.05721641704440117\n",
            "epoch 3 step 307 loss 0.09318840503692627\n",
            "epoch 3 step 308 loss 0.09518589079380035\n",
            "epoch 3 step 309 loss 0.06644342839717865\n",
            "epoch 3 step 310 loss 0.059369415044784546\n",
            "epoch 3 step 311 loss 0.06952674686908722\n",
            "epoch 3 step 312 loss 0.08322679996490479\n",
            "epoch 3 step 313 loss 0.04422120004892349\n",
            "epoch 3 step 314 loss 0.09149205684661865\n",
            "epoch 3 step 315 loss 0.0661216452717781\n",
            "epoch 3 step 316 loss 0.07968875765800476\n",
            "epoch 3 step 317 loss 0.059890829026699066\n",
            "epoch 3 step 318 loss 0.08363202214241028\n",
            "epoch 3 step 319 loss 0.0735328271985054\n",
            "epoch 3 step 320 loss 0.05340172350406647\n",
            "epoch 3 step 321 loss 0.06213182955980301\n",
            "epoch 3 step 322 loss 0.059379950165748596\n",
            "epoch 3 step 323 loss 0.06832427531480789\n",
            "epoch 3 step 324 loss 0.051954545080661774\n",
            "epoch 3 step 325 loss 0.07043057680130005\n",
            "epoch 3 step 326 loss 0.11666673421859741\n",
            "epoch 3 step 327 loss 0.13907353579998016\n",
            "epoch 3 step 328 loss 0.050517067313194275\n",
            "epoch 3 step 329 loss 0.04575018212199211\n",
            "epoch 3 step 330 loss 0.055279918015003204\n",
            "epoch 3 step 331 loss 0.04955121502280235\n",
            "epoch 3 step 332 loss 0.055018723011016846\n",
            "epoch 3 step 333 loss 0.09607718884944916\n",
            "epoch 3 step 334 loss 0.06992191076278687\n",
            "epoch 3 step 335 loss 0.0779714584350586\n",
            "epoch 3 step 336 loss 0.06835339963436127\n",
            "epoch 3 step 337 loss 0.08116105943918228\n",
            "epoch 3 step 338 loss 0.10122478008270264\n",
            "epoch 3 step 339 loss 0.07416108250617981\n",
            "epoch 3 step 340 loss 0.07344678789377213\n",
            "epoch 3 step 341 loss 0.05322336405515671\n",
            "epoch 3 step 342 loss 0.09705416858196259\n",
            "epoch 3 step 343 loss 0.05986704304814339\n",
            "epoch 3 step 344 loss 0.1078358143568039\n",
            "epoch 3 step 345 loss 0.11124999076128006\n",
            "epoch 3 step 346 loss 0.1516926884651184\n",
            "epoch 3 step 347 loss 0.08668595552444458\n",
            "epoch 3 step 348 loss 0.10632773488759995\n",
            "epoch 3 step 349 loss 0.07189151644706726\n",
            "epoch 3 step 350 loss 0.06057746708393097\n",
            "epoch 3 step 351 loss 0.09494777768850327\n",
            "epoch 3 step 352 loss 0.0834369882941246\n",
            "epoch 3 step 353 loss 0.08795397728681564\n",
            "epoch 3 step 354 loss 0.11166618764400482\n",
            "epoch 3 step 355 loss 0.11670109629631042\n",
            "epoch 3 step 356 loss 0.050784844905138016\n",
            "epoch 3 step 357 loss 0.08781319111585617\n",
            "epoch 3 step 358 loss 0.06961110234260559\n",
            "epoch 3 step 359 loss 0.10979747027158737\n",
            "epoch 3 step 360 loss 0.11689300090074539\n",
            "epoch 3 step 361 loss 0.0556529276072979\n",
            "epoch 3 step 362 loss 0.05597774684429169\n",
            "epoch 3 step 363 loss 0.10290183126926422\n",
            "epoch 3 step 364 loss 0.07936332374811172\n",
            "epoch 3 step 365 loss 0.08675410598516464\n",
            "epoch 3 step 366 loss 0.09952570497989655\n",
            "epoch 3 step 367 loss 0.04630805924534798\n",
            "epoch 3 step 368 loss 0.10018128901720047\n",
            "epoch 3 step 369 loss 0.0779431015253067\n",
            "epoch 3 step 370 loss 0.06915168464183807\n",
            "epoch 3 step 371 loss 0.09993346035480499\n",
            "epoch 3 step 372 loss 0.04020506888628006\n",
            "epoch 3 step 373 loss 0.09120465815067291\n",
            "epoch 3 step 374 loss 0.10834172368049622\n",
            "epoch 3 step 375 loss 0.06321318447589874\n",
            "epoch 3 step 376 loss 0.043740786612033844\n",
            "epoch 3 step 377 loss 0.056314073503017426\n",
            "epoch 3 step 378 loss 0.0713302493095398\n",
            "epoch 3 step 379 loss 0.13190215826034546\n",
            "epoch 3 step 380 loss 0.10082969069480896\n",
            "epoch 3 step 381 loss 0.08364676684141159\n",
            "epoch 3 step 382 loss 0.07828499376773834\n",
            "epoch 3 step 383 loss 0.04815353825688362\n",
            "epoch 3 step 384 loss 0.0615115687251091\n",
            "epoch 3 step 385 loss 0.06860660016536713\n",
            "epoch 3 step 386 loss 0.0540984645485878\n",
            "epoch 3 step 387 loss 0.0940973162651062\n",
            "epoch 3 step 388 loss 0.06326726078987122\n",
            "epoch 3 step 389 loss 0.05692553520202637\n",
            "epoch 3 step 390 loss 0.07164090871810913\n",
            "epoch 3 step 391 loss 0.055055562406778336\n",
            "epoch 3 step 392 loss 0.06218203902244568\n",
            "epoch 3 step 393 loss 0.07571792602539062\n",
            "epoch 3 step 394 loss 0.08135851472616196\n",
            "epoch 3 step 395 loss 0.05835192650556564\n",
            "epoch 3 step 396 loss 0.08786367624998093\n",
            "epoch 3 step 397 loss 0.119609035551548\n",
            "epoch 3 step 398 loss 0.08229188621044159\n",
            "epoch 3 step 399 loss 0.05747514218091965\n",
            "epoch 3 step 400 loss 0.08963164687156677\n",
            "epoch 3 step 401 loss 0.07462581992149353\n",
            "epoch 3 step 402 loss 0.08172811567783356\n",
            "epoch 3 step 403 loss 0.05217199772596359\n",
            "epoch 3 step 404 loss 0.05416727811098099\n",
            "epoch 3 step 405 loss 0.07917462289333344\n",
            "epoch 3 step 406 loss 0.09582871198654175\n",
            "epoch 3 step 407 loss 0.0766637772321701\n",
            "epoch 3 step 408 loss 0.0746820792555809\n",
            "epoch 3 step 409 loss 0.04640728980302811\n",
            "epoch 3 step 410 loss 0.11444620043039322\n",
            "epoch 3 step 411 loss 0.07332712411880493\n",
            "epoch 3 step 412 loss 0.05855199694633484\n",
            "epoch 3 step 413 loss 0.05127212405204773\n",
            "epoch 3 step 414 loss 0.06582002341747284\n",
            "epoch 3 step 415 loss 0.08192117512226105\n",
            "epoch 3 step 416 loss 0.05096743255853653\n",
            "epoch 3 step 417 loss 0.07278142124414444\n",
            "epoch 3 step 418 loss 0.1120176762342453\n",
            "epoch 3 step 419 loss 0.061197809875011444\n",
            "epoch 3 step 420 loss 0.06755378842353821\n",
            "epoch 3 step 421 loss 0.10896193981170654\n",
            "epoch 3 step 422 loss 0.050568968057632446\n",
            "epoch 3 step 423 loss 0.10299818217754364\n",
            "epoch 3 step 424 loss 0.08441205322742462\n",
            "epoch 3 step 425 loss 0.07834743708372116\n",
            "epoch 3 step 426 loss 0.056537751108407974\n",
            "epoch 3 step 427 loss 0.07450215518474579\n",
            "epoch 3 step 428 loss 0.12454257905483246\n",
            "epoch 3 step 429 loss 0.09597739577293396\n",
            "epoch 3 step 430 loss 0.08999103307723999\n",
            "epoch 3 step 431 loss 0.035243451595306396\n",
            "epoch 3 step 432 loss 0.0960107296705246\n",
            "epoch 3 step 433 loss 0.10811620950698853\n",
            "epoch 3 step 434 loss 0.05859906226396561\n",
            "epoch 3 step 435 loss 0.05350683629512787\n",
            "epoch 3 step 436 loss 0.06600488722324371\n",
            "epoch 3 step 437 loss 0.05226648598909378\n",
            "epoch 3 step 438 loss 0.051600463688373566\n",
            "epoch 3 step 439 loss 0.07902637869119644\n",
            "epoch 3 step 440 loss 0.03858811408281326\n",
            "epoch 3 step 441 loss 0.09483394026756287\n",
            "epoch 3 step 442 loss 0.06684589385986328\n",
            "epoch 3 step 443 loss 0.09852112084627151\n",
            "epoch 3 step 444 loss 0.0767001286149025\n",
            "epoch 3 step 445 loss 0.05940498039126396\n",
            "epoch 3 step 446 loss 0.04508122056722641\n",
            "epoch 3 step 447 loss 0.10281047224998474\n",
            "epoch 3 step 448 loss 0.1327456384897232\n",
            "epoch 3 step 449 loss 0.07348030805587769\n",
            "epoch 3 step 450 loss 0.09171763062477112\n",
            "epoch 3 step 451 loss 0.06739634275436401\n",
            "epoch 3 step 452 loss 0.050028324127197266\n",
            "epoch 3 step 453 loss 0.07228750735521317\n",
            "epoch 3 step 454 loss 0.054674480110406876\n",
            "epoch 3 step 455 loss 0.10071182250976562\n",
            "epoch 3 step 456 loss 0.054510004818439484\n",
            "epoch 3 step 457 loss 0.06962479650974274\n",
            "epoch 3 step 458 loss 0.06552571803331375\n",
            "epoch 3 step 459 loss 0.0609009824693203\n",
            "epoch 3 step 460 loss 0.06310345232486725\n",
            "epoch 3 step 461 loss 0.08796200901269913\n",
            "epoch 3 step 462 loss 0.06448815762996674\n",
            "epoch 3 step 463 loss 0.07857638597488403\n",
            "epoch 3 step 464 loss 0.10460444539785385\n",
            "epoch 3 step 465 loss 0.09130982309579849\n",
            "epoch 3 step 466 loss 0.06722131371498108\n",
            "epoch 3 step 467 loss 0.09274639189243317\n",
            "epoch 3 step 468 loss 0.07023297995328903\n",
            "epoch 3 step 469 loss 0.09813552349805832\n",
            "epoch 3 step 470 loss 0.09129011631011963\n",
            "epoch 3 step 471 loss 0.0883055031299591\n",
            "epoch 3 step 472 loss 0.07857295870780945\n",
            "epoch 3 step 473 loss 0.08613446354866028\n",
            "epoch 3 step 474 loss 0.061362333595752716\n",
            "epoch 3 step 475 loss 0.07811923325061798\n",
            "epoch 3 step 476 loss 0.09754978120326996\n",
            "epoch 3 step 477 loss 0.05908464267849922\n",
            "epoch 3 step 478 loss 0.14747682213783264\n",
            "epoch 3 step 479 loss 0.09666313230991364\n",
            "epoch 3 step 480 loss 0.11435253918170929\n",
            "epoch 3 step 481 loss 0.058451369404792786\n",
            "epoch 3 step 482 loss 0.04934314638376236\n",
            "epoch 3 step 483 loss 0.04010333865880966\n",
            "epoch 3 step 484 loss 0.07381945848464966\n",
            "epoch 3 step 485 loss 0.09441369771957397\n",
            "epoch 3 step 486 loss 0.04942276328802109\n",
            "epoch 3 step 487 loss 0.1304396539926529\n",
            "epoch 3 step 488 loss 0.16438989341259003\n",
            "epoch 3 step 489 loss 0.09650959074497223\n",
            "epoch 3 step 490 loss 0.09168662130832672\n",
            "epoch 3 step 491 loss 0.07495385408401489\n",
            "epoch 3 step 492 loss 0.07440657913684845\n",
            "epoch 3 step 493 loss 0.07908417284488678\n",
            "epoch 3 step 494 loss 0.0852128267288208\n",
            "epoch 3 step 495 loss 0.07243648171424866\n",
            "epoch 3 step 496 loss 0.06849893927574158\n",
            "epoch 3 step 497 loss 0.11004582047462463\n",
            "epoch 3 step 498 loss 0.08618026971817017\n",
            "epoch 3 step 499 loss 0.06978392601013184\n",
            "epoch 3 step 500 loss 0.15114472806453705\n",
            "epoch 3 step 501 loss 0.07469337433576584\n",
            "epoch 3 step 502 loss 0.07008744031190872\n",
            "epoch 3 step 503 loss 0.07563816756010056\n",
            "epoch 3 step 504 loss 0.09527668356895447\n",
            "epoch 3 step 505 loss 0.075301393866539\n",
            "epoch 3 step 506 loss 0.06472846865653992\n",
            "epoch 3 step 507 loss 0.08153307437896729\n",
            "epoch 3 step 508 loss 0.07559215277433395\n",
            "epoch 3 step 509 loss 0.06602862477302551\n",
            "epoch 3 step 510 loss 0.0726548433303833\n",
            "epoch 3 step 511 loss 0.08407917618751526\n",
            "epoch 3 step 512 loss 0.056219566613435745\n",
            "epoch 3 step 513 loss 0.03877584636211395\n",
            "epoch 3 step 514 loss 0.05113862454891205\n",
            "epoch 3 step 515 loss 0.07299169898033142\n",
            "epoch 3 step 516 loss 0.05328400433063507\n",
            "epoch 3 step 517 loss 0.08864739537239075\n",
            "epoch 3 step 518 loss 0.059350110590457916\n",
            "epoch 3 step 519 loss 0.07624857127666473\n",
            "epoch 3 step 520 loss 0.05595927685499191\n",
            "epoch 3 step 521 loss 0.11569088697433472\n",
            "epoch 3 step 522 loss 0.06135682016611099\n",
            "epoch 3 step 523 loss 0.06760627031326294\n",
            "epoch 3 step 524 loss 0.1026449054479599\n",
            "epoch 3 step 525 loss 0.111702099442482\n",
            "epoch 3 step 526 loss 0.05046343058347702\n",
            "epoch 3 step 527 loss 0.09778834879398346\n",
            "epoch 3 step 528 loss 0.08893713355064392\n",
            "epoch 3 step 529 loss 0.0952390655875206\n",
            "epoch 3 step 530 loss 0.07014278322458267\n",
            "epoch 3 step 531 loss 0.07154752314090729\n",
            "epoch 3 step 532 loss 0.08278726786375046\n",
            "epoch 3 step 533 loss 0.036437761038541794\n",
            "epoch 3 step 534 loss 0.0959864929318428\n",
            "epoch 3 step 535 loss 0.09699513018131256\n",
            "epoch 3 step 536 loss 0.07928209006786346\n",
            "epoch 3 step 537 loss 0.06480985134840012\n",
            "epoch 3 step 538 loss 0.0881395936012268\n",
            "epoch 3 step 539 loss 0.07185891270637512\n",
            "epoch 3 step 540 loss 0.0853109061717987\n",
            "epoch 3 step 541 loss 0.07650726288557053\n",
            "epoch 3 step 542 loss 0.06094878166913986\n",
            "epoch 3 step 543 loss 0.038448650389909744\n",
            "epoch 3 step 544 loss 0.0837145745754242\n",
            "epoch 3 step 545 loss 0.05875521898269653\n",
            "epoch 3 step 546 loss 0.07055208086967468\n",
            "epoch 3 step 547 loss 0.063486248254776\n",
            "epoch 3 step 548 loss 0.06853915005922318\n",
            "epoch 3 step 549 loss 0.10363638401031494\n",
            "epoch 3 step 550 loss 0.04475027322769165\n",
            "epoch 3 step 551 loss 0.07328180968761444\n",
            "epoch 3 step 552 loss 0.06361906230449677\n",
            "epoch 3 step 553 loss 0.05521883815526962\n",
            "epoch 3 step 554 loss 0.07128364592790604\n",
            "epoch 3 step 555 loss 0.08325226604938507\n",
            "epoch 3 step 556 loss 0.06295439600944519\n",
            "epoch 3 step 557 loss 0.05103442072868347\n",
            "epoch 3 step 558 loss 0.040516797453165054\n",
            "epoch 3 step 559 loss 0.062028173357248306\n",
            "epoch 3 step 560 loss 0.06544607877731323\n",
            "epoch 3 step 561 loss 0.07996052503585815\n",
            "epoch 3 step 562 loss 0.07816541939973831\n",
            "epoch 3 step 563 loss 0.1009337455034256\n",
            "epoch 3 step 564 loss 0.04463602602481842\n",
            "epoch 3 step 565 loss 0.06943614780902863\n",
            "epoch 3 step 566 loss 0.054729562252759933\n",
            "epoch 3 step 567 loss 0.03756663575768471\n",
            "epoch 3 step 568 loss 0.08122897148132324\n",
            "epoch 3 step 569 loss 0.05594900622963905\n",
            "epoch 3 step 570 loss 0.03935130313038826\n",
            "epoch 3 step 571 loss 0.07847191393375397\n",
            "epoch 3 step 572 loss 0.054392121732234955\n",
            "epoch 3 step 573 loss 0.17397046089172363\n",
            "epoch 3 step 574 loss 0.044334132224321365\n",
            "epoch 3 step 575 loss 0.060117218643426895\n",
            "epoch 3 step 576 loss 0.06910750269889832\n",
            "epoch 3 step 577 loss 0.1693243682384491\n",
            "epoch 3 step 578 loss 0.15180405974388123\n",
            "epoch 3 step 579 loss 0.0947706550359726\n",
            "epoch 3 step 580 loss 0.08472254127264023\n",
            "epoch 3 step 581 loss 0.06439979374408722\n",
            "epoch 3 step 582 loss 0.07136078178882599\n",
            "epoch 3 step 583 loss 0.049784041941165924\n",
            "epoch 3 step 584 loss 0.09635846316814423\n",
            "epoch 3 step 585 loss 0.10116200894117355\n",
            "epoch 3 step 586 loss 0.07826387882232666\n",
            "epoch 3 step 587 loss 0.08551112562417984\n",
            "epoch 3 step 588 loss 0.08057914674282074\n",
            "epoch 3 step 589 loss 0.054373882710933685\n",
            "epoch 3 step 590 loss 0.08336396515369415\n",
            "epoch 3 step 591 loss 0.0885995477437973\n",
            "epoch 3 step 592 loss 0.07300993800163269\n",
            "epoch 3 step 593 loss 0.11524823307991028\n",
            "epoch 3 step 594 loss 0.14996874332427979\n",
            "epoch 3 step 595 loss 0.09526848047971725\n",
            "epoch 3 step 596 loss 0.07606097310781479\n",
            "epoch 3 step 597 loss 0.0959751158952713\n",
            "epoch 3 step 598 loss 0.07644706219434738\n",
            "epoch 3 step 599 loss 0.10486524552106857\n",
            "epoch 3 step 600 loss 0.045818980783224106\n",
            "epoch 3 step 601 loss 0.11373984813690186\n",
            "epoch 3 step 602 loss 0.05728190764784813\n",
            "epoch 3 step 603 loss 0.09126468002796173\n",
            "epoch 3 step 604 loss 0.07536398619413376\n",
            "epoch 3 step 605 loss 0.05032981559634209\n",
            "epoch 3 step 606 loss 0.08251278102397919\n",
            "epoch 3 step 607 loss 0.06592307984828949\n",
            "epoch 3 step 608 loss 0.06769107282161713\n",
            "epoch 3 step 609 loss 0.09891106933355331\n",
            "epoch 3 step 610 loss 0.06561730802059174\n",
            "epoch 3 step 611 loss 0.062418822199106216\n",
            "epoch 3 step 612 loss 0.06133638322353363\n",
            "epoch 3 step 613 loss 0.0408504456281662\n",
            "epoch 3 step 614 loss 0.05249164626002312\n",
            "epoch 3 step 615 loss 0.051772817969322205\n",
            "epoch 3 step 616 loss 0.06606129556894302\n",
            "epoch 3 step 617 loss 0.07039149105548859\n",
            "epoch 3 step 618 loss 0.0387333482503891\n",
            "epoch 3 step 619 loss 0.05724022909998894\n",
            "epoch 3 step 620 loss 0.07135095447301865\n",
            "epoch 3 step 621 loss 0.08973178267478943\n",
            "epoch 3 step 622 loss 0.059896327555179596\n",
            "epoch 3 step 623 loss 0.05265142768621445\n",
            "epoch 3 step 624 loss 0.08621270954608917\n",
            "epoch 3 step 625 loss 0.06576867401599884\n",
            "epoch 3 step 626 loss 0.06403489410877228\n",
            "epoch 3 step 627 loss 0.0736350417137146\n",
            "epoch 3 step 628 loss 0.06941554695367813\n",
            "epoch 3 step 629 loss 0.07569243013858795\n",
            "epoch 3 step 630 loss 0.07104378193616867\n",
            "epoch 3 step 631 loss 0.07292377948760986\n",
            "epoch 3 step 632 loss 0.06671380251646042\n",
            "epoch 3 step 633 loss 0.061550334095954895\n",
            "epoch 3 step 634 loss 0.11043551564216614\n",
            "epoch 3 step 635 loss 0.03242917358875275\n",
            "epoch 3 step 636 loss 0.05032724142074585\n",
            "epoch 3 step 637 loss 0.08158569037914276\n",
            "epoch 3 step 638 loss 0.07670623064041138\n",
            "epoch 3 step 639 loss 0.06936532258987427\n",
            "epoch 3 step 640 loss 0.14616350829601288\n",
            "epoch 3 step 641 loss 0.08266334235668182\n",
            "epoch 3 step 642 loss 0.06970426440238953\n",
            "epoch 3 step 643 loss 0.06225572153925896\n",
            "epoch 3 step 644 loss 0.04735102504491806\n",
            "epoch 3 step 645 loss 0.05441359430551529\n",
            "epoch 3 step 646 loss 0.08866338431835175\n",
            "epoch 3 step 647 loss 0.08716954290866852\n",
            "epoch 3 step 648 loss 0.04173130542039871\n",
            "epoch 3 step 649 loss 0.056056246161460876\n",
            "epoch 3 step 650 loss 0.04247055575251579\n",
            "epoch 3 step 651 loss 0.1475335657596588\n",
            "epoch 3 step 652 loss 0.07803727686405182\n",
            "epoch 3 step 653 loss 0.051907122135162354\n",
            "epoch 3 step 654 loss 0.09376358985900879\n",
            "epoch 3 step 655 loss 0.08421014249324799\n",
            "epoch 3 step 656 loss 0.09624166786670685\n",
            "epoch 3 step 657 loss 0.11816419661045074\n",
            "epoch 3 step 658 loss 0.0666424110531807\n",
            "epoch 3 step 659 loss 0.0815771147608757\n",
            "epoch 3 step 660 loss 0.07930520176887512\n",
            "epoch 3 step 661 loss 0.07433921098709106\n",
            "epoch 3 step 662 loss 0.07350803911685944\n",
            "epoch 3 step 663 loss 0.07989639043807983\n",
            "epoch 3 step 664 loss 0.14194181561470032\n",
            "epoch 3 step 665 loss 0.06757092475891113\n",
            "epoch 3 step 666 loss 0.04331688582897186\n",
            "epoch 3 step 667 loss 0.1002214252948761\n",
            "epoch 3 step 668 loss 0.06612741202116013\n",
            "epoch 3 step 669 loss 0.05790521949529648\n",
            "epoch 3 step 670 loss 0.10942665487527847\n",
            "epoch 3 step 671 loss 0.08226724714040756\n",
            "epoch 3 step 672 loss 0.12086134403944016\n",
            "epoch 3 step 673 loss 0.07451511919498444\n",
            "epoch 3 step 674 loss 0.12300994992256165\n",
            "epoch 3 step 675 loss 0.08898739516735077\n",
            "epoch 3 step 676 loss 0.08401332050561905\n",
            "epoch 3 step 677 loss 0.08586783707141876\n",
            "epoch 3 step 678 loss 0.03767769783735275\n",
            "epoch 3 step 679 loss 0.10005733370780945\n",
            "epoch 3 step 680 loss 0.049433160573244095\n",
            "epoch 3 step 681 loss 0.046698346734046936\n",
            "epoch 3 step 682 loss 0.1104179248213768\n",
            "epoch 3 step 683 loss 0.1596093624830246\n",
            "epoch 3 step 684 loss 0.08600132912397385\n",
            "epoch 3 step 685 loss 0.11221378296613693\n",
            "epoch 3 step 686 loss 0.10396338999271393\n",
            "epoch 3 step 687 loss 0.07048843801021576\n",
            "epoch 3 step 688 loss 0.08599363267421722\n",
            "epoch 3 step 689 loss 0.10565968602895737\n",
            "epoch 3 step 690 loss 0.1375598907470703\n",
            "epoch 3 step 691 loss 0.06971107423305511\n",
            "epoch 3 step 692 loss 0.11154141277074814\n",
            "epoch 3 step 693 loss 0.15217158198356628\n",
            "epoch 3 step 694 loss 0.06522412598133087\n",
            "epoch 3 step 695 loss 0.13380883634090424\n",
            "epoch 3 step 696 loss 0.06721868366003036\n",
            "epoch 3 step 697 loss 0.05221262574195862\n",
            "epoch 3 step 698 loss 0.06626467406749725\n",
            "epoch 3 step 699 loss 0.1110590398311615\n",
            "epoch 3 step 700 loss 0.06932613253593445\n",
            "epoch 3 step 701 loss 0.06261838972568512\n",
            "epoch 3 step 702 loss 0.08090804517269135\n",
            "epoch 3 step 703 loss 0.06993518024682999\n",
            "epoch 3 step 704 loss 0.08180122077465057\n",
            "epoch 3 step 705 loss 0.12232816964387894\n",
            "epoch 3 step 706 loss 0.08096514642238617\n",
            "epoch 3 step 707 loss 0.10717535018920898\n",
            "epoch 3 step 708 loss 0.05349007248878479\n",
            "epoch 3 step 709 loss 0.08339154720306396\n",
            "epoch 3 step 710 loss 0.0908915102481842\n",
            "epoch 3 step 711 loss 0.06758107244968414\n",
            "epoch 3 step 712 loss 0.09142140299081802\n",
            "epoch 3 step 713 loss 0.10061880946159363\n",
            "epoch 3 step 714 loss 0.06624363362789154\n",
            "epoch 3 step 715 loss 0.097914919257164\n",
            "epoch 3 step 716 loss 0.1273978352546692\n",
            "epoch 3 step 717 loss 0.06500652432441711\n",
            "epoch 3 step 718 loss 0.07088600099086761\n",
            "epoch 3 step 719 loss 0.12517058849334717\n",
            "epoch 3 step 720 loss 0.10230104625225067\n",
            "epoch 3 step 721 loss 0.04894116520881653\n",
            "epoch 3 step 722 loss 0.08928842842578888\n",
            "epoch 3 step 723 loss 0.109564408659935\n",
            "epoch 3 step 724 loss 0.11831609904766083\n",
            "epoch 3 step 725 loss 0.07744447886943817\n",
            "epoch 3 step 726 loss 0.12601438164710999\n",
            "epoch 3 step 727 loss 0.10316523909568787\n",
            "epoch 3 step 728 loss 0.07250024378299713\n",
            "epoch 3 step 729 loss 0.04776844382286072\n",
            "epoch 3 step 730 loss 0.0940585657954216\n",
            "epoch 3 step 731 loss 0.05694131925702095\n",
            "epoch 3 step 732 loss 0.06190495193004608\n",
            "epoch 3 step 733 loss 0.08764322847127914\n",
            "epoch 3 step 734 loss 0.08841490745544434\n",
            "epoch 3 step 735 loss 0.05312296748161316\n",
            "epoch 3 step 736 loss 0.04702254384756088\n",
            "epoch 3 step 737 loss 0.0638192892074585\n",
            "epoch 3 step 738 loss 0.07354356348514557\n",
            "epoch 3 step 739 loss 0.038027361035346985\n",
            "epoch 3 step 740 loss 0.08442572504281998\n",
            "epoch 3 step 741 loss 0.04666832461953163\n",
            "epoch 3 step 742 loss 0.08024866133928299\n",
            "epoch 3 step 743 loss 0.052631691098213196\n",
            "epoch 3 step 744 loss 0.05724626034498215\n",
            "epoch 3 step 745 loss 0.06084265187382698\n",
            "epoch 3 step 746 loss 0.08176948875188828\n",
            "epoch 3 step 747 loss 0.08078043907880783\n",
            "epoch 3 step 748 loss 0.06242198497056961\n",
            "epoch 3 step 749 loss 0.058480650186538696\n",
            "epoch 3 step 750 loss 0.0639423206448555\n",
            "epoch 3 step 751 loss 0.09057417511940002\n",
            "epoch 3 step 752 loss 0.10353633761405945\n",
            "epoch 3 step 753 loss 0.08265306800603867\n",
            "epoch 3 step 754 loss 0.0945771336555481\n",
            "epoch 3 step 755 loss 0.07021814584732056\n",
            "epoch 3 step 756 loss 0.06483960896730423\n",
            "epoch 3 step 757 loss 0.05247395858168602\n",
            "epoch 3 step 758 loss 0.11187957972288132\n",
            "epoch 3 step 759 loss 0.08744407445192337\n",
            "epoch 3 step 760 loss 0.056828372180461884\n",
            "epoch 3 step 761 loss 0.047810088843107224\n",
            "epoch 3 step 762 loss 0.06028643622994423\n",
            "epoch 3 step 763 loss 0.06398199498653412\n",
            "epoch 3 step 764 loss 0.05993238463997841\n",
            "epoch 3 step 765 loss 0.08497615903615952\n",
            "epoch 3 step 766 loss 0.05844602733850479\n",
            "epoch 3 step 767 loss 0.0647193193435669\n",
            "epoch 3 step 768 loss 0.06254517287015915\n",
            "epoch 3 step 769 loss 0.045267604291439056\n",
            "epoch 3 step 770 loss 0.07787557691335678\n",
            "epoch 3 step 771 loss 0.038647040724754333\n",
            "epoch 3 step 772 loss 0.07797788083553314\n",
            "epoch 3 step 773 loss 0.06791427731513977\n",
            "epoch 3 step 774 loss 0.05100409314036369\n",
            "epoch 3 step 775 loss 0.04578901082277298\n",
            "epoch 3 step 776 loss 0.05549287796020508\n",
            "epoch 3 step 777 loss 0.09462510794401169\n",
            "epoch 3 step 778 loss 0.06827162951231003\n",
            "epoch 3 step 779 loss 0.08382697403430939\n",
            "epoch 3 step 780 loss 0.10594882071018219\n",
            "epoch 3 step 781 loss -0.009384211152791977\n",
            "epoch 4 step 0 loss 0.10609348863363266\n",
            "epoch 4 step 1 loss 0.09555112570524216\n",
            "epoch 4 step 2 loss 0.12015382945537567\n",
            "epoch 4 step 3 loss 0.1020464301109314\n",
            "epoch 4 step 4 loss 0.07006093859672546\n",
            "epoch 4 step 5 loss 0.09821970015764236\n",
            "epoch 4 step 6 loss 0.08431114256381989\n",
            "epoch 4 step 7 loss 0.09634062647819519\n",
            "epoch 4 step 8 loss 0.06955672055482864\n",
            "epoch 4 step 9 loss 0.07988505065441132\n",
            "epoch 4 step 10 loss 0.0664656013250351\n",
            "epoch 4 step 11 loss 0.0875612124800682\n",
            "epoch 4 step 12 loss 0.0546150803565979\n",
            "epoch 4 step 13 loss 0.08623814582824707\n",
            "epoch 4 step 14 loss 0.13999800384044647\n",
            "epoch 4 step 15 loss 0.09125474095344543\n",
            "epoch 4 step 16 loss 0.05718030780553818\n",
            "epoch 4 step 17 loss 0.06718532741069794\n",
            "epoch 4 step 18 loss 0.10656878352165222\n",
            "epoch 4 step 19 loss 0.0689464658498764\n",
            "epoch 4 step 20 loss 0.08599817752838135\n",
            "epoch 4 step 21 loss 0.07369925081729889\n",
            "epoch 4 step 22 loss 0.07167866826057434\n",
            "epoch 4 step 23 loss 0.13866430521011353\n",
            "epoch 4 step 24 loss 0.1047089695930481\n",
            "epoch 4 step 25 loss 0.0633355900645256\n",
            "epoch 4 step 26 loss 0.07533243298530579\n",
            "epoch 4 step 27 loss 0.09928954392671585\n",
            "epoch 4 step 28 loss 0.13892853260040283\n",
            "epoch 4 step 29 loss 0.07583408057689667\n",
            "epoch 4 step 30 loss 0.07614374905824661\n",
            "epoch 4 step 31 loss 0.08558493852615356\n",
            "epoch 4 step 32 loss 0.09069886803627014\n",
            "epoch 4 step 33 loss 0.06505005061626434\n",
            "epoch 4 step 34 loss 0.11512614786624908\n",
            "epoch 4 step 35 loss 0.0625525489449501\n",
            "epoch 4 step 36 loss 0.04279410094022751\n",
            "epoch 4 step 37 loss 0.0653827041387558\n",
            "epoch 4 step 38 loss 0.09029947966337204\n",
            "epoch 4 step 39 loss 0.04429604113101959\n",
            "epoch 4 step 40 loss 0.08903176337480545\n",
            "epoch 4 step 41 loss 0.1377117931842804\n",
            "epoch 4 step 42 loss 0.07981561124324799\n",
            "epoch 4 step 43 loss 0.09559015929698944\n",
            "epoch 4 step 44 loss 0.1145196408033371\n",
            "epoch 4 step 45 loss 0.08104195445775986\n",
            "epoch 4 step 46 loss 0.0951581746339798\n",
            "epoch 4 step 47 loss 0.056658633053302765\n",
            "epoch 4 step 48 loss 0.09782426804304123\n",
            "epoch 4 step 49 loss 0.06424566358327866\n",
            "epoch 4 step 50 loss 0.07800181210041046\n",
            "epoch 4 step 51 loss 0.05673336982727051\n",
            "epoch 4 step 52 loss 0.09029310196638107\n",
            "epoch 4 step 53 loss 0.04894629120826721\n",
            "epoch 4 step 54 loss 0.12171901762485504\n",
            "epoch 4 step 55 loss 0.06073242425918579\n",
            "epoch 4 step 56 loss 0.059956274926662445\n",
            "epoch 4 step 57 loss 0.06485164165496826\n",
            "epoch 4 step 58 loss 0.10566899925470352\n",
            "epoch 4 step 59 loss 0.03799000382423401\n",
            "epoch 4 step 60 loss 0.08540436625480652\n",
            "epoch 4 step 61 loss 0.07800780981779099\n",
            "epoch 4 step 62 loss 0.06356821954250336\n",
            "epoch 4 step 63 loss 0.0626700222492218\n",
            "epoch 4 step 64 loss 0.04105782136321068\n",
            "epoch 4 step 65 loss 0.08267197757959366\n",
            "epoch 4 step 66 loss 0.06246776878833771\n",
            "epoch 4 step 67 loss 0.10722064971923828\n",
            "epoch 4 step 68 loss 0.06745384633541107\n",
            "epoch 4 step 69 loss 0.05582293123006821\n",
            "epoch 4 step 70 loss 0.06675465404987335\n",
            "epoch 4 step 71 loss 0.04522370174527168\n",
            "epoch 4 step 72 loss 0.06447368860244751\n",
            "epoch 4 step 73 loss 0.10009042918682098\n",
            "epoch 4 step 74 loss 0.07528276741504669\n",
            "epoch 4 step 75 loss 0.1047479659318924\n",
            "epoch 4 step 76 loss 0.09354975819587708\n",
            "epoch 4 step 77 loss 0.10447610914707184\n",
            "epoch 4 step 78 loss 0.07588687539100647\n",
            "epoch 4 step 79 loss 0.09547080099582672\n",
            "epoch 4 step 80 loss 0.08035040646791458\n",
            "epoch 4 step 81 loss 0.127046138048172\n",
            "epoch 4 step 82 loss 0.09689154475927353\n",
            "epoch 4 step 83 loss 0.07691033184528351\n",
            "epoch 4 step 84 loss 0.0873044803738594\n",
            "epoch 4 step 85 loss 0.06576713919639587\n",
            "epoch 4 step 86 loss 0.04957147687673569\n",
            "epoch 4 step 87 loss 0.06056830286979675\n",
            "epoch 4 step 88 loss 0.03741178661584854\n",
            "epoch 4 step 89 loss 0.07566340267658234\n",
            "epoch 4 step 90 loss 0.07152552902698517\n",
            "epoch 4 step 91 loss 0.06749464571475983\n",
            "epoch 4 step 92 loss 0.06722162663936615\n",
            "epoch 4 step 93 loss 0.06562788784503937\n",
            "epoch 4 step 94 loss 0.14544552564620972\n",
            "epoch 4 step 95 loss 0.054418668150901794\n",
            "epoch 4 step 96 loss 0.05206276476383209\n",
            "epoch 4 step 97 loss 0.07784689962863922\n",
            "epoch 4 step 98 loss 0.08977632224559784\n",
            "epoch 4 step 99 loss 0.03829296678304672\n",
            "epoch 4 step 100 loss 0.0808822363615036\n",
            "epoch 4 step 101 loss 0.044918421655893326\n",
            "epoch 4 step 102 loss 0.09095259010791779\n",
            "epoch 4 step 103 loss 0.045040443539619446\n",
            "epoch 4 step 104 loss 0.2374030351638794\n",
            "epoch 4 step 105 loss 0.11188875883817673\n",
            "epoch 4 step 106 loss 0.08203683793544769\n",
            "epoch 4 step 107 loss 0.13944092392921448\n",
            "epoch 4 step 108 loss 0.11493314802646637\n",
            "epoch 4 step 109 loss 0.13418494164943695\n",
            "epoch 4 step 110 loss 0.16320814192295074\n",
            "epoch 4 step 111 loss 0.1569449007511139\n",
            "epoch 4 step 112 loss 0.06879672408103943\n",
            "epoch 4 step 113 loss 0.07338810712099075\n",
            "epoch 4 step 114 loss 0.0497177392244339\n",
            "epoch 4 step 115 loss 0.07203015685081482\n",
            "epoch 4 step 116 loss 0.06020362675189972\n",
            "epoch 4 step 117 loss 0.11234728991985321\n",
            "epoch 4 step 118 loss 0.07080145925283432\n",
            "epoch 4 step 119 loss 0.046772636473178864\n",
            "epoch 4 step 120 loss 0.06754729151725769\n",
            "epoch 4 step 121 loss 0.09762220084667206\n",
            "epoch 4 step 122 loss 0.09850631654262543\n",
            "epoch 4 step 123 loss 0.0881820023059845\n",
            "epoch 4 step 124 loss 0.09304030239582062\n",
            "epoch 4 step 125 loss 0.0712675005197525\n",
            "epoch 4 step 126 loss 0.08609600365161896\n",
            "epoch 4 step 127 loss 0.0707964301109314\n",
            "epoch 4 step 128 loss 0.05758061632514\n",
            "epoch 4 step 129 loss 0.06559673696756363\n",
            "epoch 4 step 130 loss 0.07180750370025635\n",
            "epoch 4 step 131 loss 0.07028485834598541\n",
            "epoch 4 step 132 loss 0.1476607620716095\n",
            "epoch 4 step 133 loss 0.05564499646425247\n",
            "epoch 4 step 134 loss 0.07691333442926407\n",
            "epoch 4 step 135 loss 0.05192388594150543\n",
            "epoch 4 step 136 loss 0.060765340924263\n",
            "epoch 4 step 137 loss 0.05093659460544586\n",
            "epoch 4 step 138 loss 0.08364942669868469\n",
            "epoch 4 step 139 loss 0.10890200734138489\n",
            "epoch 4 step 140 loss 0.0656650960445404\n",
            "epoch 4 step 141 loss 0.07385240495204926\n",
            "epoch 4 step 142 loss 0.08239319920539856\n",
            "epoch 4 step 143 loss 0.11495929211378098\n",
            "epoch 4 step 144 loss 0.07159826904535294\n",
            "epoch 4 step 145 loss 0.08022234588861465\n",
            "epoch 4 step 146 loss 0.08299723267555237\n",
            "epoch 4 step 147 loss 0.06698166579008102\n",
            "epoch 4 step 148 loss 0.09884430468082428\n",
            "epoch 4 step 149 loss 0.07181528210639954\n",
            "epoch 4 step 150 loss 0.07096254825592041\n",
            "epoch 4 step 151 loss 0.07941795140504837\n",
            "epoch 4 step 152 loss 0.05647886544466019\n",
            "epoch 4 step 153 loss 0.058462902903556824\n",
            "epoch 4 step 154 loss 0.06837001442909241\n",
            "epoch 4 step 155 loss 0.06933490931987762\n",
            "epoch 4 step 156 loss 0.07232261449098587\n",
            "epoch 4 step 157 loss 0.07889341562986374\n",
            "epoch 4 step 158 loss 0.07750695943832397\n",
            "epoch 4 step 159 loss 0.1302078366279602\n",
            "epoch 4 step 160 loss 0.04720645397901535\n",
            "epoch 4 step 161 loss 0.059095799922943115\n",
            "epoch 4 step 162 loss 0.04899340122938156\n",
            "epoch 4 step 163 loss 0.04298422485589981\n",
            "epoch 4 step 164 loss 0.07719498872756958\n",
            "epoch 4 step 165 loss 0.05216424912214279\n",
            "epoch 4 step 166 loss 0.08092518895864487\n",
            "epoch 4 step 167 loss 0.051326215267181396\n",
            "epoch 4 step 168 loss 0.06333478540182114\n",
            "epoch 4 step 169 loss 0.07822979986667633\n",
            "epoch 4 step 170 loss 0.08541937172412872\n",
            "epoch 4 step 171 loss 0.053444258868694305\n",
            "epoch 4 step 172 loss 0.06100963056087494\n",
            "epoch 4 step 173 loss 0.08751991391181946\n",
            "epoch 4 step 174 loss 0.06552258133888245\n",
            "epoch 4 step 175 loss 0.0745341032743454\n",
            "epoch 4 step 176 loss 0.09238021075725555\n",
            "epoch 4 step 177 loss 0.09648901969194412\n",
            "epoch 4 step 178 loss 0.06610812246799469\n",
            "epoch 4 step 179 loss 0.07605816423892975\n",
            "epoch 4 step 180 loss 0.0646107941865921\n",
            "epoch 4 step 181 loss 0.048907145857810974\n",
            "epoch 4 step 182 loss 0.056509342044591904\n",
            "epoch 4 step 183 loss 0.08317085355520248\n",
            "epoch 4 step 184 loss 0.06459739804267883\n",
            "epoch 4 step 185 loss 0.05271047726273537\n",
            "epoch 4 step 186 loss 0.0769052654504776\n",
            "epoch 4 step 187 loss 0.08005626499652863\n",
            "epoch 4 step 188 loss 0.0561852902173996\n",
            "epoch 4 step 189 loss 0.06518056988716125\n",
            "epoch 4 step 190 loss 0.04305429756641388\n",
            "epoch 4 step 191 loss 0.05467047542333603\n",
            "epoch 4 step 192 loss 0.03623882681131363\n",
            "epoch 4 step 193 loss 0.06687575578689575\n",
            "epoch 4 step 194 loss 0.06257941573858261\n",
            "epoch 4 step 195 loss 0.055069535970687866\n",
            "epoch 4 step 196 loss 0.048763006925582886\n",
            "epoch 4 step 197 loss 0.0780225470662117\n",
            "epoch 4 step 198 loss 0.10509638488292694\n",
            "epoch 4 step 199 loss 0.05622611567378044\n",
            "epoch 4 step 200 loss 0.05308506265282631\n",
            "epoch 4 step 201 loss 0.06484848260879517\n",
            "epoch 4 step 202 loss 0.11848233640193939\n",
            "epoch 4 step 203 loss 0.07661890983581543\n",
            "epoch 4 step 204 loss 0.06098815053701401\n",
            "epoch 4 step 205 loss 0.0444486066699028\n",
            "epoch 4 step 206 loss 0.08126111328601837\n",
            "epoch 4 step 207 loss 0.04434308409690857\n",
            "epoch 4 step 208 loss 0.06192510947585106\n",
            "epoch 4 step 209 loss 0.061922989785671234\n",
            "epoch 4 step 210 loss 0.06703482568264008\n",
            "epoch 4 step 211 loss 0.0937347561120987\n",
            "epoch 4 step 212 loss 0.031784623861312866\n",
            "epoch 4 step 213 loss 0.0683877244591713\n",
            "epoch 4 step 214 loss 0.05760124698281288\n",
            "epoch 4 step 215 loss 0.07425663620233536\n",
            "epoch 4 step 216 loss 0.07043009996414185\n",
            "epoch 4 step 217 loss 0.0686737596988678\n",
            "epoch 4 step 218 loss 0.0734243243932724\n",
            "epoch 4 step 219 loss 0.04219888895750046\n",
            "epoch 4 step 220 loss 0.038171567022800446\n",
            "epoch 4 step 221 loss 0.05549073591828346\n",
            "epoch 4 step 222 loss 0.09235765039920807\n",
            "epoch 4 step 223 loss 0.0650220662355423\n",
            "epoch 4 step 224 loss 0.060486745089292526\n",
            "epoch 4 step 225 loss 0.050592225044965744\n",
            "epoch 4 step 226 loss 0.06843620538711548\n",
            "epoch 4 step 227 loss 0.08760486543178558\n",
            "epoch 4 step 228 loss 0.05210153013467789\n",
            "epoch 4 step 229 loss 0.06438382714986801\n",
            "epoch 4 step 230 loss 0.05300619453191757\n",
            "epoch 4 step 231 loss 0.04844526946544647\n",
            "epoch 4 step 232 loss 0.06382770091295242\n",
            "epoch 4 step 233 loss 0.05906856432557106\n",
            "epoch 4 step 234 loss 0.07048378139734268\n",
            "epoch 4 step 235 loss 0.06897339224815369\n",
            "epoch 4 step 236 loss 0.10151830315589905\n",
            "epoch 4 step 237 loss 0.05105867609381676\n",
            "epoch 4 step 238 loss 0.04480696842074394\n",
            "epoch 4 step 239 loss 0.04913775622844696\n",
            "epoch 4 step 240 loss 0.08135252445936203\n",
            "epoch 4 step 241 loss 0.07096919417381287\n",
            "epoch 4 step 242 loss 0.10715213418006897\n",
            "epoch 4 step 243 loss 0.0596560463309288\n",
            "epoch 4 step 244 loss 0.07718219608068466\n",
            "epoch 4 step 245 loss 0.06059461086988449\n",
            "epoch 4 step 246 loss 0.06401117146015167\n",
            "epoch 4 step 247 loss 0.07829155027866364\n",
            "epoch 4 step 248 loss 0.03588922321796417\n",
            "epoch 4 step 249 loss 0.0511854887008667\n",
            "epoch 4 step 250 loss 0.06813178956508636\n",
            "epoch 4 step 251 loss 0.07518405467271805\n",
            "epoch 4 step 252 loss 0.06177740916609764\n",
            "epoch 4 step 253 loss 0.09447729587554932\n",
            "epoch 4 step 254 loss 0.048760510981082916\n",
            "epoch 4 step 255 loss 0.0816599428653717\n",
            "epoch 4 step 256 loss 0.058621056377887726\n",
            "epoch 4 step 257 loss 0.08727581053972244\n",
            "epoch 4 step 258 loss 0.06040814518928528\n",
            "epoch 4 step 259 loss 0.05837259441614151\n",
            "epoch 4 step 260 loss 0.0637473613023758\n",
            "epoch 4 step 261 loss 0.0870433896780014\n",
            "epoch 4 step 262 loss 0.055675093084573746\n",
            "epoch 4 step 263 loss 0.04104674607515335\n",
            "epoch 4 step 264 loss 0.03629482910037041\n",
            "epoch 4 step 265 loss 0.028561126440763474\n",
            "epoch 4 step 266 loss 0.0974956676363945\n",
            "epoch 4 step 267 loss 0.04765317961573601\n",
            "epoch 4 step 268 loss 0.047256771475076675\n",
            "epoch 4 step 269 loss 0.04585789889097214\n",
            "epoch 4 step 270 loss 0.07731485366821289\n",
            "epoch 4 step 271 loss 0.06078193336725235\n",
            "epoch 4 step 272 loss 0.0925331562757492\n",
            "epoch 4 step 273 loss 0.06592515110969543\n",
            "epoch 4 step 274 loss 0.0917692631483078\n",
            "epoch 4 step 275 loss 0.06189434230327606\n",
            "epoch 4 step 276 loss 0.06162016838788986\n",
            "epoch 4 step 277 loss 0.08226469904184341\n",
            "epoch 4 step 278 loss 0.06282611191272736\n",
            "epoch 4 step 279 loss 0.059456717222929\n",
            "epoch 4 step 280 loss 0.06915578246116638\n",
            "epoch 4 step 281 loss 0.07483964413404465\n",
            "epoch 4 step 282 loss 0.05233725905418396\n",
            "epoch 4 step 283 loss 0.05956937000155449\n",
            "epoch 4 step 284 loss 0.08717961609363556\n",
            "epoch 4 step 285 loss 0.034204646944999695\n",
            "epoch 4 step 286 loss 0.062114860862493515\n",
            "epoch 4 step 287 loss 0.042384132742881775\n",
            "epoch 4 step 288 loss 0.05961590260267258\n",
            "epoch 4 step 289 loss 0.09028221666812897\n",
            "epoch 4 step 290 loss 0.05245581269264221\n",
            "epoch 4 step 291 loss 0.0905931144952774\n",
            "epoch 4 step 292 loss 0.07620522379875183\n",
            "epoch 4 step 293 loss 0.051129914820194244\n",
            "epoch 4 step 294 loss 0.08230750262737274\n",
            "epoch 4 step 295 loss 0.05212307721376419\n",
            "epoch 4 step 296 loss 0.046314261853694916\n",
            "epoch 4 step 297 loss 0.06828127801418304\n",
            "epoch 4 step 298 loss 0.06152673065662384\n",
            "epoch 4 step 299 loss 0.04824099689722061\n",
            "epoch 4 step 300 loss 0.052812449634075165\n",
            "epoch 4 step 301 loss 0.041863132268190384\n",
            "epoch 4 step 302 loss 0.05197783559560776\n",
            "epoch 4 step 303 loss 0.08854690194129944\n",
            "epoch 4 step 304 loss 0.07871435582637787\n",
            "epoch 4 step 305 loss 0.04899735003709793\n",
            "epoch 4 step 306 loss 0.09854947030544281\n",
            "epoch 4 step 307 loss 0.04570991173386574\n",
            "epoch 4 step 308 loss 0.05212439224123955\n",
            "epoch 4 step 309 loss 0.05236426740884781\n",
            "epoch 4 step 310 loss 0.04698201268911362\n",
            "epoch 4 step 311 loss 0.06191524863243103\n",
            "epoch 4 step 312 loss 0.0541439950466156\n",
            "epoch 4 step 313 loss 0.07374319434165955\n",
            "epoch 4 step 314 loss 0.0515374056994915\n",
            "epoch 4 step 315 loss 0.04772163927555084\n",
            "epoch 4 step 316 loss 0.11172576248645782\n",
            "epoch 4 step 317 loss 0.0691928043961525\n",
            "epoch 4 step 318 loss 0.048487283289432526\n",
            "epoch 4 step 319 loss 0.09773068130016327\n",
            "epoch 4 step 320 loss 0.05076996237039566\n",
            "epoch 4 step 321 loss 0.0760924369096756\n",
            "epoch 4 step 322 loss 0.07830006629228592\n",
            "epoch 4 step 323 loss 0.07402229309082031\n",
            "epoch 4 step 324 loss 0.09366700053215027\n",
            "epoch 4 step 325 loss 0.07015568763017654\n",
            "epoch 4 step 326 loss 0.06467053294181824\n",
            "epoch 4 step 327 loss 0.11794036626815796\n",
            "epoch 4 step 328 loss 0.0781930461525917\n",
            "epoch 4 step 329 loss 0.04715203493833542\n",
            "epoch 4 step 330 loss 0.07578978687524796\n",
            "epoch 4 step 331 loss 0.04615175351500511\n",
            "epoch 4 step 332 loss 0.10776571184396744\n",
            "epoch 4 step 333 loss 0.08048252761363983\n",
            "epoch 4 step 334 loss 0.05541335791349411\n",
            "epoch 4 step 335 loss 0.0247817263007164\n",
            "epoch 4 step 336 loss 0.08537589013576508\n",
            "epoch 4 step 337 loss 0.05890458822250366\n",
            "epoch 4 step 338 loss 0.08196388930082321\n",
            "epoch 4 step 339 loss 0.04506890848278999\n",
            "epoch 4 step 340 loss 0.05722178518772125\n",
            "epoch 4 step 341 loss 0.06379850208759308\n",
            "epoch 4 step 342 loss 0.036084577441215515\n",
            "epoch 4 step 343 loss 0.09683632850646973\n",
            "epoch 4 step 344 loss 0.060262858867645264\n",
            "epoch 4 step 345 loss 0.19225406646728516\n",
            "epoch 4 step 346 loss 0.08427775651216507\n",
            "epoch 4 step 347 loss 0.0687328428030014\n",
            "epoch 4 step 348 loss 0.073496513068676\n",
            "epoch 4 step 349 loss 0.04868152737617493\n",
            "epoch 4 step 350 loss 0.03880643844604492\n",
            "epoch 4 step 351 loss 0.08232181519269943\n",
            "epoch 4 step 352 loss 0.10158634930849075\n",
            "epoch 4 step 353 loss 0.06083036586642265\n",
            "epoch 4 step 354 loss 0.0616416409611702\n",
            "epoch 4 step 355 loss 0.1466224491596222\n",
            "epoch 4 step 356 loss 0.06082528457045555\n",
            "epoch 4 step 357 loss 0.053316064178943634\n",
            "epoch 4 step 358 loss 0.0640861839056015\n",
            "epoch 4 step 359 loss 0.07495881617069244\n",
            "epoch 4 step 360 loss 0.1146388128399849\n",
            "epoch 4 step 361 loss 0.060870956629514694\n",
            "epoch 4 step 362 loss 0.09418551623821259\n",
            "epoch 4 step 363 loss 0.06957632303237915\n",
            "epoch 4 step 364 loss 0.08653245121240616\n",
            "epoch 4 step 365 loss 0.06675642728805542\n",
            "epoch 4 step 366 loss 0.06476697325706482\n",
            "epoch 4 step 367 loss 0.11530950665473938\n",
            "epoch 4 step 368 loss 0.10974924266338348\n",
            "epoch 4 step 369 loss 0.050535038113594055\n",
            "epoch 4 step 370 loss 0.061498284339904785\n",
            "epoch 4 step 371 loss 0.05542731657624245\n",
            "epoch 4 step 372 loss 0.04019265994429588\n",
            "epoch 4 step 373 loss 0.07012607157230377\n",
            "epoch 4 step 374 loss 0.035467728972435\n",
            "epoch 4 step 375 loss 0.09176322817802429\n",
            "epoch 4 step 376 loss 0.07164642214775085\n",
            "epoch 4 step 377 loss 0.05606570094823837\n",
            "epoch 4 step 378 loss 0.0846717432141304\n",
            "epoch 4 step 379 loss 0.07025735080242157\n",
            "epoch 4 step 380 loss 0.08460643142461777\n",
            "epoch 4 step 381 loss 0.09932856261730194\n",
            "epoch 4 step 382 loss 0.03422314301133156\n",
            "epoch 4 step 383 loss 0.052765827625989914\n",
            "epoch 4 step 384 loss 0.06199246644973755\n",
            "epoch 4 step 385 loss 0.12378375977277756\n",
            "epoch 4 step 386 loss 0.040579259395599365\n",
            "epoch 4 step 387 loss 0.05027330666780472\n",
            "epoch 4 step 388 loss 0.08185938000679016\n",
            "epoch 4 step 389 loss 0.05124017596244812\n",
            "epoch 4 step 390 loss 0.07628338038921356\n",
            "epoch 4 step 391 loss 0.06433161348104477\n",
            "epoch 4 step 392 loss 0.03262002766132355\n",
            "epoch 4 step 393 loss 0.0817832201719284\n",
            "epoch 4 step 394 loss 0.0788722038269043\n",
            "epoch 4 step 395 loss 0.07687206566333771\n",
            "epoch 4 step 396 loss 0.09057573974132538\n",
            "epoch 4 step 397 loss 0.09681858122348785\n",
            "epoch 4 step 398 loss 0.05809660628437996\n",
            "epoch 4 step 399 loss 0.07641824334859848\n",
            "epoch 4 step 400 loss 0.04885954409837723\n",
            "epoch 4 step 401 loss 0.05041128396987915\n",
            "epoch 4 step 402 loss 0.055524781346321106\n",
            "epoch 4 step 403 loss 0.04614786431193352\n",
            "epoch 4 step 404 loss 0.10419441759586334\n",
            "epoch 4 step 405 loss 0.09314373880624771\n",
            "epoch 4 step 406 loss 0.0782097727060318\n",
            "epoch 4 step 407 loss 0.061986081302165985\n",
            "epoch 4 step 408 loss 0.1108974739909172\n",
            "epoch 4 step 409 loss 0.06995582580566406\n",
            "epoch 4 step 410 loss 0.047217853367328644\n",
            "epoch 4 step 411 loss 0.062339846044778824\n",
            "epoch 4 step 412 loss 0.04234813153743744\n",
            "epoch 4 step 413 loss 0.08543188869953156\n",
            "epoch 4 step 414 loss 0.05249108374118805\n",
            "epoch 4 step 415 loss 0.05077323317527771\n",
            "epoch 4 step 416 loss 0.06827063113451004\n",
            "epoch 4 step 417 loss 0.1121755912899971\n",
            "epoch 4 step 418 loss 0.052837200462818146\n",
            "epoch 4 step 419 loss 0.1231858879327774\n",
            "epoch 4 step 420 loss 0.08847522735595703\n",
            "epoch 4 step 421 loss 0.05051673203706741\n",
            "epoch 4 step 422 loss 0.06186927482485771\n",
            "epoch 4 step 423 loss 0.07970251142978668\n",
            "epoch 4 step 424 loss 0.06619320809841156\n",
            "epoch 4 step 425 loss 0.0658288449048996\n",
            "epoch 4 step 426 loss 0.03974294289946556\n",
            "epoch 4 step 427 loss 0.06643329560756683\n",
            "epoch 4 step 428 loss 0.07099082320928574\n",
            "epoch 4 step 429 loss 0.07743924111127853\n",
            "epoch 4 step 430 loss 0.043230343610048294\n",
            "epoch 4 step 431 loss 0.0846545547246933\n",
            "epoch 4 step 432 loss 0.0348607562482357\n",
            "epoch 4 step 433 loss 0.0641004666686058\n",
            "epoch 4 step 434 loss 0.09702560305595398\n",
            "epoch 4 step 435 loss 0.06693068146705627\n",
            "epoch 4 step 436 loss 0.07760097086429596\n",
            "epoch 4 step 437 loss 0.08932872116565704\n",
            "epoch 4 step 438 loss 0.07310111820697784\n",
            "epoch 4 step 439 loss 0.05302571877837181\n",
            "epoch 4 step 440 loss 0.11393134295940399\n",
            "epoch 4 step 441 loss 0.08219701051712036\n",
            "epoch 4 step 442 loss 0.08485904335975647\n",
            "epoch 4 step 443 loss 0.07391239702701569\n",
            "epoch 4 step 444 loss 0.06504884362220764\n",
            "epoch 4 step 445 loss 0.0631038248538971\n",
            "epoch 4 step 446 loss 0.05989985167980194\n",
            "epoch 4 step 447 loss 0.0683598518371582\n",
            "epoch 4 step 448 loss 0.1257210671901703\n",
            "epoch 4 step 449 loss 0.09654349088668823\n",
            "epoch 4 step 450 loss 0.06036432087421417\n",
            "epoch 4 step 451 loss 0.08113615214824677\n",
            "epoch 4 step 452 loss 0.06623440980911255\n",
            "epoch 4 step 453 loss 0.07410340011119843\n",
            "epoch 4 step 454 loss 0.08907409757375717\n",
            "epoch 4 step 455 loss 0.05926608294248581\n",
            "epoch 4 step 456 loss 0.07360102236270905\n",
            "epoch 4 step 457 loss 0.05469278246164322\n",
            "epoch 4 step 458 loss 0.07504481077194214\n",
            "epoch 4 step 459 loss 0.07396072149276733\n",
            "epoch 4 step 460 loss 0.05870595574378967\n",
            "epoch 4 step 461 loss 0.03720204904675484\n",
            "epoch 4 step 462 loss 0.0693889781832695\n",
            "epoch 4 step 463 loss 0.046021539717912674\n",
            "epoch 4 step 464 loss 0.03991115838289261\n",
            "epoch 4 step 465 loss 0.031894832849502563\n",
            "epoch 4 step 466 loss 0.08227840065956116\n",
            "epoch 4 step 467 loss 0.06184661388397217\n",
            "epoch 4 step 468 loss 0.06405645608901978\n",
            "epoch 4 step 469 loss 0.10027173906564713\n",
            "epoch 4 step 470 loss 0.09748240560293198\n",
            "epoch 4 step 471 loss 0.05634196475148201\n",
            "epoch 4 step 472 loss 0.08115561306476593\n",
            "epoch 4 step 473 loss 0.06752602756023407\n",
            "epoch 4 step 474 loss 0.06946499645709991\n",
            "epoch 4 step 475 loss 0.07878146320581436\n",
            "epoch 4 step 476 loss 0.07062074542045593\n",
            "epoch 4 step 477 loss 0.05530235916376114\n",
            "epoch 4 step 478 loss 0.06270358711481094\n",
            "epoch 4 step 479 loss 0.06433621048927307\n",
            "epoch 4 step 480 loss 0.03769468516111374\n",
            "epoch 4 step 481 loss 0.061388492584228516\n",
            "epoch 4 step 482 loss 0.052821964025497437\n",
            "epoch 4 step 483 loss 0.07976360619068146\n",
            "epoch 4 step 484 loss 0.09832403063774109\n",
            "epoch 4 step 485 loss 0.048383112996816635\n",
            "epoch 4 step 486 loss 0.047961171716451645\n",
            "epoch 4 step 487 loss 0.05593784153461456\n",
            "epoch 4 step 488 loss 0.044998325407505035\n",
            "epoch 4 step 489 loss 0.08491060882806778\n",
            "epoch 4 step 490 loss 0.042188290506601334\n",
            "epoch 4 step 491 loss 0.07803051173686981\n",
            "epoch 4 step 492 loss 0.08999612927436829\n",
            "epoch 4 step 493 loss 0.10631846636533737\n",
            "epoch 4 step 494 loss 0.041774820536375046\n",
            "epoch 4 step 495 loss 0.06775879859924316\n",
            "epoch 4 step 496 loss 0.06556743383407593\n",
            "epoch 4 step 497 loss 0.12902933359146118\n",
            "epoch 4 step 498 loss 0.07158758491277695\n",
            "epoch 4 step 499 loss 0.04003934562206268\n",
            "epoch 4 step 500 loss 0.04506659507751465\n",
            "epoch 4 step 501 loss 0.06413064897060394\n",
            "epoch 4 step 502 loss 0.056313686072826385\n",
            "epoch 4 step 503 loss 0.04353592172265053\n",
            "epoch 4 step 504 loss 0.04030980169773102\n",
            "epoch 4 step 505 loss 0.05284629762172699\n",
            "epoch 4 step 506 loss 0.0823679119348526\n",
            "epoch 4 step 507 loss 0.05188780277967453\n",
            "epoch 4 step 508 loss 0.0571817122399807\n",
            "epoch 4 step 509 loss 0.06599308550357819\n",
            "epoch 4 step 510 loss 0.04542924463748932\n",
            "epoch 4 step 511 loss 0.07798993587493896\n",
            "epoch 4 step 512 loss 0.04998335987329483\n",
            "epoch 4 step 513 loss 0.1389659345149994\n",
            "epoch 4 step 514 loss 0.05652877688407898\n",
            "epoch 4 step 515 loss 0.06140594929456711\n",
            "epoch 4 step 516 loss 0.07706557214260101\n",
            "epoch 4 step 517 loss 0.05336243286728859\n",
            "epoch 4 step 518 loss 0.0710110291838646\n",
            "epoch 4 step 519 loss 0.08146623522043228\n",
            "epoch 4 step 520 loss 0.05026222765445709\n",
            "epoch 4 step 521 loss 0.08756576478481293\n",
            "epoch 4 step 522 loss 0.07279660552740097\n",
            "epoch 4 step 523 loss 0.058516234159469604\n",
            "epoch 4 step 524 loss 0.09657981991767883\n",
            "epoch 4 step 525 loss 0.07133931666612625\n",
            "epoch 4 step 526 loss 0.06711827963590622\n",
            "epoch 4 step 527 loss 0.06301595270633698\n",
            "epoch 4 step 528 loss 0.04812036454677582\n",
            "epoch 4 step 529 loss 0.07084356248378754\n",
            "epoch 4 step 530 loss 0.08396957814693451\n",
            "epoch 4 step 531 loss 0.0755646675825119\n",
            "epoch 4 step 532 loss 0.06974826753139496\n",
            "epoch 4 step 533 loss 0.08673514425754547\n",
            "epoch 4 step 534 loss 0.04501019045710564\n",
            "epoch 4 step 535 loss 0.07491477578878403\n",
            "epoch 4 step 536 loss 0.039247967302799225\n",
            "epoch 4 step 537 loss 0.07018377631902695\n",
            "epoch 4 step 538 loss 0.046053506433963776\n",
            "epoch 4 step 539 loss 0.04648837074637413\n",
            "epoch 4 step 540 loss 0.05722348019480705\n",
            "epoch 4 step 541 loss 0.08722000569105148\n",
            "epoch 4 step 542 loss 0.07430505007505417\n",
            "epoch 4 step 543 loss 0.06894955039024353\n",
            "epoch 4 step 544 loss 0.0723157525062561\n",
            "epoch 4 step 545 loss 0.06905834376811981\n",
            "epoch 4 step 546 loss 0.0312613882124424\n",
            "epoch 4 step 547 loss 0.10352039337158203\n",
            "epoch 4 step 548 loss 0.0656651109457016\n",
            "epoch 4 step 549 loss 0.08457549661397934\n",
            "epoch 4 step 550 loss 0.053825244307518005\n",
            "epoch 4 step 551 loss 0.08684339374303818\n",
            "epoch 4 step 552 loss 0.09793895483016968\n",
            "epoch 4 step 553 loss 0.04476352035999298\n",
            "epoch 4 step 554 loss 0.033445846289396286\n",
            "epoch 4 step 555 loss 0.09146853536367416\n",
            "epoch 4 step 556 loss 0.06351913511753082\n",
            "epoch 4 step 557 loss 0.09395984560251236\n",
            "epoch 4 step 558 loss 0.04415956139564514\n",
            "epoch 4 step 559 loss 0.07075774669647217\n",
            "epoch 4 step 560 loss 0.0919785350561142\n",
            "epoch 4 step 561 loss 0.03873978182673454\n",
            "epoch 4 step 562 loss 0.04145343601703644\n",
            "epoch 4 step 563 loss 0.08771936595439911\n",
            "epoch 4 step 564 loss 0.07741101086139679\n",
            "epoch 4 step 565 loss 0.0548563078045845\n",
            "epoch 4 step 566 loss 0.05466953292489052\n",
            "epoch 4 step 567 loss 0.08190857619047165\n",
            "epoch 4 step 568 loss 0.06601221859455109\n",
            "epoch 4 step 569 loss 0.0637025237083435\n",
            "epoch 4 step 570 loss 0.04111118242144585\n",
            "epoch 4 step 571 loss 0.08264558017253876\n",
            "epoch 4 step 572 loss 0.057286929339170456\n",
            "epoch 4 step 573 loss 0.06712014228105545\n",
            "epoch 4 step 574 loss 0.033362846821546555\n",
            "epoch 4 step 575 loss 0.062307391315698624\n",
            "epoch 4 step 576 loss 0.06497298926115036\n",
            "epoch 4 step 577 loss 0.08222678303718567\n",
            "epoch 4 step 578 loss 0.05248725041747093\n",
            "epoch 4 step 579 loss 0.033792927861213684\n",
            "epoch 4 step 580 loss 0.05652831122279167\n",
            "epoch 4 step 581 loss 0.09624660015106201\n",
            "epoch 4 step 582 loss 0.0566691979765892\n",
            "epoch 4 step 583 loss 0.04365943372249603\n",
            "epoch 4 step 584 loss 0.0607370026409626\n",
            "epoch 4 step 585 loss 0.05014853551983833\n",
            "epoch 4 step 586 loss 0.06770098209381104\n",
            "epoch 4 step 587 loss 0.046409521251916885\n",
            "epoch 4 step 588 loss 0.044650301337242126\n",
            "epoch 4 step 589 loss 0.0339861698448658\n",
            "epoch 4 step 590 loss 0.05590324103832245\n",
            "epoch 4 step 591 loss 0.05539467930793762\n",
            "epoch 4 step 592 loss 0.07293969392776489\n",
            "epoch 4 step 593 loss 0.06387485563755035\n",
            "epoch 4 step 594 loss 0.052861008793115616\n",
            "epoch 4 step 595 loss 0.1077713817358017\n",
            "epoch 4 step 596 loss 0.10789132118225098\n",
            "epoch 4 step 597 loss 0.06242147833108902\n",
            "epoch 4 step 598 loss 0.08022929728031158\n",
            "epoch 4 step 599 loss 0.06232411786913872\n",
            "epoch 4 step 600 loss 0.04128574579954147\n",
            "epoch 4 step 601 loss 0.06872577965259552\n",
            "epoch 4 step 602 loss 0.06494583934545517\n",
            "epoch 4 step 603 loss 0.06755995750427246\n",
            "epoch 4 step 604 loss 0.04463477432727814\n",
            "epoch 4 step 605 loss 0.0402812734246254\n",
            "epoch 4 step 606 loss 0.09152044355869293\n",
            "epoch 4 step 607 loss 0.08225419372320175\n",
            "epoch 4 step 608 loss 0.055534448474645615\n",
            "epoch 4 step 609 loss 0.06770547479391098\n",
            "epoch 4 step 610 loss 0.10029472410678864\n",
            "epoch 4 step 611 loss 0.05303457751870155\n",
            "epoch 4 step 612 loss 0.050840459764003754\n",
            "epoch 4 step 613 loss 0.06898142397403717\n",
            "epoch 4 step 614 loss 0.047049909830093384\n",
            "epoch 4 step 615 loss 0.05500433221459389\n",
            "epoch 4 step 616 loss 0.05222278833389282\n",
            "epoch 4 step 617 loss 0.062188487499952316\n",
            "epoch 4 step 618 loss 0.07016795873641968\n",
            "epoch 4 step 619 loss 0.05603492632508278\n",
            "epoch 4 step 620 loss 0.03948531299829483\n",
            "epoch 4 step 621 loss 0.06284493207931519\n",
            "epoch 4 step 622 loss 0.035996437072753906\n",
            "epoch 4 step 623 loss 0.0475182943046093\n",
            "epoch 4 step 624 loss 0.07020841538906097\n",
            "epoch 4 step 625 loss 0.0627036839723587\n",
            "epoch 4 step 626 loss 0.06143619120121002\n",
            "epoch 4 step 627 loss 0.04835640639066696\n",
            "epoch 4 step 628 loss 0.07759419828653336\n",
            "epoch 4 step 629 loss 0.08031555265188217\n",
            "epoch 4 step 630 loss 0.057873573154211044\n",
            "epoch 4 step 631 loss 0.061715949326753616\n",
            "epoch 4 step 632 loss 0.09448717534542084\n",
            "epoch 4 step 633 loss 0.056736405938863754\n",
            "epoch 4 step 634 loss 0.07750378549098969\n",
            "epoch 4 step 635 loss 0.030385717749595642\n",
            "epoch 4 step 636 loss 0.07016681879758835\n",
            "epoch 4 step 637 loss 0.06211034581065178\n",
            "epoch 4 step 638 loss 0.06074056774377823\n",
            "epoch 4 step 639 loss 0.050751619040966034\n",
            "epoch 4 step 640 loss 0.10542737692594528\n",
            "epoch 4 step 641 loss 0.048188552260398865\n",
            "epoch 4 step 642 loss 0.05928890407085419\n",
            "epoch 4 step 643 loss 0.05417511612176895\n",
            "epoch 4 step 644 loss 0.09204600751399994\n",
            "epoch 4 step 645 loss 0.04184238985180855\n",
            "epoch 4 step 646 loss 0.04180212318897247\n",
            "epoch 4 step 647 loss 0.031970664858818054\n",
            "epoch 4 step 648 loss 0.036351218819618225\n",
            "epoch 4 step 649 loss 0.06968821585178375\n",
            "epoch 4 step 650 loss 0.028723981231451035\n",
            "epoch 4 step 651 loss 0.0407850444316864\n",
            "epoch 4 step 652 loss 0.04452221840620041\n",
            "epoch 4 step 653 loss 0.06671491265296936\n",
            "epoch 4 step 654 loss 0.04672098532319069\n",
            "epoch 4 step 655 loss 0.049913134425878525\n",
            "epoch 4 step 656 loss 0.06435050070285797\n",
            "epoch 4 step 657 loss 0.08528715372085571\n",
            "epoch 4 step 658 loss 0.08045671135187149\n",
            "epoch 4 step 659 loss 0.0781678855419159\n",
            "epoch 4 step 660 loss 0.05139937251806259\n",
            "epoch 4 step 661 loss 0.053904540836811066\n",
            "epoch 4 step 662 loss 0.07503839582204819\n",
            "epoch 4 step 663 loss 0.07669691741466522\n",
            "epoch 4 step 664 loss 0.031600870192050934\n",
            "epoch 4 step 665 loss 0.09161623567342758\n",
            "epoch 4 step 666 loss 0.023124434053897858\n",
            "epoch 4 step 667 loss 0.03265327587723732\n",
            "epoch 4 step 668 loss 0.06549388915300369\n",
            "epoch 4 step 669 loss 0.04461270570755005\n",
            "epoch 4 step 670 loss 0.0955052375793457\n",
            "epoch 4 step 671 loss 0.05313762277364731\n",
            "epoch 4 step 672 loss 0.08005919307470322\n",
            "epoch 4 step 673 loss 0.07076730579137802\n",
            "epoch 4 step 674 loss 0.046830419450998306\n",
            "epoch 4 step 675 loss 0.06336210668087006\n",
            "epoch 4 step 676 loss 0.026384219527244568\n",
            "epoch 4 step 677 loss 0.042031072080135345\n",
            "epoch 4 step 678 loss 0.06048465892672539\n",
            "epoch 4 step 679 loss 0.03919471800327301\n",
            "epoch 4 step 680 loss 0.04595404118299484\n",
            "epoch 4 step 681 loss 0.05909832566976547\n",
            "epoch 4 step 682 loss 0.08197984099388123\n",
            "epoch 4 step 683 loss 0.13085824251174927\n",
            "epoch 4 step 684 loss 0.046156615018844604\n",
            "epoch 4 step 685 loss 0.06017095595598221\n",
            "epoch 4 step 686 loss 0.035710033029317856\n",
            "epoch 4 step 687 loss 0.08121640980243683\n",
            "epoch 4 step 688 loss 0.08954723179340363\n",
            "epoch 4 step 689 loss 0.05639638006687164\n",
            "epoch 4 step 690 loss 0.11703373491764069\n",
            "epoch 4 step 691 loss 0.04463280364871025\n",
            "epoch 4 step 692 loss 0.0406235009431839\n",
            "epoch 4 step 693 loss 0.04574303328990936\n",
            "epoch 4 step 694 loss 0.04631148278713226\n",
            "epoch 4 step 695 loss 0.0521092563867569\n",
            "epoch 4 step 696 loss 0.07062911242246628\n",
            "epoch 4 step 697 loss 0.0600414052605629\n",
            "epoch 4 step 698 loss 0.03845443204045296\n",
            "epoch 4 step 699 loss 0.06607300788164139\n",
            "epoch 4 step 700 loss 0.06419973075389862\n",
            "epoch 4 step 701 loss 0.06336750835180283\n",
            "epoch 4 step 702 loss 0.05214981362223625\n",
            "epoch 4 step 703 loss 0.08419202268123627\n",
            "epoch 4 step 704 loss 0.1349562108516693\n",
            "epoch 4 step 705 loss 0.04199914634227753\n",
            "epoch 4 step 706 loss 0.08365730941295624\n",
            "epoch 4 step 707 loss 0.054351117461919785\n",
            "epoch 4 step 708 loss 0.06547213345766068\n",
            "epoch 4 step 709 loss 0.05807088315486908\n",
            "epoch 4 step 710 loss 0.060817793011665344\n",
            "epoch 4 step 711 loss 0.0708022266626358\n",
            "epoch 4 step 712 loss 0.04833574593067169\n",
            "epoch 4 step 713 loss 0.05766969546675682\n",
            "epoch 4 step 714 loss 0.042231231927871704\n",
            "epoch 4 step 715 loss 0.08869065344333649\n",
            "epoch 4 step 716 loss 0.05937157943844795\n",
            "epoch 4 step 717 loss 0.054129473865032196\n",
            "epoch 4 step 718 loss 0.07523230463266373\n",
            "epoch 4 step 719 loss 0.09381963312625885\n",
            "epoch 4 step 720 loss 0.03395106643438339\n",
            "epoch 4 step 721 loss 0.05075245350599289\n",
            "epoch 4 step 722 loss 0.030025959014892578\n",
            "epoch 4 step 723 loss 0.046709317713975906\n",
            "epoch 4 step 724 loss 0.05068035423755646\n",
            "epoch 4 step 725 loss 0.06681326031684875\n",
            "epoch 4 step 726 loss 0.051543258130550385\n",
            "epoch 4 step 727 loss 0.03434101492166519\n",
            "epoch 4 step 728 loss 0.049989424645900726\n",
            "epoch 4 step 729 loss 0.04659491404891014\n",
            "epoch 4 step 730 loss 0.07134688645601273\n",
            "epoch 4 step 731 loss 0.05779530107975006\n",
            "epoch 4 step 732 loss 0.03894317150115967\n",
            "epoch 4 step 733 loss 0.03272915631532669\n",
            "epoch 4 step 734 loss 0.025237318128347397\n",
            "epoch 4 step 735 loss 0.08074219524860382\n",
            "epoch 4 step 736 loss 0.04932456836104393\n",
            "epoch 4 step 737 loss 0.05258452892303467\n",
            "epoch 4 step 738 loss 0.041915059089660645\n",
            "epoch 4 step 739 loss 0.096991166472435\n",
            "epoch 4 step 740 loss 0.031777676194906235\n",
            "epoch 4 step 741 loss 0.07021504640579224\n",
            "epoch 4 step 742 loss 0.1043611615896225\n",
            "epoch 4 step 743 loss 0.06033947318792343\n",
            "epoch 4 step 744 loss 0.05803843215107918\n",
            "epoch 4 step 745 loss 0.09247928857803345\n",
            "epoch 4 step 746 loss 0.037753261625766754\n",
            "epoch 4 step 747 loss 0.05565563961863518\n",
            "epoch 4 step 748 loss 0.03846557438373566\n",
            "epoch 4 step 749 loss 0.055348753929138184\n",
            "epoch 4 step 750 loss 0.06829345226287842\n",
            "epoch 4 step 751 loss 0.09954382479190826\n",
            "epoch 4 step 752 loss 0.04143076390028\n",
            "epoch 4 step 753 loss 0.06451232731342316\n",
            "epoch 4 step 754 loss 0.0781935527920723\n",
            "epoch 4 step 755 loss 0.05800389125943184\n",
            "epoch 4 step 756 loss 0.0662953108549118\n",
            "epoch 4 step 757 loss 0.07904243469238281\n",
            "epoch 4 step 758 loss 0.05877433717250824\n",
            "epoch 4 step 759 loss 0.04991782456636429\n",
            "epoch 4 step 760 loss 0.03302396833896637\n",
            "epoch 4 step 761 loss 0.07462340593338013\n",
            "epoch 4 step 762 loss 0.11148778349161148\n",
            "epoch 4 step 763 loss 0.030842915177345276\n",
            "epoch 4 step 764 loss 0.049747250974178314\n",
            "epoch 4 step 765 loss 0.047468286007642746\n",
            "epoch 4 step 766 loss 0.06402593851089478\n",
            "epoch 4 step 767 loss 0.049118705093860626\n",
            "epoch 4 step 768 loss 0.061016205698251724\n",
            "epoch 4 step 769 loss 0.018827883526682854\n",
            "epoch 4 step 770 loss 0.025899527594447136\n",
            "epoch 4 step 771 loss 0.07839418202638626\n",
            "epoch 4 step 772 loss 0.04165169596672058\n",
            "epoch 4 step 773 loss 0.03669970482587814\n",
            "epoch 4 step 774 loss 0.07788300514221191\n",
            "epoch 4 step 775 loss 0.04460147023200989\n",
            "epoch 4 step 776 loss 0.05657367408275604\n",
            "epoch 4 step 777 loss 0.09723175317049026\n",
            "epoch 4 step 778 loss 0.09135499596595764\n",
            "epoch 4 step 779 loss 0.048946551978588104\n",
            "epoch 4 step 780 loss 0.08854857087135315\n",
            "epoch 4 step 781 loss 0.08234618604183197\n",
            "epoch 5 step 0 loss 0.06864845752716064\n",
            "epoch 5 step 1 loss 0.05834013968706131\n",
            "epoch 5 step 2 loss 0.04728414863348007\n",
            "epoch 5 step 3 loss 0.04092226177453995\n",
            "epoch 5 step 4 loss 0.04199529439210892\n",
            "epoch 5 step 5 loss 0.04915324226021767\n",
            "epoch 5 step 6 loss 0.0530817024409771\n",
            "epoch 5 step 7 loss 0.05302184820175171\n",
            "epoch 5 step 8 loss 0.05540864169597626\n",
            "epoch 5 step 9 loss 0.07147222757339478\n",
            "epoch 5 step 10 loss 0.06881256401538849\n",
            "epoch 5 step 11 loss 0.06994439661502838\n",
            "epoch 5 step 12 loss 0.03945932537317276\n",
            "epoch 5 step 13 loss 0.08358998596668243\n",
            "epoch 5 step 14 loss 0.038325779139995575\n",
            "epoch 5 step 15 loss 0.06149047613143921\n",
            "epoch 5 step 16 loss 0.11754149198532104\n",
            "epoch 5 step 17 loss 0.06067580729722977\n",
            "epoch 5 step 18 loss 0.05438515916466713\n",
            "epoch 5 step 19 loss 0.07321398705244064\n",
            "epoch 5 step 20 loss 0.031025035306811333\n",
            "epoch 5 step 21 loss 0.07029452919960022\n",
            "epoch 5 step 22 loss 0.04756924882531166\n",
            "epoch 5 step 23 loss 0.04739537462592125\n",
            "epoch 5 step 24 loss 0.04661179333925247\n",
            "epoch 5 step 25 loss 0.05110497027635574\n",
            "epoch 5 step 26 loss 0.041394904255867004\n",
            "epoch 5 step 27 loss 0.051270607858896255\n",
            "epoch 5 step 28 loss 0.08447808027267456\n",
            "epoch 5 step 29 loss 0.056609898805618286\n",
            "epoch 5 step 30 loss 0.038093917071819305\n",
            "epoch 5 step 31 loss 0.043801840394735336\n",
            "epoch 5 step 32 loss 0.07110142707824707\n",
            "epoch 5 step 33 loss 0.0763869434595108\n",
            "epoch 5 step 34 loss 0.06317722797393799\n",
            "epoch 5 step 35 loss 0.04435833543539047\n",
            "epoch 5 step 36 loss 0.050853949040174484\n",
            "epoch 5 step 37 loss 0.06441185623407364\n",
            "epoch 5 step 38 loss 0.04530555009841919\n",
            "epoch 5 step 39 loss 0.13363447785377502\n",
            "epoch 5 step 40 loss 0.048981960862874985\n",
            "epoch 5 step 41 loss 0.0721658393740654\n",
            "epoch 5 step 42 loss 0.05821489170193672\n",
            "epoch 5 step 43 loss 0.020847246050834656\n",
            "epoch 5 step 44 loss 0.06580977141857147\n",
            "epoch 5 step 45 loss 0.05111893638968468\n",
            "epoch 5 step 46 loss 0.05985914170742035\n",
            "epoch 5 step 47 loss 0.07611682265996933\n",
            "epoch 5 step 48 loss 0.06732726842164993\n",
            "epoch 5 step 49 loss 0.0906372144818306\n",
            "epoch 5 step 50 loss 0.10137026757001877\n",
            "epoch 5 step 51 loss 0.053436487913131714\n",
            "epoch 5 step 52 loss 0.09625977277755737\n",
            "epoch 5 step 53 loss 0.05481372028589249\n",
            "epoch 5 step 54 loss 0.08714920282363892\n",
            "epoch 5 step 55 loss 0.07125099003314972\n",
            "epoch 5 step 56 loss 0.03933950886130333\n",
            "epoch 5 step 57 loss 0.06385935097932816\n",
            "epoch 5 step 58 loss 0.05036071687936783\n",
            "epoch 5 step 59 loss 0.035716667771339417\n",
            "epoch 5 step 60 loss 0.07696664333343506\n",
            "epoch 5 step 61 loss 0.04822012782096863\n",
            "epoch 5 step 62 loss 0.05510557442903519\n",
            "epoch 5 step 63 loss 0.04734329879283905\n",
            "epoch 5 step 64 loss 0.04216368496417999\n",
            "epoch 5 step 65 loss 0.10124848783016205\n",
            "epoch 5 step 66 loss 0.080264613032341\n",
            "epoch 5 step 67 loss 0.0449514165520668\n",
            "epoch 5 step 68 loss 0.04086659103631973\n",
            "epoch 5 step 69 loss 0.058519214391708374\n",
            "epoch 5 step 70 loss 0.071382537484169\n",
            "epoch 5 step 71 loss 0.04060366377234459\n",
            "epoch 5 step 72 loss 0.04818125069141388\n",
            "epoch 5 step 73 loss 0.051495373249053955\n",
            "epoch 5 step 74 loss 0.038913946598768234\n",
            "epoch 5 step 75 loss 0.04082896560430527\n",
            "epoch 5 step 76 loss 0.057375214993953705\n",
            "epoch 5 step 77 loss 0.04257399961352348\n",
            "epoch 5 step 78 loss 0.05200947821140289\n",
            "epoch 5 step 79 loss 0.07526614516973495\n",
            "epoch 5 step 80 loss 0.05758189037442207\n",
            "epoch 5 step 81 loss 0.05797272175550461\n",
            "epoch 5 step 82 loss 0.05389793589711189\n",
            "epoch 5 step 83 loss 0.032093171030282974\n",
            "epoch 5 step 84 loss 0.08804367482662201\n",
            "epoch 5 step 85 loss 0.03421113267540932\n",
            "epoch 5 step 86 loss 0.05684145539999008\n",
            "epoch 5 step 87 loss 0.07549619674682617\n",
            "epoch 5 step 88 loss 0.04265337064862251\n",
            "epoch 5 step 89 loss 0.06971532851457596\n",
            "epoch 5 step 90 loss 0.061726443469524384\n",
            "epoch 5 step 91 loss 0.08032122999429703\n",
            "epoch 5 step 92 loss 0.05893969163298607\n",
            "epoch 5 step 93 loss 0.1376539170742035\n",
            "epoch 5 step 94 loss 0.07145717740058899\n",
            "epoch 5 step 95 loss 0.062599316239357\n",
            "epoch 5 step 96 loss 0.04715477675199509\n",
            "epoch 5 step 97 loss 0.05526323243975639\n",
            "epoch 5 step 98 loss 0.06622278690338135\n",
            "epoch 5 step 99 loss 0.06825687736272812\n",
            "epoch 5 step 100 loss 0.03730642795562744\n",
            "epoch 5 step 101 loss 0.11600667238235474\n",
            "epoch 5 step 102 loss 0.0765160396695137\n",
            "epoch 5 step 103 loss 0.0543089359998703\n",
            "epoch 5 step 104 loss 0.06324957311153412\n",
            "epoch 5 step 105 loss 0.049744345247745514\n",
            "epoch 5 step 106 loss 0.07504156231880188\n",
            "epoch 5 step 107 loss 0.0928880050778389\n",
            "epoch 5 step 108 loss 0.041479915380477905\n",
            "epoch 5 step 109 loss 0.059172626584768295\n",
            "epoch 5 step 110 loss 0.09310689568519592\n",
            "epoch 5 step 111 loss 0.075783371925354\n",
            "epoch 5 step 112 loss 0.08994300663471222\n",
            "epoch 5 step 113 loss 0.08053132146596909\n",
            "epoch 5 step 114 loss 0.07188014686107635\n",
            "epoch 5 step 115 loss 0.060915507376194\n",
            "epoch 5 step 116 loss 0.0463557131588459\n",
            "epoch 5 step 117 loss 0.03918561339378357\n",
            "epoch 5 step 118 loss 0.08037485182285309\n",
            "epoch 5 step 119 loss 0.06558509171009064\n",
            "epoch 5 step 120 loss 0.04374195262789726\n",
            "epoch 5 step 121 loss 0.06115923449397087\n",
            "epoch 5 step 122 loss 0.04231588542461395\n",
            "epoch 5 step 123 loss 0.08113539218902588\n",
            "epoch 5 step 124 loss 0.04771525785326958\n",
            "epoch 5 step 125 loss 0.04422364383935928\n",
            "epoch 5 step 126 loss 0.04495735466480255\n",
            "epoch 5 step 127 loss 0.05213828384876251\n",
            "epoch 5 step 128 loss 0.03969060629606247\n",
            "epoch 5 step 129 loss 0.04950316622853279\n",
            "epoch 5 step 130 loss 0.08678928017616272\n",
            "epoch 5 step 131 loss 0.05329893156886101\n",
            "epoch 5 step 132 loss 0.08028160035610199\n",
            "epoch 5 step 133 loss 0.05023137480020523\n",
            "epoch 5 step 134 loss 0.04649271070957184\n",
            "epoch 5 step 135 loss 0.08407898247241974\n",
            "epoch 5 step 136 loss 0.08063890784978867\n",
            "epoch 5 step 137 loss 0.09070096909999847\n",
            "epoch 5 step 138 loss 0.05794430524110794\n",
            "epoch 5 step 139 loss 0.09489119052886963\n",
            "epoch 5 step 140 loss 0.05908407270908356\n",
            "epoch 5 step 141 loss 0.047100432217121124\n",
            "epoch 5 step 142 loss 0.14275029301643372\n",
            "epoch 5 step 143 loss 0.06569387018680573\n",
            "epoch 5 step 144 loss 0.11194013059139252\n",
            "epoch 5 step 145 loss 0.053826704621315\n",
            "epoch 5 step 146 loss 0.0627712830901146\n",
            "epoch 5 step 147 loss 0.09805525094270706\n",
            "epoch 5 step 148 loss 0.09299876540899277\n",
            "epoch 5 step 149 loss 0.08659226447343826\n",
            "epoch 5 step 150 loss 0.08232022821903229\n",
            "epoch 5 step 151 loss 0.057291917502880096\n",
            "epoch 5 step 152 loss 0.11344428360462189\n",
            "epoch 5 step 153 loss 0.09710177034139633\n",
            "epoch 5 step 154 loss 0.07337870448827744\n",
            "epoch 5 step 155 loss 0.05225921794772148\n",
            "epoch 5 step 156 loss 0.10431471467018127\n",
            "epoch 5 step 157 loss 0.102093905210495\n",
            "epoch 5 step 158 loss 0.0567607581615448\n",
            "epoch 5 step 159 loss 0.06010592728853226\n",
            "epoch 5 step 160 loss 0.04792207479476929\n",
            "epoch 5 step 161 loss 0.08668830245733261\n",
            "epoch 5 step 162 loss 0.05072515830397606\n",
            "epoch 5 step 163 loss 0.08778253942728043\n",
            "epoch 5 step 164 loss 0.06548922508955002\n",
            "epoch 5 step 165 loss 0.08121724426746368\n",
            "epoch 5 step 166 loss 0.051660116761922836\n",
            "epoch 5 step 167 loss 0.07154287397861481\n",
            "epoch 5 step 168 loss 0.047063350677490234\n",
            "epoch 5 step 169 loss 0.05653778836131096\n",
            "epoch 5 step 170 loss 0.08534608036279678\n",
            "epoch 5 step 171 loss 0.04617749899625778\n",
            "epoch 5 step 172 loss 0.1154610812664032\n",
            "epoch 5 step 173 loss 0.07746412605047226\n",
            "epoch 5 step 174 loss 0.07666734606027603\n",
            "epoch 5 step 175 loss 0.0592648908495903\n",
            "epoch 5 step 176 loss 0.08070878684520721\n",
            "epoch 5 step 177 loss 0.08105482161045074\n",
            "epoch 5 step 178 loss 0.03265177085995674\n",
            "epoch 5 step 179 loss 0.055441536009311676\n",
            "epoch 5 step 180 loss 0.04463578760623932\n",
            "epoch 5 step 181 loss 0.08393066376447678\n",
            "epoch 5 step 182 loss 0.03291569650173187\n",
            "epoch 5 step 183 loss 0.056590065360069275\n",
            "epoch 5 step 184 loss 0.037783652544021606\n",
            "epoch 5 step 185 loss 0.04610859602689743\n",
            "epoch 5 step 186 loss 0.03708109259605408\n",
            "epoch 5 step 187 loss 0.05320676043629646\n",
            "epoch 5 step 188 loss 0.0637807697057724\n",
            "epoch 5 step 189 loss 0.06313936412334442\n",
            "epoch 5 step 190 loss 0.06724592298269272\n",
            "epoch 5 step 191 loss 0.06647710502147675\n",
            "epoch 5 step 192 loss 0.07436076551675797\n",
            "epoch 5 step 193 loss 0.07709280401468277\n",
            "epoch 5 step 194 loss 0.06843707710504532\n",
            "epoch 5 step 195 loss 0.047332748770713806\n",
            "epoch 5 step 196 loss 0.03725974261760712\n",
            "epoch 5 step 197 loss 0.06608492136001587\n",
            "epoch 5 step 198 loss 0.05351807177066803\n",
            "epoch 5 step 199 loss 0.044915806502103806\n",
            "epoch 5 step 200 loss 0.06470883637666702\n",
            "epoch 5 step 201 loss 0.07267199456691742\n",
            "epoch 5 step 202 loss 0.09353630989789963\n",
            "epoch 5 step 203 loss 0.05174344405531883\n",
            "epoch 5 step 204 loss 0.07095205038785934\n",
            "epoch 5 step 205 loss 0.09428612887859344\n",
            "epoch 5 step 206 loss 0.04127047210931778\n",
            "epoch 5 step 207 loss 0.02933831699192524\n",
            "epoch 5 step 208 loss 0.05068168416619301\n",
            "epoch 5 step 209 loss 0.06760146468877792\n",
            "epoch 5 step 210 loss 0.08122340589761734\n",
            "epoch 5 step 211 loss 0.06471087038516998\n",
            "epoch 5 step 212 loss 0.04687095433473587\n",
            "epoch 5 step 213 loss 0.05150296539068222\n",
            "epoch 5 step 214 loss 0.12105122208595276\n",
            "epoch 5 step 215 loss 0.03958079218864441\n",
            "epoch 5 step 216 loss 0.07645927369594574\n",
            "epoch 5 step 217 loss 0.03144978731870651\n",
            "epoch 5 step 218 loss 0.10440148413181305\n",
            "epoch 5 step 219 loss 0.012806694954633713\n",
            "epoch 5 step 220 loss 0.07074641436338425\n",
            "epoch 5 step 221 loss 0.0554128959774971\n",
            "epoch 5 step 222 loss 0.09665828943252563\n",
            "epoch 5 step 223 loss 0.04602247104048729\n",
            "epoch 5 step 224 loss 0.09650472551584244\n",
            "epoch 5 step 225 loss 0.022220833227038383\n",
            "epoch 5 step 226 loss 0.027116423472762108\n",
            "epoch 5 step 227 loss 0.06520172953605652\n",
            "epoch 5 step 228 loss 0.07208405435085297\n",
            "epoch 5 step 229 loss 0.03577723354101181\n",
            "epoch 5 step 230 loss 0.05211573466658592\n",
            "epoch 5 step 231 loss 0.05584925413131714\n",
            "epoch 5 step 232 loss 0.06590887904167175\n",
            "epoch 5 step 233 loss 0.051666419953107834\n",
            "epoch 5 step 234 loss 0.05276249721646309\n",
            "epoch 5 step 235 loss 0.08377717435359955\n",
            "epoch 5 step 236 loss 0.05112174153327942\n",
            "epoch 5 step 237 loss 0.0589018277823925\n",
            "epoch 5 step 238 loss 0.07443250715732574\n",
            "epoch 5 step 239 loss 0.06284500658512115\n",
            "epoch 5 step 240 loss 0.03323462978005409\n",
            "epoch 5 step 241 loss 0.07644496858119965\n",
            "epoch 5 step 242 loss 0.07054746896028519\n",
            "epoch 5 step 243 loss 0.051672231405973434\n",
            "epoch 5 step 244 loss 0.0791155993938446\n",
            "epoch 5 step 245 loss 0.05287718027830124\n",
            "epoch 5 step 246 loss 0.018551673740148544\n",
            "epoch 5 step 247 loss 0.05757962539792061\n",
            "epoch 5 step 248 loss 0.06825023889541626\n",
            "epoch 5 step 249 loss 0.0795264020562172\n",
            "epoch 5 step 250 loss 0.05587024986743927\n",
            "epoch 5 step 251 loss 0.06354738771915436\n",
            "epoch 5 step 252 loss 0.03820261359214783\n",
            "epoch 5 step 253 loss 0.06392309069633484\n",
            "epoch 5 step 254 loss 0.07476290315389633\n",
            "epoch 5 step 255 loss 0.06123005598783493\n",
            "epoch 5 step 256 loss 0.054520003497600555\n",
            "epoch 5 step 257 loss 0.08080779761075974\n",
            "epoch 5 step 258 loss 0.04962312430143356\n",
            "epoch 5 step 259 loss 0.051445573568344116\n",
            "epoch 5 step 260 loss 0.08385200798511505\n",
            "epoch 5 step 261 loss 0.03823119401931763\n",
            "epoch 5 step 262 loss 0.057128578424453735\n",
            "epoch 5 step 263 loss 0.04445049166679382\n",
            "epoch 5 step 264 loss 0.053082700818777084\n",
            "epoch 5 step 265 loss 0.052098967134952545\n",
            "epoch 5 step 266 loss 0.05894490331411362\n",
            "epoch 5 step 267 loss 0.035982221364974976\n",
            "epoch 5 step 268 loss 0.04803835600614548\n",
            "epoch 5 step 269 loss 0.07268717885017395\n",
            "epoch 5 step 270 loss 0.03831333667039871\n",
            "epoch 5 step 271 loss 0.03345177322626114\n",
            "epoch 5 step 272 loss 0.055503763258457184\n",
            "epoch 5 step 273 loss 0.03869837522506714\n",
            "epoch 5 step 274 loss 0.06017575040459633\n",
            "epoch 5 step 275 loss 0.04399316757917404\n",
            "epoch 5 step 276 loss 0.03787342458963394\n",
            "epoch 5 step 277 loss 0.07089228928089142\n",
            "epoch 5 step 278 loss 0.07408615946769714\n",
            "epoch 5 step 279 loss 0.07262451946735382\n",
            "epoch 5 step 280 loss 0.06287261843681335\n",
            "epoch 5 step 281 loss 0.049466848373413086\n",
            "epoch 5 step 282 loss 0.034449830651283264\n",
            "epoch 5 step 283 loss 0.02907886542379856\n",
            "epoch 5 step 284 loss 0.0489947535097599\n",
            "epoch 5 step 285 loss 0.0777081549167633\n",
            "epoch 5 step 286 loss 0.06734229624271393\n",
            "epoch 5 step 287 loss 0.04544082656502724\n",
            "epoch 5 step 288 loss 0.036068279296159744\n",
            "epoch 5 step 289 loss 0.04776214063167572\n",
            "epoch 5 step 290 loss 0.06192906200885773\n",
            "epoch 5 step 291 loss 0.04062101989984512\n",
            "epoch 5 step 292 loss 0.07567451894283295\n",
            "epoch 5 step 293 loss 0.04335911571979523\n",
            "epoch 5 step 294 loss 0.0742405503988266\n",
            "epoch 5 step 295 loss 0.05521354824304581\n",
            "epoch 5 step 296 loss 0.04449476674199104\n",
            "epoch 5 step 297 loss 0.07117755711078644\n",
            "epoch 5 step 298 loss 0.049849577248096466\n",
            "epoch 5 step 299 loss 0.06287777423858643\n",
            "epoch 5 step 300 loss 0.06188664957880974\n",
            "epoch 5 step 301 loss 0.036424096673727036\n",
            "epoch 5 step 302 loss 0.09210759401321411\n",
            "epoch 5 step 303 loss 0.043781206011772156\n",
            "epoch 5 step 304 loss 0.03396084904670715\n",
            "epoch 5 step 305 loss 0.038603052496910095\n",
            "epoch 5 step 306 loss 0.06397303193807602\n",
            "epoch 5 step 307 loss 0.03916611149907112\n",
            "epoch 5 step 308 loss 0.037787098437547684\n",
            "epoch 5 step 309 loss 0.055293578654527664\n",
            "epoch 5 step 310 loss 0.03777117654681206\n",
            "epoch 5 step 311 loss 0.05272771045565605\n",
            "epoch 5 step 312 loss 0.050328683108091354\n",
            "epoch 5 step 313 loss 0.09630449116230011\n",
            "epoch 5 step 314 loss 0.03363807499408722\n",
            "epoch 5 step 315 loss 0.06222718954086304\n",
            "epoch 5 step 316 loss 0.05862491577863693\n",
            "epoch 5 step 317 loss 0.04940059781074524\n",
            "epoch 5 step 318 loss 0.04266724735498428\n",
            "epoch 5 step 319 loss 0.05957547575235367\n",
            "epoch 5 step 320 loss 0.06181313470005989\n",
            "epoch 5 step 321 loss 0.05831718072295189\n",
            "epoch 5 step 322 loss 0.058339618146419525\n",
            "epoch 5 step 323 loss 0.018132269382476807\n",
            "epoch 5 step 324 loss 0.03328339383006096\n",
            "epoch 5 step 325 loss 0.06255999207496643\n",
            "epoch 5 step 326 loss 0.02726132422685623\n",
            "epoch 5 step 327 loss 0.06897128373384476\n",
            "epoch 5 step 328 loss 0.08357808738946915\n",
            "epoch 5 step 329 loss 0.0760292112827301\n",
            "epoch 5 step 330 loss 0.055600620806217194\n",
            "epoch 5 step 331 loss 0.02430362068116665\n",
            "epoch 5 step 332 loss 0.0645979568362236\n",
            "epoch 5 step 333 loss 0.09166252613067627\n",
            "epoch 5 step 334 loss 0.03749547898769379\n",
            "epoch 5 step 335 loss 0.030826181173324585\n",
            "epoch 5 step 336 loss 0.060440465807914734\n",
            "epoch 5 step 337 loss 0.09853428602218628\n",
            "epoch 5 step 338 loss 0.05080210790038109\n",
            "epoch 5 step 339 loss 0.03896915167570114\n",
            "epoch 5 step 340 loss 0.07797106355428696\n",
            "epoch 5 step 341 loss 0.08323384821414948\n",
            "epoch 5 step 342 loss 0.031133517622947693\n",
            "epoch 5 step 343 loss 0.04476397857069969\n",
            "epoch 5 step 344 loss 0.023518063127994537\n",
            "epoch 5 step 345 loss 0.07914762198925018\n",
            "epoch 5 step 346 loss 0.07157018035650253\n",
            "epoch 5 step 347 loss 0.06751366704702377\n",
            "epoch 5 step 348 loss 0.04988023638725281\n",
            "epoch 5 step 349 loss 0.07556355744600296\n",
            "epoch 5 step 350 loss 0.044858843088150024\n",
            "epoch 5 step 351 loss 0.04192720353603363\n",
            "epoch 5 step 352 loss 0.040968067944049835\n",
            "epoch 5 step 353 loss 0.01178564503788948\n",
            "epoch 5 step 354 loss 0.015105893835425377\n",
            "epoch 5 step 355 loss 0.047204285860061646\n",
            "epoch 5 step 356 loss 0.06341470777988434\n",
            "epoch 5 step 357 loss 0.0835341364145279\n",
            "epoch 5 step 358 loss 0.06481023132801056\n",
            "epoch 5 step 359 loss 0.03486965596675873\n",
            "epoch 5 step 360 loss 0.06296347826719284\n",
            "epoch 5 step 361 loss 0.08337840437889099\n",
            "epoch 5 step 362 loss 0.08048836886882782\n",
            "epoch 5 step 363 loss 0.05759035795927048\n",
            "epoch 5 step 364 loss 0.07556021213531494\n",
            "epoch 5 step 365 loss 0.05114535242319107\n",
            "epoch 5 step 366 loss 0.04385848715901375\n",
            "epoch 5 step 367 loss 0.04530789703130722\n",
            "epoch 5 step 368 loss 0.05378634110093117\n",
            "epoch 5 step 369 loss 0.02248396910727024\n",
            "epoch 5 step 370 loss 0.04766220599412918\n",
            "epoch 5 step 371 loss 0.055495794862508774\n",
            "epoch 5 step 372 loss 0.0649629607796669\n",
            "epoch 5 step 373 loss 0.06546466052532196\n",
            "epoch 5 step 374 loss 0.026671338826417923\n",
            "epoch 5 step 375 loss 0.07189550995826721\n",
            "epoch 5 step 376 loss 0.059638235718011856\n",
            "epoch 5 step 377 loss 0.04475284740328789\n",
            "epoch 5 step 378 loss 0.0712033212184906\n",
            "epoch 5 step 379 loss 0.041794151067733765\n",
            "epoch 5 step 380 loss 0.04793287813663483\n",
            "epoch 5 step 381 loss 0.041319362819194794\n",
            "epoch 5 step 382 loss 0.06828910857439041\n",
            "epoch 5 step 383 loss 0.0682818740606308\n",
            "epoch 5 step 384 loss 0.032986655831336975\n",
            "epoch 5 step 385 loss 0.0593235045671463\n",
            "epoch 5 step 386 loss 0.07225202023983002\n",
            "epoch 5 step 387 loss 0.03513091802597046\n",
            "epoch 5 step 388 loss 0.06164099648594856\n",
            "epoch 5 step 389 loss 0.04488968476653099\n",
            "epoch 5 step 390 loss 0.07696212828159332\n",
            "epoch 5 step 391 loss 0.07419730722904205\n",
            "epoch 5 step 392 loss 0.03217463940382004\n",
            "epoch 5 step 393 loss 0.06290847808122635\n",
            "epoch 5 step 394 loss 0.05112603306770325\n",
            "epoch 5 step 395 loss 0.05917627364397049\n",
            "epoch 5 step 396 loss 0.032882317900657654\n",
            "epoch 5 step 397 loss 0.05123242735862732\n",
            "epoch 5 step 398 loss 0.06838738918304443\n",
            "epoch 5 step 399 loss 0.08952337503433228\n",
            "epoch 5 step 400 loss 0.09203049540519714\n",
            "epoch 5 step 401 loss 0.028880402445793152\n",
            "epoch 5 step 402 loss 0.03499212861061096\n",
            "epoch 5 step 403 loss 0.050353117287158966\n",
            "epoch 5 step 404 loss 0.06888604909181595\n",
            "epoch 5 step 405 loss 0.05672826990485191\n",
            "epoch 5 step 406 loss 0.05230485275387764\n",
            "epoch 5 step 407 loss 0.039289940148591995\n",
            "epoch 5 step 408 loss 0.06662388145923615\n",
            "epoch 5 step 409 loss 0.0467810183763504\n",
            "epoch 5 step 410 loss 0.039050228893756866\n",
            "epoch 5 step 411 loss 0.031191980466246605\n",
            "epoch 5 step 412 loss 0.06634949147701263\n",
            "epoch 5 step 413 loss 0.03897017240524292\n",
            "epoch 5 step 414 loss 0.0420398972928524\n",
            "epoch 5 step 415 loss 0.04066362604498863\n",
            "epoch 5 step 416 loss 0.08653101325035095\n",
            "epoch 5 step 417 loss 0.05989128723740578\n",
            "epoch 5 step 418 loss 0.057626232504844666\n",
            "epoch 5 step 419 loss 0.04288031533360481\n",
            "epoch 5 step 420 loss 0.07592230290174484\n",
            "epoch 5 step 421 loss 0.0626601055264473\n",
            "epoch 5 step 422 loss 0.07553140819072723\n",
            "epoch 5 step 423 loss 0.04603872820734978\n",
            "epoch 5 step 424 loss 0.09756195545196533\n",
            "epoch 5 step 425 loss 0.061203233897686005\n",
            "epoch 5 step 426 loss 0.05972420796751976\n",
            "epoch 5 step 427 loss 0.07444581389427185\n",
            "epoch 5 step 428 loss 0.13502512872219086\n",
            "epoch 5 step 429 loss 0.06779544800519943\n",
            "epoch 5 step 430 loss 0.0859755426645279\n",
            "epoch 5 step 431 loss 0.08914071321487427\n",
            "epoch 5 step 432 loss 0.045952942222356796\n",
            "epoch 5 step 433 loss 0.054543331265449524\n",
            "epoch 5 step 434 loss 0.058452896773815155\n",
            "epoch 5 step 435 loss 0.047361183911561966\n",
            "epoch 5 step 436 loss 0.11164926737546921\n",
            "epoch 5 step 437 loss 0.0777159333229065\n",
            "epoch 5 step 438 loss 0.04026690497994423\n",
            "epoch 5 step 439 loss 0.06568235903978348\n",
            "epoch 5 step 440 loss 0.05083739012479782\n",
            "epoch 5 step 441 loss 0.08059313893318176\n",
            "epoch 5 step 442 loss 0.10819375514984131\n",
            "epoch 5 step 443 loss 0.06095818057656288\n",
            "epoch 5 step 444 loss 0.03518601506948471\n",
            "epoch 5 step 445 loss 0.0418282113969326\n",
            "epoch 5 step 446 loss 0.05274193361401558\n",
            "epoch 5 step 447 loss 0.06750515103340149\n",
            "epoch 5 step 448 loss 0.05839049816131592\n",
            "epoch 5 step 449 loss 0.08658228814601898\n",
            "epoch 5 step 450 loss 0.03518937528133392\n",
            "epoch 5 step 451 loss 0.03755785524845123\n",
            "epoch 5 step 452 loss 0.07012782990932465\n",
            "epoch 5 step 453 loss 0.09064441919326782\n",
            "epoch 5 step 454 loss 0.02046845480799675\n",
            "epoch 5 step 455 loss 0.048699937760829926\n",
            "epoch 5 step 456 loss 0.0794605165719986\n",
            "epoch 5 step 457 loss 0.07630014419555664\n",
            "epoch 5 step 458 loss 0.04219495877623558\n",
            "epoch 5 step 459 loss 0.06353513896465302\n",
            "epoch 5 step 460 loss 0.07222960889339447\n",
            "epoch 5 step 461 loss 0.03800579905509949\n",
            "epoch 5 step 462 loss 0.046255480498075485\n",
            "epoch 5 step 463 loss 0.05076897516846657\n",
            "epoch 5 step 464 loss 0.07393645495176315\n",
            "epoch 5 step 465 loss 0.06052716448903084\n",
            "epoch 5 step 466 loss 0.12988057732582092\n",
            "epoch 5 step 467 loss 0.05200523883104324\n",
            "epoch 5 step 468 loss 0.04859668016433716\n",
            "epoch 5 step 469 loss 0.04026060923933983\n",
            "epoch 5 step 470 loss 0.05158056318759918\n",
            "epoch 5 step 471 loss 0.07533445954322815\n",
            "epoch 5 step 472 loss 0.07275517284870148\n",
            "epoch 5 step 473 loss 0.05160379037261009\n",
            "epoch 5 step 474 loss 0.0618625208735466\n",
            "epoch 5 step 475 loss 0.09019310772418976\n",
            "epoch 5 step 476 loss 0.024915942922234535\n",
            "epoch 5 step 477 loss 0.07944703102111816\n",
            "epoch 5 step 478 loss 0.07907311618328094\n",
            "epoch 5 step 479 loss 0.05582413077354431\n",
            "epoch 5 step 480 loss 0.05272858589887619\n",
            "epoch 5 step 481 loss 0.04181218892335892\n",
            "epoch 5 step 482 loss 0.07620067894458771\n",
            "epoch 5 step 483 loss 0.06558764725923538\n",
            "epoch 5 step 484 loss 0.06574860215187073\n",
            "epoch 5 step 485 loss 0.0740266665816307\n",
            "epoch 5 step 486 loss 0.07671009749174118\n",
            "epoch 5 step 487 loss 0.05617257207632065\n",
            "epoch 5 step 488 loss 0.05682546645402908\n",
            "epoch 5 step 489 loss 0.04249662905931473\n",
            "epoch 5 step 490 loss 0.053562961518764496\n",
            "epoch 5 step 491 loss 0.09267573803663254\n",
            "epoch 5 step 492 loss 0.05947978049516678\n",
            "epoch 5 step 493 loss 0.04794847220182419\n",
            "epoch 5 step 494 loss 0.06498761475086212\n",
            "epoch 5 step 495 loss 0.029524968937039375\n",
            "epoch 5 step 496 loss 0.058944862335920334\n",
            "epoch 5 step 497 loss 0.04443865269422531\n",
            "epoch 5 step 498 loss 0.06636251509189606\n",
            "epoch 5 step 499 loss 0.04145617038011551\n",
            "epoch 5 step 500 loss 0.06138863041996956\n",
            "epoch 5 step 501 loss 0.06543941795825958\n",
            "epoch 5 step 502 loss 0.06285691261291504\n",
            "epoch 5 step 503 loss 0.051650747656822205\n",
            "epoch 5 step 504 loss 0.0623994879424572\n",
            "epoch 5 step 505 loss 0.09548643231391907\n",
            "epoch 5 step 506 loss 0.04689357429742813\n",
            "epoch 5 step 507 loss 0.05071592330932617\n",
            "epoch 5 step 508 loss 0.046519048511981964\n",
            "epoch 5 step 509 loss 0.05297708511352539\n",
            "epoch 5 step 510 loss 0.047422852367162704\n",
            "epoch 5 step 511 loss 0.04511246830224991\n",
            "epoch 5 step 512 loss 0.040544167160987854\n",
            "epoch 5 step 513 loss 0.03832825645804405\n",
            "epoch 5 step 514 loss 0.08225591480731964\n",
            "epoch 5 step 515 loss 0.07125067710876465\n",
            "epoch 5 step 516 loss 0.0914028212428093\n",
            "epoch 5 step 517 loss 0.036057572811841965\n",
            "epoch 5 step 518 loss 0.0486108660697937\n",
            "epoch 5 step 519 loss 0.0950106829404831\n",
            "epoch 5 step 520 loss 0.059270620346069336\n",
            "epoch 5 step 521 loss 0.037412822246551514\n",
            "epoch 5 step 522 loss 0.048342738300561905\n",
            "epoch 5 step 523 loss 0.09124068170785904\n",
            "epoch 5 step 524 loss 0.05063782259821892\n",
            "epoch 5 step 525 loss 0.05930786579847336\n",
            "epoch 5 step 526 loss 0.04415896534919739\n",
            "epoch 5 step 527 loss 0.06587383151054382\n",
            "epoch 5 step 528 loss 0.061159174889326096\n",
            "epoch 5 step 529 loss 0.05230925977230072\n",
            "epoch 5 step 530 loss 0.05103842914104462\n",
            "epoch 5 step 531 loss 0.047603294253349304\n",
            "epoch 5 step 532 loss 0.07221385836601257\n",
            "epoch 5 step 533 loss 0.03550422936677933\n",
            "epoch 5 step 534 loss 0.03313077613711357\n",
            "epoch 5 step 535 loss 0.06422413140535355\n",
            "epoch 5 step 536 loss 0.09366125613451004\n",
            "epoch 5 step 537 loss 0.0992121696472168\n",
            "epoch 5 step 538 loss 0.0703497976064682\n",
            "epoch 5 step 539 loss 0.06632734835147858\n",
            "epoch 5 step 540 loss 0.08750971406698227\n",
            "epoch 5 step 541 loss 0.05404321104288101\n",
            "epoch 5 step 542 loss 0.040276236832141876\n",
            "epoch 5 step 543 loss 0.0562036968767643\n",
            "epoch 5 step 544 loss 0.09756401181221008\n",
            "epoch 5 step 545 loss 0.08941291272640228\n",
            "epoch 5 step 546 loss 0.08807195723056793\n",
            "epoch 5 step 547 loss 0.06622414290904999\n",
            "epoch 5 step 548 loss 0.09876425564289093\n",
            "epoch 5 step 549 loss 0.06862867623567581\n",
            "epoch 5 step 550 loss 0.12012109905481339\n",
            "epoch 5 step 551 loss 0.07824964821338654\n",
            "epoch 5 step 552 loss 0.08767155557870865\n",
            "epoch 5 step 553 loss 0.09626258909702301\n",
            "epoch 5 step 554 loss 0.10093909502029419\n",
            "epoch 5 step 555 loss 0.06323377788066864\n",
            "epoch 5 step 556 loss 0.07066735625267029\n",
            "epoch 5 step 557 loss 0.080177441239357\n",
            "epoch 5 step 558 loss 0.06204602122306824\n",
            "epoch 5 step 559 loss 0.07305774092674255\n",
            "epoch 5 step 560 loss 0.0864524096250534\n",
            "epoch 5 step 561 loss 0.07384452223777771\n",
            "epoch 5 step 562 loss 0.058628737926483154\n",
            "epoch 5 step 563 loss 0.08203402161598206\n",
            "epoch 5 step 564 loss 0.08054263889789581\n",
            "epoch 5 step 565 loss 0.0746602788567543\n",
            "epoch 5 step 566 loss 0.06728626787662506\n",
            "epoch 5 step 567 loss 0.06745545566082001\n",
            "epoch 5 step 568 loss 0.07003134489059448\n",
            "epoch 5 step 569 loss 0.0658835917711258\n",
            "epoch 5 step 570 loss 0.06479687988758087\n",
            "epoch 5 step 571 loss 0.06085260957479477\n",
            "epoch 5 step 572 loss 0.071428582072258\n",
            "epoch 5 step 573 loss 0.055640000849962234\n",
            "epoch 5 step 574 loss 0.08076412230730057\n",
            "epoch 5 step 575 loss 0.07705046981573105\n",
            "epoch 5 step 576 loss 0.05492880940437317\n",
            "epoch 5 step 577 loss 0.0632944256067276\n",
            "epoch 5 step 578 loss 0.055059220641851425\n",
            "epoch 5 step 579 loss 0.08081185817718506\n",
            "epoch 5 step 580 loss 0.09831075370311737\n",
            "epoch 5 step 581 loss 0.07009870558977127\n",
            "epoch 5 step 582 loss 0.08000043779611588\n",
            "epoch 5 step 583 loss 0.055638328194618225\n",
            "epoch 5 step 584 loss 0.05215213820338249\n",
            "epoch 5 step 585 loss 0.057971637696027756\n",
            "epoch 5 step 586 loss 0.04886367917060852\n",
            "epoch 5 step 587 loss 0.08346893638372421\n",
            "epoch 5 step 588 loss 0.04067737236618996\n",
            "epoch 5 step 589 loss 0.03567197173833847\n",
            "epoch 5 step 590 loss 0.059354133903980255\n",
            "epoch 5 step 591 loss 0.03470979630947113\n",
            "epoch 5 step 592 loss 0.053035005927085876\n",
            "epoch 5 step 593 loss 0.06476408243179321\n",
            "epoch 5 step 594 loss 0.07361862063407898\n",
            "epoch 5 step 595 loss 0.06967511773109436\n",
            "epoch 5 step 596 loss 0.05587075278162956\n",
            "epoch 5 step 597 loss 0.05931418016552925\n",
            "epoch 5 step 598 loss 0.051899224519729614\n",
            "epoch 5 step 599 loss 0.08541543036699295\n",
            "epoch 5 step 600 loss 0.056925930082798004\n",
            "epoch 5 step 601 loss 0.06240995600819588\n",
            "epoch 5 step 602 loss 0.04406450688838959\n",
            "epoch 5 step 603 loss 0.04737687483429909\n",
            "epoch 5 step 604 loss 0.05011400580406189\n",
            "epoch 5 step 605 loss 0.057092130184173584\n",
            "epoch 5 step 606 loss 0.08119307458400726\n",
            "epoch 5 step 607 loss 0.06513965129852295\n",
            "epoch 5 step 608 loss 0.03767292946577072\n",
            "epoch 5 step 609 loss 0.055680133402347565\n",
            "epoch 5 step 610 loss 0.07990915328264236\n",
            "epoch 5 step 611 loss 0.046249717473983765\n",
            "epoch 5 step 612 loss 0.07210119068622589\n",
            "epoch 5 step 613 loss 0.026752399280667305\n",
            "epoch 5 step 614 loss 0.048409223556518555\n",
            "epoch 5 step 615 loss 0.05719625577330589\n",
            "epoch 5 step 616 loss 0.04095727205276489\n",
            "epoch 5 step 617 loss 0.04213595390319824\n",
            "epoch 5 step 618 loss 0.04453577101230621\n",
            "epoch 5 step 619 loss 0.03554696962237358\n",
            "epoch 5 step 620 loss 0.05652831494808197\n",
            "epoch 5 step 621 loss 0.04937472194433212\n",
            "epoch 5 step 622 loss 0.08492595702409744\n",
            "epoch 5 step 623 loss 0.02811630256474018\n",
            "epoch 5 step 624 loss 0.09927897155284882\n",
            "epoch 5 step 625 loss 0.05635269358754158\n",
            "epoch 5 step 626 loss 0.04842769354581833\n",
            "epoch 5 step 627 loss 0.05724010244011879\n",
            "epoch 5 step 628 loss 0.030491352081298828\n",
            "epoch 5 step 629 loss 0.05054498463869095\n",
            "epoch 5 step 630 loss 0.019136812537908554\n",
            "epoch 5 step 631 loss 0.09523545205593109\n",
            "epoch 5 step 632 loss 0.05758466571569443\n",
            "epoch 5 step 633 loss 0.07840925455093384\n",
            "epoch 5 step 634 loss 0.05911361426115036\n",
            "epoch 5 step 635 loss 0.07180997729301453\n",
            "epoch 5 step 636 loss 0.05568996071815491\n",
            "epoch 5 step 637 loss 0.08263687789440155\n",
            "epoch 5 step 638 loss 0.03834189102053642\n",
            "epoch 5 step 639 loss 0.09611502289772034\n",
            "epoch 5 step 640 loss 0.0781799778342247\n",
            "epoch 5 step 641 loss 0.09365008026361465\n",
            "epoch 5 step 642 loss 0.04525691270828247\n",
            "epoch 5 step 643 loss 0.03999455273151398\n",
            "epoch 5 step 644 loss 0.04924841970205307\n",
            "epoch 5 step 645 loss 0.06971681863069534\n",
            "epoch 5 step 646 loss 0.053004804998636246\n",
            "epoch 5 step 647 loss 0.0366954505443573\n",
            "epoch 5 step 648 loss 0.06757811456918716\n",
            "epoch 5 step 649 loss 0.07917199283838272\n",
            "epoch 5 step 650 loss 0.045295726507902145\n",
            "epoch 5 step 651 loss 0.05699294060468674\n",
            "epoch 5 step 652 loss 0.06388703733682632\n",
            "epoch 5 step 653 loss 0.07028408348560333\n",
            "epoch 5 step 654 loss 0.05019412934780121\n",
            "epoch 5 step 655 loss 0.04329703375697136\n",
            "epoch 5 step 656 loss 0.04532869905233383\n",
            "epoch 5 step 657 loss 0.09701783955097198\n",
            "epoch 5 step 658 loss 0.02693815901875496\n",
            "epoch 5 step 659 loss 0.034993916749954224\n",
            "epoch 5 step 660 loss 0.055296219885349274\n",
            "epoch 5 step 661 loss 0.03425025939941406\n",
            "epoch 5 step 662 loss 0.03968360275030136\n",
            "epoch 5 step 663 loss 0.03682522848248482\n",
            "epoch 5 step 664 loss 0.04625478759407997\n",
            "epoch 5 step 665 loss 0.04423869401216507\n",
            "epoch 5 step 666 loss 0.039407506585121155\n",
            "epoch 5 step 667 loss 0.05885244905948639\n",
            "epoch 5 step 668 loss 0.07432742416858673\n",
            "epoch 5 step 669 loss 0.05517348274588585\n",
            "epoch 5 step 670 loss 0.05328533798456192\n",
            "epoch 5 step 671 loss 0.06475049257278442\n",
            "epoch 5 step 672 loss 0.038500554859638214\n",
            "epoch 5 step 673 loss 0.06321735680103302\n",
            "epoch 5 step 674 loss 0.04685087129473686\n",
            "epoch 5 step 675 loss 0.0771002545952797\n",
            "epoch 5 step 676 loss 0.0533350370824337\n",
            "epoch 5 step 677 loss 0.08398479223251343\n",
            "epoch 5 step 678 loss 0.06080632656812668\n",
            "epoch 5 step 679 loss 0.040969446301460266\n",
            "epoch 5 step 680 loss 0.024345744401216507\n",
            "epoch 5 step 681 loss 0.0648198351264\n",
            "epoch 5 step 682 loss 0.044442929327487946\n",
            "epoch 5 step 683 loss 0.07586202025413513\n",
            "epoch 5 step 684 loss 0.024161607027053833\n",
            "epoch 5 step 685 loss 0.05959575995802879\n",
            "epoch 5 step 686 loss 0.058127980679273605\n",
            "epoch 5 step 687 loss 0.07555388659238815\n",
            "epoch 5 step 688 loss 0.04642210155725479\n",
            "epoch 5 step 689 loss 0.05395575612783432\n",
            "epoch 5 step 690 loss 0.063961923122406\n",
            "epoch 5 step 691 loss 0.07268787920475006\n",
            "epoch 5 step 692 loss 0.07814931869506836\n",
            "epoch 5 step 693 loss 0.05418413132429123\n",
            "epoch 5 step 694 loss 0.0815192312002182\n",
            "epoch 5 step 695 loss 0.05543547868728638\n",
            "epoch 5 step 696 loss 0.0315619632601738\n",
            "epoch 5 step 697 loss 0.03900923952460289\n",
            "epoch 5 step 698 loss 0.04292383790016174\n",
            "epoch 5 step 699 loss 0.06079481542110443\n",
            "epoch 5 step 700 loss 0.03615026921033859\n",
            "epoch 5 step 701 loss 0.04479815438389778\n",
            "epoch 5 step 702 loss 0.05317287519574165\n",
            "epoch 5 step 703 loss 0.06512772291898727\n",
            "epoch 5 step 704 loss 0.05485118180513382\n",
            "epoch 5 step 705 loss 0.04218374565243721\n",
            "epoch 5 step 706 loss 0.0368119552731514\n",
            "epoch 5 step 707 loss 0.11470986902713776\n",
            "epoch 5 step 708 loss 0.06861002743244171\n",
            "epoch 5 step 709 loss 0.10090450942516327\n",
            "epoch 5 step 710 loss 0.047845128923654556\n",
            "epoch 5 step 711 loss 0.04543861374258995\n",
            "epoch 5 step 712 loss 0.04126459360122681\n",
            "epoch 5 step 713 loss 0.06305840611457825\n",
            "epoch 5 step 714 loss 0.054180294275283813\n",
            "epoch 5 step 715 loss 0.06006176769733429\n",
            "epoch 5 step 716 loss 0.06301804631948471\n",
            "epoch 5 step 717 loss 0.07443741708993912\n",
            "epoch 5 step 718 loss 0.04284686595201492\n",
            "epoch 5 step 719 loss 0.0501280203461647\n",
            "epoch 5 step 720 loss 0.02564782276749611\n",
            "epoch 5 step 721 loss 0.05790286138653755\n",
            "epoch 5 step 722 loss 0.05780300498008728\n",
            "epoch 5 step 723 loss 0.025116819888353348\n",
            "epoch 5 step 724 loss 0.03884448856115341\n",
            "epoch 5 step 725 loss 0.03403926640748978\n",
            "epoch 5 step 726 loss 0.07828273624181747\n",
            "epoch 5 step 727 loss 0.041574686765670776\n",
            "epoch 5 step 728 loss 0.044420838356018066\n",
            "epoch 5 step 729 loss 0.03264710307121277\n",
            "epoch 5 step 730 loss 0.04628809168934822\n",
            "epoch 5 step 731 loss 0.03889310359954834\n",
            "epoch 5 step 732 loss 0.10443967580795288\n",
            "epoch 5 step 733 loss 0.07159978151321411\n",
            "epoch 5 step 734 loss 0.05852680653333664\n",
            "epoch 5 step 735 loss 0.06826283037662506\n",
            "epoch 5 step 736 loss 0.03859781101346016\n",
            "epoch 5 step 737 loss 0.053092267364263535\n",
            "epoch 5 step 738 loss 0.04066583886742592\n",
            "epoch 5 step 739 loss 0.04904311150312424\n",
            "epoch 5 step 740 loss 0.0630674660205841\n",
            "epoch 5 step 741 loss 0.06710531562566757\n",
            "epoch 5 step 742 loss 0.04809252545237541\n",
            "epoch 5 step 743 loss 0.05004781857132912\n",
            "epoch 5 step 744 loss 0.047430865466594696\n",
            "epoch 5 step 745 loss 0.03941911458969116\n",
            "epoch 5 step 746 loss 0.055900949984788895\n",
            "epoch 5 step 747 loss 0.10590222477912903\n",
            "epoch 5 step 748 loss 0.060413457453250885\n",
            "epoch 5 step 749 loss 0.05336959660053253\n",
            "epoch 5 step 750 loss 0.07147854566574097\n",
            "epoch 5 step 751 loss 0.06029371917247772\n",
            "epoch 5 step 752 loss 0.16158196330070496\n",
            "epoch 5 step 753 loss 0.057542622089385986\n",
            "epoch 5 step 754 loss 0.06750348210334778\n",
            "epoch 5 step 755 loss 0.05858200043439865\n",
            "epoch 5 step 756 loss 0.05627044290304184\n",
            "epoch 5 step 757 loss 0.06164797022938728\n",
            "epoch 5 step 758 loss 0.09451654553413391\n",
            "epoch 5 step 759 loss 0.03894977644085884\n",
            "epoch 5 step 760 loss 0.06078508123755455\n",
            "epoch 5 step 761 loss 0.046001728624105453\n",
            "epoch 5 step 762 loss 0.051433809101581573\n",
            "epoch 5 step 763 loss 0.04332749545574188\n",
            "epoch 5 step 764 loss 0.07233691960573196\n",
            "epoch 5 step 765 loss 0.03806833177804947\n",
            "epoch 5 step 766 loss 0.06442447751760483\n",
            "epoch 5 step 767 loss 0.028782742097973824\n",
            "epoch 5 step 768 loss 0.02899428829550743\n",
            "epoch 5 step 769 loss 0.05064046382904053\n",
            "epoch 5 step 770 loss 0.04583609849214554\n",
            "epoch 5 step 771 loss 0.03986433893442154\n",
            "epoch 5 step 772 loss 0.07708638906478882\n",
            "epoch 5 step 773 loss 0.060525912791490555\n",
            "epoch 5 step 774 loss 0.04245082288980484\n",
            "epoch 5 step 775 loss 0.04914305359125137\n",
            "epoch 5 step 776 loss 0.05646660923957825\n",
            "epoch 5 step 777 loss 0.044042058289051056\n",
            "epoch 5 step 778 loss 0.037240516394376755\n",
            "epoch 5 step 779 loss 0.0699136033654213\n",
            "epoch 5 step 780 loss 0.06368792802095413\n",
            "epoch 5 step 781 loss 0.07368152588605881\n",
            "epoch 6 step 0 loss 0.058495741337537766\n",
            "epoch 6 step 1 loss 0.04189324378967285\n",
            "epoch 6 step 2 loss 0.03335653990507126\n",
            "epoch 6 step 3 loss 0.10571693629026413\n",
            "epoch 6 step 4 loss 0.07050735503435135\n",
            "epoch 6 step 5 loss 0.059684306383132935\n",
            "epoch 6 step 6 loss 0.07222810387611389\n",
            "epoch 6 step 7 loss 0.0510183684527874\n",
            "epoch 6 step 8 loss 0.0698334276676178\n",
            "epoch 6 step 9 loss 0.03206068277359009\n",
            "epoch 6 step 10 loss 0.029621250927448273\n",
            "epoch 6 step 11 loss 0.045489899814128876\n",
            "epoch 6 step 12 loss 0.02324412763118744\n",
            "epoch 6 step 13 loss 0.07168779522180557\n",
            "epoch 6 step 14 loss 0.07227107882499695\n",
            "epoch 6 step 15 loss 0.0612587034702301\n",
            "epoch 6 step 16 loss 0.04502081125974655\n",
            "epoch 6 step 17 loss 0.09072187542915344\n",
            "epoch 6 step 18 loss 0.04840186983346939\n",
            "epoch 6 step 19 loss 0.04456062614917755\n",
            "epoch 6 step 20 loss 0.061636004596948624\n",
            "epoch 6 step 21 loss 0.04685574769973755\n",
            "epoch 6 step 22 loss 0.08549563586711884\n",
            "epoch 6 step 23 loss 0.05257662758231163\n",
            "epoch 6 step 24 loss 0.06075659766793251\n",
            "epoch 6 step 25 loss 0.04836095869541168\n",
            "epoch 6 step 26 loss 0.026670411229133606\n",
            "epoch 6 step 27 loss 0.02729334868490696\n",
            "epoch 6 step 28 loss 0.08772209286689758\n",
            "epoch 6 step 29 loss 0.10787244141101837\n",
            "epoch 6 step 30 loss 0.03766918182373047\n",
            "epoch 6 step 31 loss 0.049587421119213104\n",
            "epoch 6 step 32 loss 0.016235943883657455\n",
            "epoch 6 step 33 loss 0.04406566172838211\n",
            "epoch 6 step 34 loss 0.05464078485965729\n",
            "epoch 6 step 35 loss 0.039725709706544876\n",
            "epoch 6 step 36 loss 0.02927742898464203\n",
            "epoch 6 step 37 loss 0.06182781234383583\n",
            "epoch 6 step 38 loss 0.063105508685112\n",
            "epoch 6 step 39 loss 0.05049006640911102\n",
            "epoch 6 step 40 loss 0.03965029865503311\n",
            "epoch 6 step 41 loss 0.03989655151963234\n",
            "epoch 6 step 42 loss 0.04178223758935928\n",
            "epoch 6 step 43 loss 0.05805601179599762\n",
            "epoch 6 step 44 loss 0.04707717150449753\n",
            "epoch 6 step 45 loss 0.036143846809864044\n",
            "epoch 6 step 46 loss 0.09323714673519135\n",
            "epoch 6 step 47 loss 0.06062460318207741\n",
            "epoch 6 step 48 loss 0.029595868661999702\n",
            "epoch 6 step 49 loss 0.05697500705718994\n",
            "epoch 6 step 50 loss 0.048110783100128174\n",
            "epoch 6 step 51 loss 0.13366934657096863\n",
            "epoch 6 step 52 loss 0.05440764129161835\n",
            "epoch 6 step 53 loss 0.06342454254627228\n",
            "epoch 6 step 54 loss 0.08272376656532288\n",
            "epoch 6 step 55 loss 0.05023840442299843\n",
            "epoch 6 step 56 loss 0.04131253808736801\n",
            "epoch 6 step 57 loss 0.02958139032125473\n",
            "epoch 6 step 58 loss 0.057390302419662476\n",
            "epoch 6 step 59 loss 0.08588963747024536\n",
            "epoch 6 step 60 loss 0.03280164301395416\n",
            "epoch 6 step 61 loss 0.037334226071834564\n",
            "epoch 6 step 62 loss 0.050437383353710175\n",
            "epoch 6 step 63 loss 0.09067925065755844\n",
            "epoch 6 step 64 loss 0.04744156077504158\n",
            "epoch 6 step 65 loss 0.07402699440717697\n",
            "epoch 6 step 66 loss 0.06580663472414017\n",
            "epoch 6 step 67 loss 0.05207793414592743\n",
            "epoch 6 step 68 loss 0.05663342773914337\n",
            "epoch 6 step 69 loss 0.056097738444805145\n",
            "epoch 6 step 70 loss 0.04772195219993591\n",
            "epoch 6 step 71 loss 0.03155246376991272\n",
            "epoch 6 step 72 loss 0.06273231655359268\n",
            "epoch 6 step 73 loss 0.04502599686384201\n",
            "epoch 6 step 74 loss 0.07054515182971954\n",
            "epoch 6 step 75 loss 0.022000841796398163\n",
            "epoch 6 step 76 loss 0.08672413229942322\n",
            "epoch 6 step 77 loss 0.03494228422641754\n",
            "epoch 6 step 78 loss 0.03576866537332535\n",
            "epoch 6 step 79 loss 0.0595327727496624\n",
            "epoch 6 step 80 loss 0.042005009949207306\n",
            "epoch 6 step 81 loss 0.07044641673564911\n",
            "epoch 6 step 82 loss 0.05888693034648895\n",
            "epoch 6 step 83 loss 0.03767077252268791\n",
            "epoch 6 step 84 loss 0.07045059651136398\n",
            "epoch 6 step 85 loss 0.11804264783859253\n",
            "epoch 6 step 86 loss 0.03891148418188095\n",
            "epoch 6 step 87 loss 0.03788921609520912\n",
            "epoch 6 step 88 loss 0.04560361057519913\n",
            "epoch 6 step 89 loss 0.04884551465511322\n",
            "epoch 6 step 90 loss 0.06825605779886246\n",
            "epoch 6 step 91 loss 0.07256189733743668\n",
            "epoch 6 step 92 loss 0.046537451446056366\n",
            "epoch 6 step 93 loss 0.03985188156366348\n",
            "epoch 6 step 94 loss 0.10153776407241821\n",
            "epoch 6 step 95 loss 0.05105555057525635\n",
            "epoch 6 step 96 loss 0.1145821362733841\n",
            "epoch 6 step 97 loss 0.04538793861865997\n",
            "epoch 6 step 98 loss 0.0702502653002739\n",
            "epoch 6 step 99 loss 0.07829413563013077\n",
            "epoch 6 step 100 loss 0.05757997930049896\n",
            "epoch 6 step 101 loss 0.05785580724477768\n",
            "epoch 6 step 102 loss 0.040863510221242905\n",
            "epoch 6 step 103 loss 0.06899593770503998\n",
            "epoch 6 step 104 loss 0.02165645733475685\n",
            "epoch 6 step 105 loss 0.09138965606689453\n",
            "epoch 6 step 106 loss 0.062226537615060806\n",
            "epoch 6 step 107 loss 0.059019435197114944\n",
            "epoch 6 step 108 loss 0.03610081225633621\n",
            "epoch 6 step 109 loss 0.04131445288658142\n",
            "epoch 6 step 110 loss 0.04564062878489494\n",
            "epoch 6 step 111 loss 0.05206608772277832\n",
            "epoch 6 step 112 loss 0.04098743200302124\n",
            "epoch 6 step 113 loss 0.04578609764575958\n",
            "epoch 6 step 114 loss 0.035694047808647156\n",
            "epoch 6 step 115 loss 0.03953079134225845\n",
            "epoch 6 step 116 loss 0.04242228716611862\n",
            "epoch 6 step 117 loss 0.04187207296490669\n",
            "epoch 6 step 118 loss 0.061226312071084976\n",
            "epoch 6 step 119 loss 0.03760846331715584\n",
            "epoch 6 step 120 loss 0.048806849867105484\n",
            "epoch 6 step 121 loss 0.023798031732439995\n",
            "epoch 6 step 122 loss 0.04772757738828659\n",
            "epoch 6 step 123 loss 0.0602404847741127\n",
            "epoch 6 step 124 loss 0.08058911561965942\n",
            "epoch 6 step 125 loss 0.03493469953536987\n",
            "epoch 6 step 126 loss 0.0459308959543705\n",
            "epoch 6 step 127 loss 0.0779024064540863\n",
            "epoch 6 step 128 loss 0.049876708537340164\n",
            "epoch 6 step 129 loss 0.07884186506271362\n",
            "epoch 6 step 130 loss 0.059240180999040604\n",
            "epoch 6 step 131 loss 0.039154209196567535\n",
            "epoch 6 step 132 loss 0.04602265730500221\n",
            "epoch 6 step 133 loss 0.06714416295289993\n",
            "epoch 6 step 134 loss 0.025884348899126053\n",
            "epoch 6 step 135 loss 0.08671312034130096\n",
            "epoch 6 step 136 loss 0.055700622498989105\n",
            "epoch 6 step 137 loss 0.026729576289653778\n",
            "epoch 6 step 138 loss 0.05775042623281479\n",
            "epoch 6 step 139 loss 0.07115428149700165\n",
            "epoch 6 step 140 loss 0.04929237812757492\n",
            "epoch 6 step 141 loss 0.031193599104881287\n",
            "epoch 6 step 142 loss 0.04597048833966255\n",
            "epoch 6 step 143 loss 0.03634525090456009\n",
            "epoch 6 step 144 loss 0.05142957717180252\n",
            "epoch 6 step 145 loss 0.05894104763865471\n",
            "epoch 6 step 146 loss 0.03768981248140335\n",
            "epoch 6 step 147 loss 0.07173949480056763\n",
            "epoch 6 step 148 loss 0.04165831208229065\n",
            "epoch 6 step 149 loss 0.012860947288572788\n",
            "epoch 6 step 150 loss 0.08522002398967743\n",
            "epoch 6 step 151 loss 0.02425483427941799\n",
            "epoch 6 step 152 loss 0.05685117468237877\n",
            "epoch 6 step 153 loss 0.0684160590171814\n",
            "epoch 6 step 154 loss 0.01727050542831421\n",
            "epoch 6 step 155 loss 0.033839210867881775\n",
            "epoch 6 step 156 loss 0.08036772906780243\n",
            "epoch 6 step 157 loss 0.0410960428416729\n",
            "epoch 6 step 158 loss 0.037145309150218964\n",
            "epoch 6 step 159 loss 0.0524640791118145\n",
            "epoch 6 step 160 loss 0.0383869931101799\n",
            "epoch 6 step 161 loss 0.04667523503303528\n",
            "epoch 6 step 162 loss 0.048613592982292175\n",
            "epoch 6 step 163 loss 0.07446189224720001\n",
            "epoch 6 step 164 loss 0.03533031791448593\n",
            "epoch 6 step 165 loss 0.10959915816783905\n",
            "epoch 6 step 166 loss 0.04031498730182648\n",
            "epoch 6 step 167 loss 0.04167870059609413\n",
            "epoch 6 step 168 loss 0.05497375503182411\n",
            "epoch 6 step 169 loss 0.023046977818012238\n",
            "epoch 6 step 170 loss 0.06586999446153641\n",
            "epoch 6 step 171 loss 0.0497116893529892\n",
            "epoch 6 step 172 loss 0.023316994309425354\n",
            "epoch 6 step 173 loss 0.01507691852748394\n",
            "epoch 6 step 174 loss 0.056094806641340256\n",
            "epoch 6 step 175 loss 0.08702193200588226\n",
            "epoch 6 step 176 loss 0.05294244736433029\n",
            "epoch 6 step 177 loss 0.0498419925570488\n",
            "epoch 6 step 178 loss 0.061546869575977325\n",
            "epoch 6 step 179 loss 0.05989747494459152\n",
            "epoch 6 step 180 loss 0.048318684101104736\n",
            "epoch 6 step 181 loss 0.06016814708709717\n",
            "epoch 6 step 182 loss 0.02736056037247181\n",
            "epoch 6 step 183 loss 0.0548422709107399\n",
            "epoch 6 step 184 loss 0.0460432693362236\n",
            "epoch 6 step 185 loss 0.07866847515106201\n",
            "epoch 6 step 186 loss 0.020391574129462242\n",
            "epoch 6 step 187 loss 0.053147412836551666\n",
            "epoch 6 step 188 loss 0.0583295077085495\n",
            "epoch 6 step 189 loss 0.07322049885988235\n",
            "epoch 6 step 190 loss 0.039886973798274994\n",
            "epoch 6 step 191 loss 0.07672800123691559\n",
            "epoch 6 step 192 loss 0.04860972240567207\n",
            "epoch 6 step 193 loss 0.04441618546843529\n",
            "epoch 6 step 194 loss 0.0347885936498642\n",
            "epoch 6 step 195 loss 0.06459421664476395\n",
            "epoch 6 step 196 loss 0.08521829545497894\n",
            "epoch 6 step 197 loss 0.056474655866622925\n",
            "epoch 6 step 198 loss 0.06915212422609329\n",
            "epoch 6 step 199 loss 0.03106105886399746\n",
            "epoch 6 step 200 loss 0.0468495637178421\n",
            "epoch 6 step 201 loss 0.09505681693553925\n",
            "epoch 6 step 202 loss 0.0465785451233387\n",
            "epoch 6 step 203 loss 0.04580456390976906\n",
            "epoch 6 step 204 loss 0.06207156926393509\n",
            "epoch 6 step 205 loss 0.0761808454990387\n",
            "epoch 6 step 206 loss 0.057025227695703506\n",
            "epoch 6 step 207 loss 0.07452289760112762\n",
            "epoch 6 step 208 loss 0.07393668591976166\n",
            "epoch 6 step 209 loss 0.06652547419071198\n",
            "epoch 6 step 210 loss 0.11124275624752045\n",
            "epoch 6 step 211 loss 0.06724932789802551\n",
            "epoch 6 step 212 loss 0.06278811395168304\n",
            "epoch 6 step 213 loss 0.03365856036543846\n",
            "epoch 6 step 214 loss 0.08474621921777725\n",
            "epoch 6 step 215 loss 0.036792486906051636\n",
            "epoch 6 step 216 loss 0.04283234849572182\n",
            "epoch 6 step 217 loss 0.07227951288223267\n",
            "epoch 6 step 218 loss 0.05255358666181564\n",
            "epoch 6 step 219 loss 0.04605366289615631\n",
            "epoch 6 step 220 loss 0.06783676892518997\n",
            "epoch 6 step 221 loss 0.0613546222448349\n",
            "epoch 6 step 222 loss 0.0727451965212822\n",
            "epoch 6 step 223 loss 0.08923956751823425\n",
            "epoch 6 step 224 loss 0.1288244128227234\n",
            "epoch 6 step 225 loss 0.019428003579378128\n",
            "epoch 6 step 226 loss 0.0697382241487503\n",
            "epoch 6 step 227 loss 0.05129445344209671\n",
            "epoch 6 step 228 loss 0.034225281327962875\n",
            "epoch 6 step 229 loss 0.07811987400054932\n",
            "epoch 6 step 230 loss 0.060302115976810455\n",
            "epoch 6 step 231 loss 0.049896273761987686\n",
            "epoch 6 step 232 loss 0.061791010200977325\n",
            "epoch 6 step 233 loss 0.047865621745586395\n",
            "epoch 6 step 234 loss 0.049803100526332855\n",
            "epoch 6 step 235 loss 0.04562901332974434\n",
            "epoch 6 step 236 loss 0.0318005308508873\n",
            "epoch 6 step 237 loss 0.0234433114528656\n",
            "epoch 6 step 238 loss 0.05752979964017868\n",
            "epoch 6 step 239 loss 0.13865534961223602\n",
            "epoch 6 step 240 loss 0.06924576312303543\n",
            "epoch 6 step 241 loss 0.06331437826156616\n",
            "epoch 6 step 242 loss 0.029565265402197838\n",
            "epoch 6 step 243 loss 0.1155240535736084\n",
            "epoch 6 step 244 loss 0.04470580071210861\n",
            "epoch 6 step 245 loss 0.058440424501895905\n",
            "epoch 6 step 246 loss 0.04505221173167229\n",
            "epoch 6 step 247 loss 0.08030816912651062\n",
            "epoch 6 step 248 loss 0.0696626827120781\n",
            "epoch 6 step 249 loss 0.046610474586486816\n",
            "epoch 6 step 250 loss 0.04177241027355194\n",
            "epoch 6 step 251 loss 0.08820901066064835\n",
            "epoch 6 step 252 loss 0.06927752494812012\n",
            "epoch 6 step 253 loss 0.0491873137652874\n",
            "epoch 6 step 254 loss 0.054485004395246506\n",
            "epoch 6 step 255 loss 0.07273051142692566\n",
            "epoch 6 step 256 loss 0.08291316032409668\n",
            "epoch 6 step 257 loss 0.04855748638510704\n",
            "epoch 6 step 258 loss 0.04035766422748566\n",
            "epoch 6 step 259 loss 0.024677395820617676\n",
            "epoch 6 step 260 loss 0.03819593787193298\n",
            "epoch 6 step 261 loss 0.06402681767940521\n",
            "epoch 6 step 262 loss 0.05275872349739075\n",
            "epoch 6 step 263 loss 0.08402326703071594\n",
            "epoch 6 step 264 loss 0.06204694136977196\n",
            "epoch 6 step 265 loss 0.03856552019715309\n",
            "epoch 6 step 266 loss 0.06289634108543396\n",
            "epoch 6 step 267 loss 0.04116746038198471\n",
            "epoch 6 step 268 loss 0.041625700891017914\n",
            "epoch 6 step 269 loss 0.05822813883423805\n",
            "epoch 6 step 270 loss 0.06586164236068726\n",
            "epoch 6 step 271 loss 0.022423842921853065\n",
            "epoch 6 step 272 loss 0.05292436480522156\n",
            "epoch 6 step 273 loss 0.06371250003576279\n",
            "epoch 6 step 274 loss 0.042813491076231\n",
            "epoch 6 step 275 loss 0.041360363364219666\n",
            "epoch 6 step 276 loss 0.05166228115558624\n",
            "epoch 6 step 277 loss 0.030805416405200958\n",
            "epoch 6 step 278 loss 0.06876309961080551\n",
            "epoch 6 step 279 loss 0.06874573975801468\n",
            "epoch 6 step 280 loss 0.05173378437757492\n",
            "epoch 6 step 281 loss 0.037622421979904175\n",
            "epoch 6 step 282 loss 0.052167922258377075\n",
            "epoch 6 step 283 loss 0.037978675216436386\n",
            "epoch 6 step 284 loss 0.04766911268234253\n",
            "epoch 6 step 285 loss 0.03823787719011307\n",
            "epoch 6 step 286 loss 0.0749891996383667\n",
            "epoch 6 step 287 loss 0.029710575938224792\n",
            "epoch 6 step 288 loss 0.05407201498746872\n",
            "epoch 6 step 289 loss 0.07535640895366669\n",
            "epoch 6 step 290 loss 0.054792050272226334\n",
            "epoch 6 step 291 loss 0.07384076714515686\n",
            "epoch 6 step 292 loss 0.09431298077106476\n",
            "epoch 6 step 293 loss 0.06284026801586151\n",
            "epoch 6 step 294 loss 0.058090608566999435\n",
            "epoch 6 step 295 loss 0.05728679150342941\n",
            "epoch 6 step 296 loss 0.07170715928077698\n",
            "epoch 6 step 297 loss 0.04583904892206192\n",
            "epoch 6 step 298 loss 0.05515444278717041\n",
            "epoch 6 step 299 loss 0.05928092077374458\n",
            "epoch 6 step 300 loss 0.06343039870262146\n",
            "epoch 6 step 301 loss 0.09570792317390442\n",
            "epoch 6 step 302 loss 0.04447350651025772\n",
            "epoch 6 step 303 loss 0.04616938531398773\n",
            "epoch 6 step 304 loss 0.04212167114019394\n",
            "epoch 6 step 305 loss 0.058625299483537674\n",
            "epoch 6 step 306 loss 0.06113282963633537\n",
            "epoch 6 step 307 loss 0.02866634912788868\n",
            "epoch 6 step 308 loss 0.10127091407775879\n",
            "epoch 6 step 309 loss 0.053153492510318756\n",
            "epoch 6 step 310 loss 0.040445927530527115\n",
            "epoch 6 step 311 loss 0.056431420147418976\n",
            "epoch 6 step 312 loss 0.025565773248672485\n",
            "epoch 6 step 313 loss 0.04196285083889961\n",
            "epoch 6 step 314 loss 0.0635734349489212\n",
            "epoch 6 step 315 loss 0.02980971708893776\n",
            "epoch 6 step 316 loss 0.07742781937122345\n",
            "epoch 6 step 317 loss 0.0851103663444519\n",
            "epoch 6 step 318 loss 0.04277057200670242\n",
            "epoch 6 step 319 loss 0.041729316115379333\n",
            "epoch 6 step 320 loss 0.10430508852005005\n",
            "epoch 6 step 321 loss 0.03628837317228317\n",
            "epoch 6 step 322 loss 0.059299975633621216\n",
            "epoch 6 step 323 loss 0.05257533863186836\n",
            "epoch 6 step 324 loss 0.0500456765294075\n",
            "epoch 6 step 325 loss 0.03954858332872391\n",
            "epoch 6 step 326 loss 0.04331641644239426\n",
            "epoch 6 step 327 loss 0.03159873187541962\n",
            "epoch 6 step 328 loss 0.04614783450961113\n",
            "epoch 6 step 329 loss 0.04652399942278862\n",
            "epoch 6 step 330 loss 0.034039925783872604\n",
            "epoch 6 step 331 loss 0.052834466099739075\n",
            "epoch 6 step 332 loss 0.03948140889406204\n",
            "epoch 6 step 333 loss 0.07574322074651718\n",
            "epoch 6 step 334 loss 0.021801922470331192\n",
            "epoch 6 step 335 loss 0.04579818248748779\n",
            "epoch 6 step 336 loss 0.0469808429479599\n",
            "epoch 6 step 337 loss 0.036621805280447006\n",
            "epoch 6 step 338 loss 0.03914602845907211\n",
            "epoch 6 step 339 loss 0.041607566177845\n",
            "epoch 6 step 340 loss 0.03809812292456627\n",
            "epoch 6 step 341 loss 0.05354415997862816\n",
            "epoch 6 step 342 loss 0.045405276119709015\n",
            "epoch 6 step 343 loss 0.06205806881189346\n",
            "epoch 6 step 344 loss 0.03672996535897255\n",
            "epoch 6 step 345 loss 0.028142116963863373\n",
            "epoch 6 step 346 loss 0.022494308650493622\n",
            "epoch 6 step 347 loss 0.05951927974820137\n",
            "epoch 6 step 348 loss 0.03422793000936508\n",
            "epoch 6 step 349 loss 0.04320080205798149\n",
            "epoch 6 step 350 loss 0.057998865842819214\n",
            "epoch 6 step 351 loss 0.04097682237625122\n",
            "epoch 6 step 352 loss 0.06597620248794556\n",
            "epoch 6 step 353 loss 0.04788294807076454\n",
            "epoch 6 step 354 loss 0.06517449021339417\n",
            "epoch 6 step 355 loss 0.047884561121463776\n",
            "epoch 6 step 356 loss 0.06387747824192047\n",
            "epoch 6 step 357 loss 0.04278311878442764\n",
            "epoch 6 step 358 loss 0.051111966371536255\n",
            "epoch 6 step 359 loss 0.06201821565628052\n",
            "epoch 6 step 360 loss 0.03771401196718216\n",
            "epoch 6 step 361 loss 0.038698792457580566\n",
            "epoch 6 step 362 loss 0.05236324667930603\n",
            "epoch 6 step 363 loss 0.0813560038805008\n",
            "epoch 6 step 364 loss 0.03859347105026245\n",
            "epoch 6 step 365 loss 0.04099530726671219\n",
            "epoch 6 step 366 loss 0.05285949259996414\n",
            "epoch 6 step 367 loss 0.027600295841693878\n",
            "epoch 6 step 368 loss 0.05066581070423126\n",
            "epoch 6 step 369 loss 0.0575735867023468\n",
            "epoch 6 step 370 loss 0.06775164604187012\n",
            "epoch 6 step 371 loss 0.06159858778119087\n",
            "epoch 6 step 372 loss 0.04901202768087387\n",
            "epoch 6 step 373 loss 0.03145107626914978\n",
            "epoch 6 step 374 loss 0.04926414415240288\n",
            "epoch 6 step 375 loss 0.056501664221286774\n",
            "epoch 6 step 376 loss 0.04392619431018829\n",
            "epoch 6 step 377 loss 0.05871479958295822\n",
            "epoch 6 step 378 loss 0.033450908958911896\n",
            "epoch 6 step 379 loss 0.062526635825634\n",
            "epoch 6 step 380 loss 0.04513011500239372\n",
            "epoch 6 step 381 loss 0.044317588210105896\n",
            "epoch 6 step 382 loss 0.05977717041969299\n",
            "epoch 6 step 383 loss 0.045033615082502365\n",
            "epoch 6 step 384 loss 0.025295807048678398\n",
            "epoch 6 step 385 loss 0.04583404213190079\n",
            "epoch 6 step 386 loss 0.042837172746658325\n",
            "epoch 6 step 387 loss 0.04754645377397537\n",
            "epoch 6 step 388 loss 0.02647785097360611\n",
            "epoch 6 step 389 loss 0.05820304527878761\n",
            "epoch 6 step 390 loss 0.05777006596326828\n",
            "epoch 6 step 391 loss 0.07743177562952042\n",
            "epoch 6 step 392 loss 0.03554558753967285\n",
            "epoch 6 step 393 loss 0.06586221605539322\n",
            "epoch 6 step 394 loss 0.031041044741868973\n",
            "epoch 6 step 395 loss 0.07225543260574341\n",
            "epoch 6 step 396 loss 0.04286686331033707\n",
            "epoch 6 step 397 loss 0.0425843819975853\n",
            "epoch 6 step 398 loss 0.03259369730949402\n",
            "epoch 6 step 399 loss 0.06262029707431793\n",
            "epoch 6 step 400 loss 0.05889738351106644\n",
            "epoch 6 step 401 loss 0.03099900856614113\n",
            "epoch 6 step 402 loss 0.026787932962179184\n",
            "epoch 6 step 403 loss 0.05019811913371086\n",
            "epoch 6 step 404 loss 0.054294828325510025\n",
            "epoch 6 step 405 loss 0.057443343102931976\n",
            "epoch 6 step 406 loss 0.03691398724913597\n",
            "epoch 6 step 407 loss 0.06295456737279892\n",
            "epoch 6 step 408 loss 0.03582632541656494\n",
            "epoch 6 step 409 loss 0.07316554337739944\n",
            "epoch 6 step 410 loss 0.04704836755990982\n",
            "epoch 6 step 411 loss 0.06297224760055542\n",
            "epoch 6 step 412 loss 0.06845264136791229\n",
            "epoch 6 step 413 loss 0.037337031215429306\n",
            "epoch 6 step 414 loss 0.06448011100292206\n",
            "epoch 6 step 415 loss 0.029760735109448433\n",
            "epoch 6 step 416 loss 0.040971748530864716\n",
            "epoch 6 step 417 loss 0.09962959587574005\n",
            "epoch 6 step 418 loss 0.06476472318172455\n",
            "epoch 6 step 419 loss 0.0381198525428772\n",
            "epoch 6 step 420 loss 0.040064118802547455\n",
            "epoch 6 step 421 loss 0.05747874826192856\n",
            "epoch 6 step 422 loss 0.07130854576826096\n",
            "epoch 6 step 423 loss 0.04505237936973572\n",
            "epoch 6 step 424 loss 0.03447096049785614\n",
            "epoch 6 step 425 loss 0.02722630277276039\n",
            "epoch 6 step 426 loss 0.05959542840719223\n",
            "epoch 6 step 427 loss 0.09178827702999115\n",
            "epoch 6 step 428 loss 0.06783738732337952\n",
            "epoch 6 step 429 loss 0.0895133912563324\n",
            "epoch 6 step 430 loss 0.04105275869369507\n",
            "epoch 6 step 431 loss 0.06811293959617615\n",
            "epoch 6 step 432 loss 0.03747659921646118\n",
            "epoch 6 step 433 loss 0.07798177003860474\n",
            "epoch 6 step 434 loss 0.061951279640197754\n",
            "epoch 6 step 435 loss 0.10805974900722504\n",
            "epoch 6 step 436 loss 0.05574587732553482\n",
            "epoch 6 step 437 loss 0.06657416373491287\n",
            "epoch 6 step 438 loss 0.051311198621988297\n",
            "epoch 6 step 439 loss 0.04978537932038307\n",
            "epoch 6 step 440 loss 0.035206831991672516\n",
            "epoch 6 step 441 loss 0.13770809769630432\n",
            "epoch 6 step 442 loss 0.10240820050239563\n",
            "epoch 6 step 443 loss 0.06040897220373154\n",
            "epoch 6 step 444 loss 0.05640939250588417\n",
            "epoch 6 step 445 loss 0.06752759963274002\n",
            "epoch 6 step 446 loss 0.06711103767156601\n",
            "epoch 6 step 447 loss 0.09381233155727386\n",
            "epoch 6 step 448 loss 0.04859274998307228\n",
            "epoch 6 step 449 loss 0.033048056066036224\n",
            "epoch 6 step 450 loss 0.065880686044693\n",
            "epoch 6 step 451 loss 0.07475534826517105\n",
            "epoch 6 step 452 loss 0.025554005056619644\n",
            "epoch 6 step 453 loss 0.04507981613278389\n",
            "epoch 6 step 454 loss 0.06095818430185318\n",
            "epoch 6 step 455 loss 0.04825564846396446\n",
            "epoch 6 step 456 loss 0.08978089690208435\n",
            "epoch 6 step 457 loss 0.049345940351486206\n",
            "epoch 6 step 458 loss 0.052391696721315384\n",
            "epoch 6 step 459 loss 0.07898174226284027\n",
            "epoch 6 step 460 loss 0.044534578919410706\n",
            "epoch 6 step 461 loss 0.041603319346904755\n",
            "epoch 6 step 462 loss 0.05817963182926178\n",
            "epoch 6 step 463 loss 0.0935727059841156\n",
            "epoch 6 step 464 loss 0.06739675998687744\n",
            "epoch 6 step 465 loss 0.029576759785413742\n",
            "epoch 6 step 466 loss 0.047408394515514374\n",
            "epoch 6 step 467 loss 0.06878158450126648\n",
            "epoch 6 step 468 loss 0.05173424258828163\n",
            "epoch 6 step 469 loss 0.0420837327837944\n",
            "epoch 6 step 470 loss 0.041256245225667953\n",
            "epoch 6 step 471 loss 0.08762317895889282\n",
            "epoch 6 step 472 loss 0.09562128782272339\n",
            "epoch 6 step 473 loss 0.07092966884374619\n",
            "epoch 6 step 474 loss 0.03412005305290222\n",
            "epoch 6 step 475 loss 0.12447451800107956\n",
            "epoch 6 step 476 loss 0.04391701519489288\n",
            "epoch 6 step 477 loss 0.08745607733726501\n",
            "epoch 6 step 478 loss 0.11943937093019485\n",
            "epoch 6 step 479 loss 0.06915327906608582\n",
            "epoch 6 step 480 loss 0.11507309228181839\n",
            "epoch 6 step 481 loss 0.03306567668914795\n",
            "epoch 6 step 482 loss 0.12662070989608765\n",
            "epoch 6 step 483 loss 0.06718506664037704\n",
            "epoch 6 step 484 loss 0.07880999147891998\n",
            "epoch 6 step 485 loss 0.09922899305820465\n",
            "epoch 6 step 486 loss 0.1252131164073944\n",
            "epoch 6 step 487 loss 0.10012692213058472\n",
            "epoch 6 step 488 loss 0.0684879869222641\n",
            "epoch 6 step 489 loss 0.09919006377458572\n",
            "epoch 6 step 490 loss 0.07169056683778763\n",
            "epoch 6 step 491 loss 0.04104016348719597\n",
            "epoch 6 step 492 loss 0.05142202973365784\n",
            "epoch 6 step 493 loss 0.08421298861503601\n",
            "epoch 6 step 494 loss 0.0705174058675766\n",
            "epoch 6 step 495 loss 0.0431501567363739\n",
            "epoch 6 step 496 loss 0.0738426074385643\n",
            "epoch 6 step 497 loss 0.07096678763628006\n",
            "epoch 6 step 498 loss 0.054170168936252594\n",
            "epoch 6 step 499 loss 0.04204582795500755\n",
            "epoch 6 step 500 loss 0.02442168816924095\n",
            "epoch 6 step 501 loss 0.048994190990924835\n",
            "epoch 6 step 502 loss 0.029177002608776093\n",
            "epoch 6 step 503 loss 0.05906864255666733\n",
            "epoch 6 step 504 loss 0.054882410913705826\n",
            "epoch 6 step 505 loss 0.06632617861032486\n",
            "epoch 6 step 506 loss 0.02761290967464447\n",
            "epoch 6 step 507 loss 0.07427919656038284\n",
            "epoch 6 step 508 loss 0.07812245935201645\n",
            "epoch 6 step 509 loss 0.03959014639258385\n",
            "epoch 6 step 510 loss 0.07524390518665314\n",
            "epoch 6 step 511 loss 0.09480246901512146\n",
            "epoch 6 step 512 loss 0.07781296223402023\n",
            "epoch 6 step 513 loss 0.09137758612632751\n",
            "epoch 6 step 514 loss 0.030432023108005524\n",
            "epoch 6 step 515 loss 0.05877429246902466\n",
            "epoch 6 step 516 loss 0.058094315230846405\n",
            "epoch 6 step 517 loss 0.03949875012040138\n",
            "epoch 6 step 518 loss 0.04272862896323204\n",
            "epoch 6 step 519 loss 0.07864639163017273\n",
            "epoch 6 step 520 loss 0.11991167068481445\n",
            "epoch 6 step 521 loss 0.09441079944372177\n",
            "epoch 6 step 522 loss 0.04963090270757675\n",
            "epoch 6 step 523 loss 0.054674066603183746\n",
            "epoch 6 step 524 loss 0.07219009101390839\n",
            "epoch 6 step 525 loss 0.026652097702026367\n",
            "epoch 6 step 526 loss 0.024777378886938095\n",
            "epoch 6 step 527 loss 0.04569737985730171\n",
            "epoch 6 step 528 loss 0.03561142832040787\n",
            "epoch 6 step 529 loss 0.07682906091213226\n",
            "epoch 6 step 530 loss 0.05599970370531082\n",
            "epoch 6 step 531 loss 0.05832405388355255\n",
            "epoch 6 step 532 loss 0.05294215679168701\n",
            "epoch 6 step 533 loss 0.040868502110242844\n",
            "epoch 6 step 534 loss 0.015681132674217224\n",
            "epoch 6 step 535 loss 0.057150617241859436\n",
            "epoch 6 step 536 loss 0.03823428601026535\n",
            "epoch 6 step 537 loss 0.06293851137161255\n",
            "epoch 6 step 538 loss 0.08317965269088745\n",
            "epoch 6 step 539 loss 0.052356839179992676\n",
            "epoch 6 step 540 loss 0.05949513614177704\n",
            "epoch 6 step 541 loss 0.06440418213605881\n",
            "epoch 6 step 542 loss 0.05129903554916382\n",
            "epoch 6 step 543 loss 0.08283470571041107\n",
            "epoch 6 step 544 loss 0.06671467423439026\n",
            "epoch 6 step 545 loss 0.056783754378557205\n",
            "epoch 6 step 546 loss 0.059450652450323105\n",
            "epoch 6 step 547 loss 0.08786560595035553\n",
            "epoch 6 step 548 loss 0.09367832541465759\n",
            "epoch 6 step 549 loss 0.022610187530517578\n",
            "epoch 6 step 550 loss 0.03455410897731781\n",
            "epoch 6 step 551 loss 0.09304836392402649\n",
            "epoch 6 step 552 loss 0.10702778398990631\n",
            "epoch 6 step 553 loss 0.06427035480737686\n",
            "epoch 6 step 554 loss 0.0553896427154541\n",
            "epoch 6 step 555 loss 0.034531135112047195\n",
            "epoch 6 step 556 loss 0.06478965282440186\n",
            "epoch 6 step 557 loss 0.03478701412677765\n",
            "epoch 6 step 558 loss 0.068475641310215\n",
            "epoch 6 step 559 loss 0.08097819238901138\n",
            "epoch 6 step 560 loss 0.06396803259849548\n",
            "epoch 6 step 561 loss 0.03192828223109245\n",
            "epoch 6 step 562 loss 0.05824688822031021\n",
            "epoch 6 step 563 loss 0.048928309231996536\n",
            "epoch 6 step 564 loss 0.044422000646591187\n",
            "epoch 6 step 565 loss 0.02818705141544342\n",
            "epoch 6 step 566 loss 0.06518898904323578\n",
            "epoch 6 step 567 loss 0.06152388080954552\n",
            "epoch 6 step 568 loss 0.04745367914438248\n",
            "epoch 6 step 569 loss 0.07678912580013275\n",
            "epoch 6 step 570 loss 0.014020577073097229\n",
            "epoch 6 step 571 loss 0.06865726411342621\n",
            "epoch 6 step 572 loss 0.06537406891584396\n",
            "epoch 6 step 573 loss 0.03310612589120865\n",
            "epoch 6 step 574 loss 0.06186767667531967\n",
            "epoch 6 step 575 loss 0.07258808612823486\n",
            "epoch 6 step 576 loss 0.05817320942878723\n",
            "epoch 6 step 577 loss 0.06540368497371674\n",
            "epoch 6 step 578 loss 0.0587395578622818\n",
            "epoch 6 step 579 loss 0.016718227416276932\n",
            "epoch 6 step 580 loss 0.035892583429813385\n",
            "epoch 6 step 581 loss 0.05245739594101906\n",
            "epoch 6 step 582 loss 0.07248933613300323\n",
            "epoch 6 step 583 loss 0.06485144048929214\n",
            "epoch 6 step 584 loss 0.041872382164001465\n",
            "epoch 6 step 585 loss 0.04626072943210602\n",
            "epoch 6 step 586 loss 0.025339167565107346\n",
            "epoch 6 step 587 loss 0.06372102349996567\n",
            "epoch 6 step 588 loss 0.06364039331674576\n",
            "epoch 6 step 589 loss 0.04416704922914505\n",
            "epoch 6 step 590 loss 0.05632174015045166\n",
            "epoch 6 step 591 loss 0.02477516420185566\n",
            "epoch 6 step 592 loss 0.06437651067972183\n",
            "epoch 6 step 593 loss 0.08674991130828857\n",
            "epoch 6 step 594 loss 0.025967441499233246\n",
            "epoch 6 step 595 loss 0.05297871306538582\n",
            "epoch 6 step 596 loss 0.059998515993356705\n",
            "epoch 6 step 597 loss 0.027720779180526733\n",
            "epoch 6 step 598 loss 0.05583425238728523\n",
            "epoch 6 step 599 loss 0.030364053323864937\n",
            "epoch 6 step 600 loss 0.02121923863887787\n",
            "epoch 6 step 601 loss 0.051447391510009766\n",
            "epoch 6 step 602 loss 0.05105631798505783\n",
            "epoch 6 step 603 loss 0.03697540983557701\n",
            "epoch 6 step 604 loss 0.054752010852098465\n",
            "epoch 6 step 605 loss 0.04575863480567932\n",
            "epoch 6 step 606 loss 0.07734858989715576\n",
            "epoch 6 step 607 loss 0.04143056273460388\n",
            "epoch 6 step 608 loss 0.037146955728530884\n",
            "epoch 6 step 609 loss 0.04674414172768593\n",
            "epoch 6 step 610 loss 0.04464755207300186\n",
            "epoch 6 step 611 loss 0.05553862452507019\n",
            "epoch 6 step 612 loss 0.026296082884073257\n",
            "epoch 6 step 613 loss 0.0813852921128273\n",
            "epoch 6 step 614 loss 0.0313267782330513\n",
            "epoch 6 step 615 loss 0.05739356577396393\n",
            "epoch 6 step 616 loss 0.029131755232810974\n",
            "epoch 6 step 617 loss 0.03419345244765282\n",
            "epoch 6 step 618 loss 0.08027186989784241\n",
            "epoch 6 step 619 loss 0.02897755801677704\n",
            "epoch 6 step 620 loss 0.04429970681667328\n",
            "epoch 6 step 621 loss 0.03658786788582802\n",
            "epoch 6 step 622 loss 0.08515740931034088\n",
            "epoch 6 step 623 loss 0.06784746050834656\n",
            "epoch 6 step 624 loss 0.029469527304172516\n",
            "epoch 6 step 625 loss 0.027214551344513893\n",
            "epoch 6 step 626 loss 0.026906810700893402\n",
            "epoch 6 step 627 loss 0.07538577914237976\n",
            "epoch 6 step 628 loss 0.02551906183362007\n",
            "epoch 6 step 629 loss 0.049325987696647644\n",
            "epoch 6 step 630 loss 0.037450406700372696\n",
            "epoch 6 step 631 loss 0.06627318263053894\n",
            "epoch 6 step 632 loss 0.056883104145526886\n",
            "epoch 6 step 633 loss 0.045241549611091614\n",
            "epoch 6 step 634 loss 0.06998221576213837\n",
            "epoch 6 step 635 loss 0.055729255080223083\n",
            "epoch 6 step 636 loss 0.0428001806139946\n",
            "epoch 6 step 637 loss 0.0509488582611084\n",
            "epoch 6 step 638 loss 0.057075150310993195\n",
            "epoch 6 step 639 loss 0.03487744927406311\n",
            "epoch 6 step 640 loss 0.029912132769823074\n",
            "epoch 6 step 641 loss 0.06509483605623245\n",
            "epoch 6 step 642 loss 0.06051245704293251\n",
            "epoch 6 step 643 loss 0.06955781579017639\n",
            "epoch 6 step 644 loss 0.10866520553827286\n",
            "epoch 6 step 645 loss 0.037464939057826996\n",
            "epoch 6 step 646 loss 0.08041758835315704\n",
            "epoch 6 step 647 loss 0.04946019500494003\n",
            "epoch 6 step 648 loss 0.04914186894893646\n",
            "epoch 6 step 649 loss 0.08128340542316437\n",
            "epoch 6 step 650 loss 0.06144885718822479\n",
            "epoch 6 step 651 loss 0.04397944360971451\n",
            "epoch 6 step 652 loss 0.054868973791599274\n",
            "epoch 6 step 653 loss 0.069761261343956\n",
            "epoch 6 step 654 loss 0.04059368371963501\n",
            "epoch 6 step 655 loss 0.04401740804314613\n",
            "epoch 6 step 656 loss 0.03835870325565338\n",
            "epoch 6 step 657 loss 0.025887969881296158\n",
            "epoch 6 step 658 loss 0.010698090307414532\n",
            "epoch 6 step 659 loss 0.06811006367206573\n",
            "epoch 6 step 660 loss 0.04029503092169762\n",
            "epoch 6 step 661 loss 0.03782439976930618\n",
            "epoch 6 step 662 loss 0.058325737714767456\n",
            "epoch 6 step 663 loss 0.05521726235747337\n",
            "epoch 6 step 664 loss 0.03186408057808876\n",
            "epoch 6 step 665 loss 0.06417916715145111\n",
            "epoch 6 step 666 loss 0.07580214738845825\n",
            "epoch 6 step 667 loss 0.05071452260017395\n",
            "epoch 6 step 668 loss 0.042452145367860794\n",
            "epoch 6 step 669 loss 0.05448180437088013\n",
            "epoch 6 step 670 loss 0.0751432403922081\n",
            "epoch 6 step 671 loss 0.028093192726373672\n",
            "epoch 6 step 672 loss 0.07371220737695694\n",
            "epoch 6 step 673 loss 0.06858214735984802\n",
            "epoch 6 step 674 loss 0.06733061373233795\n",
            "epoch 6 step 675 loss 0.02272670716047287\n",
            "epoch 6 step 676 loss 0.049037035554647446\n",
            "epoch 6 step 677 loss 0.02746872417628765\n",
            "epoch 6 step 678 loss 0.05132915824651718\n",
            "epoch 6 step 679 loss 0.057024020701646805\n",
            "epoch 6 step 680 loss 0.05928405374288559\n",
            "epoch 6 step 681 loss 0.052535295486450195\n",
            "epoch 6 step 682 loss 0.08845935761928558\n",
            "epoch 6 step 683 loss 0.04765336960554123\n",
            "epoch 6 step 684 loss 0.04440008103847504\n",
            "epoch 6 step 685 loss 0.03153187409043312\n",
            "epoch 6 step 686 loss 0.08776115626096725\n",
            "epoch 6 step 687 loss 0.03203469514846802\n",
            "epoch 6 step 688 loss 0.06917349249124527\n",
            "epoch 6 step 689 loss 0.024334413930773735\n",
            "epoch 6 step 690 loss 0.0438731387257576\n",
            "epoch 6 step 691 loss 0.08088131994009018\n",
            "epoch 6 step 692 loss 0.11901751160621643\n",
            "epoch 6 step 693 loss 0.05981656163930893\n",
            "epoch 6 step 694 loss 0.03943004459142685\n",
            "epoch 6 step 695 loss 0.030545229092240334\n",
            "epoch 6 step 696 loss 0.03454667329788208\n",
            "epoch 6 step 697 loss 0.06703974306583405\n",
            "epoch 6 step 698 loss 0.03509523719549179\n",
            "epoch 6 step 699 loss 0.0317331962287426\n",
            "epoch 6 step 700 loss 0.05667857453227043\n",
            "epoch 6 step 701 loss 0.03997784107923508\n",
            "epoch 6 step 702 loss 0.046913325786590576\n",
            "epoch 6 step 703 loss 0.037666287273168564\n",
            "epoch 6 step 704 loss 0.028263533487915993\n",
            "epoch 6 step 705 loss 0.022046692669391632\n",
            "epoch 6 step 706 loss 0.04060281440615654\n",
            "epoch 6 step 707 loss 0.034107506275177\n",
            "epoch 6 step 708 loss 0.03717421367764473\n",
            "epoch 6 step 709 loss 0.04125657677650452\n",
            "epoch 6 step 710 loss 0.05380667746067047\n",
            "epoch 6 step 711 loss 0.0626477599143982\n",
            "epoch 6 step 712 loss 0.08439439535140991\n",
            "epoch 6 step 713 loss 0.028640098869800568\n",
            "epoch 6 step 714 loss 0.05612906068563461\n",
            "epoch 6 step 715 loss -0.005592054687440395\n",
            "epoch 6 step 716 loss 0.04824218153953552\n",
            "epoch 6 step 717 loss 0.10246294736862183\n",
            "epoch 6 step 718 loss 0.06144237890839577\n",
            "epoch 6 step 719 loss 0.08295325934886932\n",
            "epoch 6 step 720 loss 0.09716904163360596\n",
            "epoch 6 step 721 loss 0.05453328788280487\n",
            "epoch 6 step 722 loss 0.11409921944141388\n",
            "epoch 6 step 723 loss 0.06033553183078766\n",
            "epoch 6 step 724 loss 0.03879451006650925\n",
            "epoch 6 step 725 loss 0.05816434323787689\n",
            "epoch 6 step 726 loss 0.06361670047044754\n",
            "epoch 6 step 727 loss 0.06171722337603569\n",
            "epoch 6 step 728 loss 0.12361845374107361\n",
            "epoch 6 step 729 loss 0.030159223824739456\n",
            "epoch 6 step 730 loss 0.04965302348136902\n",
            "epoch 6 step 731 loss 0.04263351857662201\n",
            "epoch 6 step 732 loss 0.1344970166683197\n",
            "epoch 6 step 733 loss 0.06536597013473511\n",
            "epoch 6 step 734 loss 0.04750889912247658\n",
            "epoch 6 step 735 loss 0.10399536043405533\n",
            "epoch 6 step 736 loss 0.07065601646900177\n",
            "epoch 6 step 737 loss 0.03446223586797714\n",
            "epoch 6 step 738 loss 0.034833479672670364\n",
            "epoch 6 step 739 loss 0.10425353795289993\n",
            "epoch 6 step 740 loss 0.06622917205095291\n",
            "epoch 6 step 741 loss 0.11177806556224823\n",
            "epoch 6 step 742 loss 0.06599394977092743\n",
            "epoch 6 step 743 loss 0.030454905703663826\n",
            "epoch 6 step 744 loss 0.09740632772445679\n",
            "epoch 6 step 745 loss 0.040265873074531555\n",
            "epoch 6 step 746 loss 0.05834886431694031\n",
            "epoch 6 step 747 loss 0.022290635854005814\n",
            "epoch 6 step 748 loss 0.053042858839035034\n",
            "epoch 6 step 749 loss 0.0334763377904892\n",
            "epoch 6 step 750 loss 0.06065789610147476\n",
            "epoch 6 step 751 loss 0.09637428820133209\n",
            "epoch 6 step 752 loss 0.039113469421863556\n",
            "epoch 6 step 753 loss 0.07972516864538193\n",
            "epoch 6 step 754 loss 0.06172424554824829\n",
            "epoch 6 step 755 loss 0.04422527924180031\n",
            "epoch 6 step 756 loss 0.042578548192977905\n",
            "epoch 6 step 757 loss 0.044607825577259064\n",
            "epoch 6 step 758 loss 0.05615558475255966\n",
            "epoch 6 step 759 loss 0.06212899088859558\n",
            "epoch 6 step 760 loss 0.040841370820999146\n",
            "epoch 6 step 761 loss 0.01172136701643467\n",
            "epoch 6 step 762 loss 0.04113089665770531\n",
            "epoch 6 step 763 loss 0.06766348332166672\n",
            "epoch 6 step 764 loss 0.034986745566129684\n",
            "epoch 6 step 765 loss 0.10156439244747162\n",
            "epoch 6 step 766 loss 0.03858177363872528\n",
            "epoch 6 step 767 loss 0.061041831970214844\n",
            "epoch 6 step 768 loss 0.04329099878668785\n",
            "epoch 6 step 769 loss 0.021046187728643417\n",
            "epoch 6 step 770 loss 0.10573452711105347\n",
            "epoch 6 step 771 loss 0.03696615248918533\n",
            "epoch 6 step 772 loss 0.058274395763874054\n",
            "epoch 6 step 773 loss 0.11673226207494736\n",
            "epoch 6 step 774 loss 0.05455084517598152\n",
            "epoch 6 step 775 loss 0.06903993338346481\n",
            "epoch 6 step 776 loss 0.05618296191096306\n",
            "epoch 6 step 777 loss 0.11373090744018555\n",
            "epoch 6 step 778 loss 0.03451070934534073\n",
            "epoch 6 step 779 loss 0.04623717814683914\n",
            "epoch 6 step 780 loss 0.06327403336763382\n",
            "epoch 6 step 781 loss 0.029708821326494217\n",
            "epoch 7 step 0 loss 0.062428299337625504\n",
            "epoch 7 step 1 loss 0.06975223124027252\n",
            "epoch 7 step 2 loss 0.047517016530036926\n",
            "epoch 7 step 3 loss 0.04924406111240387\n",
            "epoch 7 step 4 loss 0.02547217346727848\n",
            "epoch 7 step 5 loss 0.08259917795658112\n",
            "epoch 7 step 6 loss 0.08925654739141464\n",
            "epoch 7 step 7 loss 0.11173273622989655\n",
            "epoch 7 step 8 loss 0.09047490358352661\n",
            "epoch 7 step 9 loss 0.03123375214636326\n",
            "epoch 7 step 10 loss 0.0488789901137352\n",
            "epoch 7 step 11 loss 0.08868564665317535\n",
            "epoch 7 step 12 loss 0.05521303415298462\n",
            "epoch 7 step 13 loss 0.06154337525367737\n",
            "epoch 7 step 14 loss 0.09951943904161453\n",
            "epoch 7 step 15 loss 0.026072673499584198\n",
            "epoch 7 step 16 loss 0.06275302916765213\n",
            "epoch 7 step 17 loss 0.0533333495259285\n",
            "epoch 7 step 18 loss 0.01862310990691185\n",
            "epoch 7 step 19 loss 0.04017573595046997\n",
            "epoch 7 step 20 loss 0.06886689364910126\n",
            "epoch 7 step 21 loss 0.001255704089999199\n",
            "epoch 7 step 22 loss 0.10458064079284668\n",
            "epoch 7 step 23 loss 0.07721380889415741\n",
            "epoch 7 step 24 loss 0.004642957355827093\n",
            "epoch 7 step 25 loss 0.0222960002720356\n",
            "epoch 7 step 26 loss 0.05138232186436653\n",
            "epoch 7 step 27 loss 0.023991122841835022\n",
            "epoch 7 step 28 loss 0.009981801733374596\n",
            "epoch 7 step 29 loss 0.1071326732635498\n",
            "epoch 7 step 30 loss 0.10173682868480682\n",
            "epoch 7 step 31 loss 0.06701824069023132\n",
            "epoch 7 step 32 loss 0.05266663432121277\n",
            "epoch 7 step 33 loss 0.07717899233102798\n",
            "epoch 7 step 34 loss 0.04798838496208191\n",
            "epoch 7 step 35 loss 0.06399643421173096\n",
            "epoch 7 step 36 loss 0.11628934741020203\n",
            "epoch 7 step 37 loss 0.08602292835712433\n",
            "epoch 7 step 38 loss 0.060498908162117004\n",
            "epoch 7 step 39 loss 0.041641782969236374\n",
            "epoch 7 step 40 loss 0.024036655202507973\n",
            "epoch 7 step 41 loss 0.013931630179286003\n",
            "epoch 7 step 42 loss 0.032955288887023926\n",
            "epoch 7 step 43 loss 0.04927298426628113\n",
            "epoch 7 step 44 loss 0.06742089241743088\n",
            "epoch 7 step 45 loss 0.08438597619533539\n",
            "epoch 7 step 46 loss 0.05996708944439888\n",
            "epoch 7 step 47 loss 0.06912381947040558\n",
            "epoch 7 step 48 loss 0.049683861434459686\n",
            "epoch 7 step 49 loss 0.029738351702690125\n",
            "epoch 7 step 50 loss 0.07473783195018768\n",
            "epoch 7 step 51 loss 0.03992456942796707\n",
            "epoch 7 step 52 loss 0.03686663508415222\n",
            "epoch 7 step 53 loss 0.0791805237531662\n",
            "epoch 7 step 54 loss 0.05883060395717621\n",
            "epoch 7 step 55 loss 0.08734388649463654\n",
            "epoch 7 step 56 loss 0.06263628602027893\n",
            "epoch 7 step 57 loss 0.014829885214567184\n",
            "epoch 7 step 58 loss 0.07098987698554993\n",
            "epoch 7 step 59 loss 0.02789425104856491\n",
            "epoch 7 step 60 loss 0.04727090895175934\n",
            "epoch 7 step 61 loss 0.031706035137176514\n",
            "epoch 7 step 62 loss 0.03521452844142914\n",
            "epoch 7 step 63 loss 0.02011105790734291\n",
            "epoch 7 step 64 loss 0.0569658987224102\n",
            "epoch 7 step 65 loss 0.05739102512598038\n",
            "epoch 7 step 66 loss 0.04212527722120285\n",
            "epoch 7 step 67 loss 0.048626430332660675\n",
            "epoch 7 step 68 loss 0.050660640001297\n",
            "epoch 7 step 69 loss 0.07494498789310455\n",
            "epoch 7 step 70 loss 0.033625151962041855\n",
            "epoch 7 step 71 loss 0.03222044184803963\n",
            "epoch 7 step 72 loss 0.12896771728992462\n",
            "epoch 7 step 73 loss 0.08667078614234924\n",
            "epoch 7 step 74 loss 0.0493973009288311\n",
            "epoch 7 step 75 loss 0.04894424229860306\n",
            "epoch 7 step 76 loss 0.04293779656291008\n",
            "epoch 7 step 77 loss 0.014474731869995594\n",
            "epoch 7 step 78 loss 0.04584696143865585\n",
            "epoch 7 step 79 loss 0.035668447613716125\n",
            "epoch 7 step 80 loss 0.06256880611181259\n",
            "epoch 7 step 81 loss 0.05942270904779434\n",
            "epoch 7 step 82 loss 0.02288060262799263\n",
            "epoch 7 step 83 loss 0.042604170739650726\n",
            "epoch 7 step 84 loss 0.046084314584732056\n",
            "epoch 7 step 85 loss 0.05913865566253662\n",
            "epoch 7 step 86 loss 0.03315620869398117\n",
            "epoch 7 step 87 loss 0.07096385210752487\n",
            "epoch 7 step 88 loss 0.03868534043431282\n",
            "epoch 7 step 89 loss 0.0608445405960083\n",
            "epoch 7 step 90 loss 0.08875001221895218\n",
            "epoch 7 step 91 loss 0.0520261786878109\n",
            "epoch 7 step 92 loss 0.09510818123817444\n",
            "epoch 7 step 93 loss 0.07857871055603027\n",
            "epoch 7 step 94 loss 0.05412682518362999\n",
            "epoch 7 step 95 loss 0.047002892941236496\n",
            "epoch 7 step 96 loss 0.03391816467046738\n",
            "epoch 7 step 97 loss 0.04570769891142845\n",
            "epoch 7 step 98 loss 0.04540207237005234\n",
            "epoch 7 step 99 loss 0.02900911495089531\n",
            "epoch 7 step 100 loss 0.06606075912714005\n",
            "epoch 7 step 101 loss 0.0374700129032135\n",
            "epoch 7 step 102 loss 0.14231324195861816\n",
            "epoch 7 step 103 loss 0.04088184982538223\n",
            "epoch 7 step 104 loss 0.056079670786857605\n",
            "epoch 7 step 105 loss 0.058478619903326035\n",
            "epoch 7 step 106 loss 0.013741842471063137\n",
            "epoch 7 step 107 loss 0.03400968015193939\n",
            "epoch 7 step 108 loss 0.08781907707452774\n",
            "epoch 7 step 109 loss 0.04981286823749542\n",
            "epoch 7 step 110 loss 0.055240094661712646\n",
            "epoch 7 step 111 loss 0.045281581580638885\n",
            "epoch 7 step 112 loss 0.04380687326192856\n",
            "epoch 7 step 113 loss 0.03401898592710495\n",
            "epoch 7 step 114 loss 0.06272269785404205\n",
            "epoch 7 step 115 loss 0.05992511287331581\n",
            "epoch 7 step 116 loss 0.04036816954612732\n",
            "epoch 7 step 117 loss 0.08276362717151642\n",
            "epoch 7 step 118 loss 0.024445297196507454\n",
            "epoch 7 step 119 loss 0.032901935279369354\n",
            "epoch 7 step 120 loss 0.02885620854794979\n",
            "epoch 7 step 121 loss 0.019423503428697586\n",
            "epoch 7 step 122 loss 0.02234271727502346\n",
            "epoch 7 step 123 loss 0.025575492531061172\n",
            "epoch 7 step 124 loss 0.05927467718720436\n",
            "epoch 7 step 125 loss 0.03828860819339752\n",
            "epoch 7 step 126 loss 0.037013836205005646\n",
            "epoch 7 step 127 loss 0.05594750493764877\n",
            "epoch 7 step 128 loss 0.02160489745438099\n",
            "epoch 7 step 129 loss 0.054228127002716064\n",
            "epoch 7 step 130 loss 0.05787782371044159\n",
            "epoch 7 step 131 loss 0.06765584647655487\n",
            "epoch 7 step 132 loss 0.06789732724428177\n",
            "epoch 7 step 133 loss 0.04675862565636635\n",
            "epoch 7 step 134 loss 0.061986297369003296\n",
            "epoch 7 step 135 loss 0.06295670568943024\n",
            "epoch 7 step 136 loss 0.027931125834584236\n",
            "epoch 7 step 137 loss 0.05152663588523865\n",
            "epoch 7 step 138 loss 0.02531355433166027\n",
            "epoch 7 step 139 loss 0.019101260229945183\n",
            "epoch 7 step 140 loss 0.010065801441669464\n",
            "epoch 7 step 141 loss 0.04126254469156265\n",
            "epoch 7 step 142 loss 0.033618491142988205\n",
            "epoch 7 step 143 loss 0.026225954294204712\n",
            "epoch 7 step 144 loss 0.027526987716555595\n",
            "epoch 7 step 145 loss 0.03054685704410076\n",
            "epoch 7 step 146 loss 0.014259368181228638\n",
            "epoch 7 step 147 loss 0.06971700489521027\n",
            "epoch 7 step 148 loss 0.04497993364930153\n",
            "epoch 7 step 149 loss 0.03148121014237404\n",
            "epoch 7 step 150 loss 0.0974060595035553\n",
            "epoch 7 step 151 loss 0.011750418692827225\n",
            "epoch 7 step 152 loss 0.02585121989250183\n",
            "epoch 7 step 153 loss 0.019179899245500565\n",
            "epoch 7 step 154 loss 0.026233922690153122\n",
            "epoch 7 step 155 loss 0.019293639808893204\n",
            "epoch 7 step 156 loss 0.05572841688990593\n",
            "epoch 7 step 157 loss 0.057030968368053436\n",
            "epoch 7 step 158 loss 0.056983865797519684\n",
            "epoch 7 step 159 loss 0.052442457526922226\n",
            "epoch 7 step 160 loss 0.029214588925242424\n",
            "epoch 7 step 161 loss 0.09478479623794556\n",
            "epoch 7 step 162 loss 0.011890113353729248\n",
            "epoch 7 step 163 loss 0.06527482718229294\n",
            "epoch 7 step 164 loss 0.08101960271596909\n",
            "epoch 7 step 165 loss 0.045294176787137985\n",
            "epoch 7 step 166 loss 0.005935852415859699\n",
            "epoch 7 step 167 loss 0.03763571381568909\n",
            "epoch 7 step 168 loss 0.06389690935611725\n",
            "epoch 7 step 169 loss 0.06467900425195694\n",
            "epoch 7 step 170 loss 0.03603215515613556\n",
            "epoch 7 step 171 loss 0.06101521849632263\n",
            "epoch 7 step 172 loss 0.03141040354967117\n",
            "epoch 7 step 173 loss 0.07167026400566101\n",
            "epoch 7 step 174 loss 0.022930435836315155\n",
            "epoch 7 step 175 loss 0.04008597880601883\n",
            "epoch 7 step 176 loss 0.04015025496482849\n",
            "epoch 7 step 177 loss 0.078822061419487\n",
            "epoch 7 step 178 loss 0.04803941398859024\n",
            "epoch 7 step 179 loss 0.08636889606714249\n",
            "epoch 7 step 180 loss 0.026331046596169472\n",
            "epoch 7 step 181 loss 0.07982300221920013\n",
            "epoch 7 step 182 loss 0.0459480807185173\n",
            "epoch 7 step 183 loss 0.07364238053560257\n",
            "epoch 7 step 184 loss 0.030074290931224823\n",
            "epoch 7 step 185 loss 0.052763570100069046\n",
            "epoch 7 step 186 loss 0.11028357595205307\n",
            "epoch 7 step 187 loss 0.08555164933204651\n",
            "epoch 7 step 188 loss 0.026217591017484665\n",
            "epoch 7 step 189 loss 0.05852755159139633\n",
            "epoch 7 step 190 loss 0.08077434450387955\n",
            "epoch 7 step 191 loss 0.030334748327732086\n",
            "epoch 7 step 192 loss 0.05141840875148773\n",
            "epoch 7 step 193 loss 0.03552057594060898\n",
            "epoch 7 step 194 loss 0.02105439081788063\n",
            "epoch 7 step 195 loss 0.057069867849349976\n",
            "epoch 7 step 196 loss 0.051319267600774765\n",
            "epoch 7 step 197 loss 0.07300245761871338\n",
            "epoch 7 step 198 loss 0.08673427999019623\n",
            "epoch 7 step 199 loss 0.04725979268550873\n",
            "epoch 7 step 200 loss 0.019960690289735794\n",
            "epoch 7 step 201 loss 0.01404990442097187\n",
            "epoch 7 step 202 loss 0.08546466380357742\n",
            "epoch 7 step 203 loss 0.13054265081882477\n",
            "epoch 7 step 204 loss 0.03885163366794586\n",
            "epoch 7 step 205 loss 0.04290241375565529\n",
            "epoch 7 step 206 loss 0.03880072757601738\n",
            "epoch 7 step 207 loss 0.06264098733663559\n",
            "epoch 7 step 208 loss 0.05297034978866577\n",
            "epoch 7 step 209 loss 0.029474057257175446\n",
            "epoch 7 step 210 loss 0.052348263561725616\n",
            "epoch 7 step 211 loss 0.08895017951726913\n",
            "epoch 7 step 212 loss 0.05471135675907135\n",
            "epoch 7 step 213 loss 0.04462142288684845\n",
            "epoch 7 step 214 loss 0.05058794468641281\n",
            "epoch 7 step 215 loss 0.05863448232412338\n",
            "epoch 7 step 216 loss 0.046078771352767944\n",
            "epoch 7 step 217 loss 0.054282352328300476\n",
            "epoch 7 step 218 loss 0.07524842768907547\n",
            "epoch 7 step 219 loss 0.060813117772340775\n",
            "epoch 7 step 220 loss 0.07869690656661987\n",
            "epoch 7 step 221 loss 0.04222723841667175\n",
            "epoch 7 step 222 loss 0.04180842638015747\n",
            "epoch 7 step 223 loss 0.06713332235813141\n",
            "epoch 7 step 224 loss 0.03304484486579895\n",
            "epoch 7 step 225 loss 0.03830111771821976\n",
            "epoch 7 step 226 loss 0.028863580897450447\n",
            "epoch 7 step 227 loss 0.017371410503983498\n",
            "epoch 7 step 228 loss 0.09051914513111115\n",
            "epoch 7 step 229 loss 0.06209202855825424\n",
            "epoch 7 step 230 loss 0.023117896169424057\n",
            "epoch 7 step 231 loss 0.06124494597315788\n",
            "epoch 7 step 232 loss 0.024905383586883545\n",
            "epoch 7 step 233 loss 0.08236769586801529\n",
            "epoch 7 step 234 loss 0.049073077738285065\n",
            "epoch 7 step 235 loss 0.03607753664255142\n",
            "epoch 7 step 236 loss 0.033724501729011536\n",
            "epoch 7 step 237 loss 0.048994891345500946\n",
            "epoch 7 step 238 loss 0.0298919640481472\n",
            "epoch 7 step 239 loss 0.018705926835536957\n",
            "epoch 7 step 240 loss 0.037789396941661835\n",
            "epoch 7 step 241 loss 0.02480638399720192\n",
            "epoch 7 step 242 loss 0.08598863333463669\n",
            "epoch 7 step 243 loss 0.052247174084186554\n",
            "epoch 7 step 244 loss 0.06066396087408066\n",
            "epoch 7 step 245 loss 0.024187250062823296\n",
            "epoch 7 step 246 loss 0.01754637621343136\n",
            "epoch 7 step 247 loss 0.050472818315029144\n",
            "epoch 7 step 248 loss 0.05095934122800827\n",
            "epoch 7 step 249 loss 0.037435293197631836\n",
            "epoch 7 step 250 loss 0.05396505445241928\n",
            "epoch 7 step 251 loss 0.03703176975250244\n",
            "epoch 7 step 252 loss 0.13268937170505524\n",
            "epoch 7 step 253 loss 0.08259139955043793\n",
            "epoch 7 step 254 loss 0.06283672153949738\n",
            "epoch 7 step 255 loss 0.08733217418193817\n",
            "epoch 7 step 256 loss 0.04590022563934326\n",
            "epoch 7 step 257 loss 0.04226180166006088\n",
            "epoch 7 step 258 loss 0.0500762052834034\n",
            "epoch 7 step 259 loss 0.05019386112689972\n",
            "epoch 7 step 260 loss 0.046555452048778534\n",
            "epoch 7 step 261 loss 0.06602611392736435\n",
            "epoch 7 step 262 loss 0.07674282789230347\n",
            "epoch 7 step 263 loss 0.06660457700490952\n",
            "epoch 7 step 264 loss 0.03328794613480568\n",
            "epoch 7 step 265 loss 0.05661603435873985\n",
            "epoch 7 step 266 loss 0.06471720337867737\n",
            "epoch 7 step 267 loss 0.028741534799337387\n",
            "epoch 7 step 268 loss 0.052807554602622986\n",
            "epoch 7 step 269 loss 0.08700290322303772\n",
            "epoch 7 step 270 loss 0.04488098621368408\n",
            "epoch 7 step 271 loss 0.03943183273077011\n",
            "epoch 7 step 272 loss 0.04838891699910164\n",
            "epoch 7 step 273 loss 0.058530185371637344\n",
            "epoch 7 step 274 loss 0.08666128665208817\n",
            "epoch 7 step 275 loss 0.0742705762386322\n",
            "epoch 7 step 276 loss 0.04871802031993866\n",
            "epoch 7 step 277 loss 0.06593485176563263\n",
            "epoch 7 step 278 loss 0.06906858086585999\n",
            "epoch 7 step 279 loss 0.06362675130367279\n",
            "epoch 7 step 280 loss 0.057683125138282776\n",
            "epoch 7 step 281 loss 0.05806782841682434\n",
            "epoch 7 step 282 loss 0.05039362609386444\n",
            "epoch 7 step 283 loss 0.0732082724571228\n",
            "epoch 7 step 284 loss 0.04944362863898277\n",
            "epoch 7 step 285 loss 0.06821417808532715\n",
            "epoch 7 step 286 loss 0.06546132266521454\n",
            "epoch 7 step 287 loss 0.03963988274335861\n",
            "epoch 7 step 288 loss 0.0449189767241478\n",
            "epoch 7 step 289 loss 0.05589930713176727\n",
            "epoch 7 step 290 loss 0.045744866132736206\n",
            "epoch 7 step 291 loss 0.060749977827072144\n",
            "epoch 7 step 292 loss 0.07910718023777008\n",
            "epoch 7 step 293 loss 0.03715124726295471\n",
            "epoch 7 step 294 loss 0.04270574450492859\n",
            "epoch 7 step 295 loss 0.05508963763713837\n",
            "epoch 7 step 296 loss 0.06701784580945969\n",
            "epoch 7 step 297 loss 0.06916826963424683\n",
            "epoch 7 step 298 loss 0.0668269693851471\n",
            "epoch 7 step 299 loss 0.06538822501897812\n",
            "epoch 7 step 300 loss 0.07955420762300491\n",
            "epoch 7 step 301 loss 0.08680123090744019\n",
            "epoch 7 step 302 loss 0.09022758901119232\n",
            "epoch 7 step 303 loss 0.05573059991002083\n",
            "epoch 7 step 304 loss 0.04535479098558426\n",
            "epoch 7 step 305 loss 0.03673931956291199\n",
            "epoch 7 step 306 loss 0.04380572587251663\n",
            "epoch 7 step 307 loss 0.03982659429311752\n",
            "epoch 7 step 308 loss 0.0292951762676239\n",
            "epoch 7 step 309 loss 0.033924397081136703\n",
            "epoch 7 step 310 loss 0.07605701684951782\n",
            "epoch 7 step 311 loss 0.08033384382724762\n",
            "epoch 7 step 312 loss 0.042586907744407654\n",
            "epoch 7 step 313 loss 0.06246154382824898\n",
            "epoch 7 step 314 loss 0.09145751595497131\n",
            "epoch 7 step 315 loss 0.04095103591680527\n",
            "epoch 7 step 316 loss 0.04805808514356613\n",
            "epoch 7 step 317 loss 0.042872194200754166\n",
            "epoch 7 step 318 loss 0.04873310774564743\n",
            "epoch 7 step 319 loss 0.14835965633392334\n",
            "epoch 7 step 320 loss 0.055090561509132385\n",
            "epoch 7 step 321 loss 0.05377478152513504\n",
            "epoch 7 step 322 loss 0.0368616059422493\n",
            "epoch 7 step 323 loss 0.04698588326573372\n",
            "epoch 7 step 324 loss 0.044583022594451904\n",
            "epoch 7 step 325 loss 0.05977870523929596\n",
            "epoch 7 step 326 loss 0.07413831353187561\n",
            "epoch 7 step 327 loss 0.045371849089860916\n",
            "epoch 7 step 328 loss 0.07624715566635132\n",
            "epoch 7 step 329 loss 0.04909095540642738\n",
            "epoch 7 step 330 loss 0.05851716920733452\n",
            "epoch 7 step 331 loss 0.041370831429958344\n",
            "epoch 7 step 332 loss 0.03137986734509468\n",
            "epoch 7 step 333 loss 0.041802965104579926\n",
            "epoch 7 step 334 loss 0.06516522169113159\n",
            "epoch 7 step 335 loss 0.04357285052537918\n",
            "epoch 7 step 336 loss 0.06063930317759514\n",
            "epoch 7 step 337 loss 0.05219314247369766\n",
            "epoch 7 step 338 loss 0.045099012553691864\n",
            "epoch 7 step 339 loss 0.03608718514442444\n",
            "epoch 7 step 340 loss 0.042482130229473114\n",
            "epoch 7 step 341 loss 0.05641375482082367\n",
            "epoch 7 step 342 loss 0.06388790905475616\n",
            "epoch 7 step 343 loss 0.07840467244386673\n",
            "epoch 7 step 344 loss 0.048723503947257996\n",
            "epoch 7 step 345 loss 0.06878543645143509\n",
            "epoch 7 step 346 loss 0.02892260253429413\n",
            "epoch 7 step 347 loss 0.06173190474510193\n",
            "epoch 7 step 348 loss 0.040517378598451614\n",
            "epoch 7 step 349 loss 0.048892512917518616\n",
            "epoch 7 step 350 loss 0.06085516884922981\n",
            "epoch 7 step 351 loss 0.0463787317276001\n",
            "epoch 7 step 352 loss 0.05558369308710098\n",
            "epoch 7 step 353 loss 0.04906377196311951\n",
            "epoch 7 step 354 loss 0.0533205009996891\n",
            "epoch 7 step 355 loss 0.043412867933511734\n",
            "epoch 7 step 356 loss 0.03664441406726837\n",
            "epoch 7 step 357 loss 0.03843164071440697\n",
            "epoch 7 step 358 loss 0.026066765189170837\n",
            "epoch 7 step 359 loss 0.06810499727725983\n",
            "epoch 7 step 360 loss 0.05737004056572914\n",
            "epoch 7 step 361 loss 0.04272730275988579\n",
            "epoch 7 step 362 loss 0.03547573834657669\n",
            "epoch 7 step 363 loss 0.06899315863847733\n",
            "epoch 7 step 364 loss 0.1016061082482338\n",
            "epoch 7 step 365 loss 0.04641171544790268\n",
            "epoch 7 step 366 loss 0.06330424547195435\n",
            "epoch 7 step 367 loss 0.03767264634370804\n",
            "epoch 7 step 368 loss 0.040830545127391815\n",
            "epoch 7 step 369 loss 0.03151408210396767\n",
            "epoch 7 step 370 loss 0.03225041925907135\n",
            "epoch 7 step 371 loss 0.03910248354077339\n",
            "epoch 7 step 372 loss 0.05860019102692604\n",
            "epoch 7 step 373 loss 0.04573344066739082\n",
            "epoch 7 step 374 loss 0.07484827935695648\n",
            "epoch 7 step 375 loss 0.047069475054740906\n",
            "epoch 7 step 376 loss 0.05381610989570618\n",
            "epoch 7 step 377 loss 0.06954432278871536\n",
            "epoch 7 step 378 loss 0.054002199321985245\n",
            "epoch 7 step 379 loss 0.03323506936430931\n",
            "epoch 7 step 380 loss 0.07022524625062943\n",
            "epoch 7 step 381 loss 0.04523158818483353\n",
            "epoch 7 step 382 loss 0.06262554228305817\n",
            "epoch 7 step 383 loss 0.029210176318883896\n",
            "epoch 7 step 384 loss 0.03145658224821091\n",
            "epoch 7 step 385 loss 0.03148402273654938\n",
            "epoch 7 step 386 loss 0.03495572879910469\n",
            "epoch 7 step 387 loss 0.17535555362701416\n",
            "epoch 7 step 388 loss 0.07027333974838257\n",
            "epoch 7 step 389 loss 0.042691685259342194\n",
            "epoch 7 step 390 loss 0.052531879395246506\n",
            "epoch 7 step 391 loss 0.03877587988972664\n",
            "epoch 7 step 392 loss 0.038148850202560425\n",
            "epoch 7 step 393 loss 0.06799449026584625\n",
            "epoch 7 step 394 loss 0.05356898158788681\n",
            "epoch 7 step 395 loss 0.07976144552230835\n",
            "epoch 7 step 396 loss 0.07261139154434204\n",
            "epoch 7 step 397 loss 0.10508009791374207\n",
            "epoch 7 step 398 loss 0.05802426487207413\n",
            "epoch 7 step 399 loss 0.09083506464958191\n",
            "epoch 7 step 400 loss 0.040165554732084274\n",
            "epoch 7 step 401 loss 0.03782953321933746\n",
            "epoch 7 step 402 loss 0.04607050120830536\n",
            "epoch 7 step 403 loss 0.06279687583446503\n",
            "epoch 7 step 404 loss 0.05714505910873413\n",
            "epoch 7 step 405 loss 0.06718870252370834\n",
            "epoch 7 step 406 loss 0.06668803095817566\n",
            "epoch 7 step 407 loss 0.07600168883800507\n",
            "epoch 7 step 408 loss 0.052701033651828766\n",
            "epoch 7 step 409 loss 0.036104388535022736\n",
            "epoch 7 step 410 loss 0.029037801548838615\n",
            "epoch 7 step 411 loss 0.019627021625638008\n",
            "epoch 7 step 412 loss 0.01899915188550949\n",
            "epoch 7 step 413 loss 0.04362174868583679\n",
            "epoch 7 step 414 loss 0.03943991661071777\n",
            "epoch 7 step 415 loss 0.040543507784605026\n",
            "epoch 7 step 416 loss 0.05987435579299927\n",
            "epoch 7 step 417 loss 0.04292398691177368\n",
            "epoch 7 step 418 loss 0.025390423834323883\n",
            "epoch 7 step 419 loss 0.03788718581199646\n",
            "epoch 7 step 420 loss 0.05526430904865265\n",
            "epoch 7 step 421 loss 0.033412519842386246\n",
            "epoch 7 step 422 loss 0.030383631587028503\n",
            "epoch 7 step 423 loss 0.07076643407344818\n",
            "epoch 7 step 424 loss 0.0675642192363739\n",
            "epoch 7 step 425 loss 0.05881443992257118\n",
            "epoch 7 step 426 loss 0.0337371826171875\n",
            "epoch 7 step 427 loss 0.030598308891057968\n",
            "epoch 7 step 428 loss 0.05979786068201065\n",
            "epoch 7 step 429 loss 0.048661909997463226\n",
            "epoch 7 step 430 loss 0.06443551927804947\n",
            "epoch 7 step 431 loss 0.03594345971941948\n",
            "epoch 7 step 432 loss 0.05394639074802399\n",
            "epoch 7 step 433 loss 0.048070698976516724\n",
            "epoch 7 step 434 loss 0.05947662517428398\n",
            "epoch 7 step 435 loss 0.03653108328580856\n",
            "epoch 7 step 436 loss 0.03405795246362686\n",
            "epoch 7 step 437 loss 0.08528125286102295\n",
            "epoch 7 step 438 loss 0.06591032445430756\n",
            "epoch 7 step 439 loss 0.04889257252216339\n",
            "epoch 7 step 440 loss 0.04597365856170654\n",
            "epoch 7 step 441 loss 0.06542564183473587\n",
            "epoch 7 step 442 loss 0.13130733370780945\n",
            "epoch 7 step 443 loss 0.07955484092235565\n",
            "epoch 7 step 444 loss 0.044333212077617645\n",
            "epoch 7 step 445 loss 0.06173405796289444\n",
            "epoch 7 step 446 loss 0.047255564481019974\n",
            "epoch 7 step 447 loss 0.09181801229715347\n",
            "epoch 7 step 448 loss 0.06377141177654266\n",
            "epoch 7 step 449 loss 0.10865259915590286\n",
            "epoch 7 step 450 loss 0.0967496782541275\n",
            "epoch 7 step 451 loss 0.04189467802643776\n",
            "epoch 7 step 452 loss 0.0935305580496788\n",
            "epoch 7 step 453 loss 0.06531721353530884\n",
            "epoch 7 step 454 loss 0.09917204082012177\n",
            "epoch 7 step 455 loss 0.04406087100505829\n",
            "epoch 7 step 456 loss 0.0625925362110138\n",
            "epoch 7 step 457 loss 0.05850665271282196\n",
            "epoch 7 step 458 loss 0.04890250042080879\n",
            "epoch 7 step 459 loss 0.053487326949834824\n",
            "epoch 7 step 460 loss 0.05924049764871597\n",
            "epoch 7 step 461 loss 0.0872422307729721\n",
            "epoch 7 step 462 loss 0.09210368990898132\n",
            "epoch 7 step 463 loss 0.036806847900152206\n",
            "epoch 7 step 464 loss 0.07498058676719666\n",
            "epoch 7 step 465 loss 0.022859932854771614\n",
            "epoch 7 step 466 loss 0.07169479131698608\n",
            "epoch 7 step 467 loss 0.18401725590229034\n",
            "epoch 7 step 468 loss 0.06530064344406128\n",
            "epoch 7 step 469 loss 0.10285436362028122\n",
            "epoch 7 step 470 loss 0.01667388342320919\n",
            "epoch 7 step 471 loss 0.04422999173402786\n",
            "epoch 7 step 472 loss 0.08346010744571686\n",
            "epoch 7 step 473 loss 0.02686995640397072\n",
            "epoch 7 step 474 loss 0.08386307954788208\n",
            "epoch 7 step 475 loss 0.09884364902973175\n",
            "epoch 7 step 476 loss 0.0695645660161972\n",
            "epoch 7 step 477 loss 0.04490917921066284\n",
            "epoch 7 step 478 loss 0.05238261818885803\n",
            "epoch 7 step 479 loss 0.07308928668498993\n",
            "epoch 7 step 480 loss 0.09499503672122955\n",
            "epoch 7 step 481 loss 0.0693800300359726\n",
            "epoch 7 step 482 loss 0.07308951020240784\n",
            "epoch 7 step 483 loss 0.05416509881615639\n",
            "epoch 7 step 484 loss 0.04607212916016579\n",
            "epoch 7 step 485 loss 0.1364530324935913\n",
            "epoch 7 step 486 loss 0.09529043734073639\n",
            "epoch 7 step 487 loss 0.09753327071666718\n",
            "epoch 7 step 488 loss 0.052970778197050095\n",
            "epoch 7 step 489 loss 0.062051936984062195\n",
            "epoch 7 step 490 loss 0.07191651314496994\n",
            "epoch 7 step 491 loss 0.048610687255859375\n",
            "epoch 7 step 492 loss 0.04314647242426872\n",
            "epoch 7 step 493 loss 0.03138451650738716\n",
            "epoch 7 step 494 loss 0.043519750237464905\n",
            "epoch 7 step 495 loss 0.057889316231012344\n",
            "epoch 7 step 496 loss 0.08357291668653488\n",
            "epoch 7 step 497 loss 0.04101186245679855\n",
            "epoch 7 step 498 loss 0.06283372640609741\n",
            "epoch 7 step 499 loss 0.12170902639627457\n",
            "epoch 7 step 500 loss 0.09645994007587433\n",
            "epoch 7 step 501 loss 0.026517067104578018\n",
            "epoch 7 step 502 loss 0.06257189065217972\n",
            "epoch 7 step 503 loss 0.03875390440225601\n",
            "epoch 7 step 504 loss 0.05366663634777069\n",
            "epoch 7 step 505 loss 0.13355803489685059\n",
            "epoch 7 step 506 loss 0.05000193417072296\n",
            "epoch 7 step 507 loss 0.07093258202075958\n",
            "epoch 7 step 508 loss 0.028233304619789124\n",
            "epoch 7 step 509 loss 0.06550168246030807\n",
            "epoch 7 step 510 loss 0.07747898250818253\n",
            "epoch 7 step 511 loss 0.021804386749863625\n",
            "epoch 7 step 512 loss 0.07737832516431808\n",
            "epoch 7 step 513 loss 0.09880469739437103\n",
            "epoch 7 step 514 loss 0.03359120339155197\n",
            "epoch 7 step 515 loss 0.03216186538338661\n",
            "epoch 7 step 516 loss 0.031119773164391518\n",
            "epoch 7 step 517 loss 0.034218624234199524\n",
            "epoch 7 step 518 loss 0.020078731700778008\n",
            "epoch 7 step 519 loss 0.04567524790763855\n",
            "epoch 7 step 520 loss 0.03194249048829079\n",
            "epoch 7 step 521 loss 0.03996970131993294\n",
            "epoch 7 step 522 loss 0.04681166261434555\n",
            "epoch 7 step 523 loss 0.030834542587399483\n",
            "epoch 7 step 524 loss 0.07078029960393906\n",
            "epoch 7 step 525 loss 0.08219169080257416\n",
            "epoch 7 step 526 loss 0.05355074629187584\n",
            "epoch 7 step 527 loss 0.06431092321872711\n",
            "epoch 7 step 528 loss 0.037015415728092194\n",
            "epoch 7 step 529 loss 0.024565763771533966\n",
            "epoch 7 step 530 loss 0.06791435182094574\n",
            "epoch 7 step 531 loss 0.022950589656829834\n",
            "epoch 7 step 532 loss 0.03220445290207863\n",
            "epoch 7 step 533 loss 0.08249267190694809\n",
            "epoch 7 step 534 loss 0.09451861679553986\n",
            "epoch 7 step 535 loss 0.08655528724193573\n",
            "epoch 7 step 536 loss 0.05242137610912323\n",
            "epoch 7 step 537 loss 0.04582381248474121\n",
            "epoch 7 step 538 loss 0.0214700810611248\n",
            "epoch 7 step 539 loss 0.026223307475447655\n",
            "epoch 7 step 540 loss 0.05035921186208725\n",
            "epoch 7 step 541 loss 0.07376585155725479\n",
            "epoch 7 step 542 loss 0.025066345930099487\n",
            "epoch 7 step 543 loss 0.057552896440029144\n",
            "epoch 7 step 544 loss 0.010228930972516537\n",
            "epoch 7 step 545 loss 0.050069354474544525\n",
            "epoch 7 step 546 loss 0.0784783735871315\n",
            "epoch 7 step 547 loss 0.007788109127432108\n",
            "epoch 7 step 548 loss 0.031118493527173996\n",
            "epoch 7 step 549 loss 0.06108681485056877\n",
            "epoch 7 step 550 loss 0.04087144881486893\n",
            "epoch 7 step 551 loss 0.04776030033826828\n",
            "epoch 7 step 552 loss 0.09693308174610138\n",
            "epoch 7 step 553 loss 0.03736305981874466\n",
            "epoch 7 step 554 loss 0.07134722918272018\n",
            "epoch 7 step 555 loss 0.052103813737630844\n",
            "epoch 7 step 556 loss 0.016700085252523422\n",
            "epoch 7 step 557 loss 0.0494418665766716\n",
            "epoch 7 step 558 loss 0.05171428620815277\n",
            "epoch 7 step 559 loss 0.04890064895153046\n",
            "epoch 7 step 560 loss 0.05190432071685791\n",
            "epoch 7 step 561 loss 0.04599401727318764\n",
            "epoch 7 step 562 loss 0.04877164587378502\n",
            "epoch 7 step 563 loss 0.04325304180383682\n",
            "epoch 7 step 564 loss 0.03925013542175293\n",
            "epoch 7 step 565 loss 0.04283291473984718\n",
            "epoch 7 step 566 loss 0.07159087806940079\n",
            "epoch 7 step 567 loss 0.03947027772665024\n",
            "epoch 7 step 568 loss 0.058580171316862106\n",
            "epoch 7 step 569 loss 0.06797857582569122\n",
            "epoch 7 step 570 loss 0.05048982799053192\n",
            "epoch 7 step 571 loss 0.029960986226797104\n",
            "epoch 7 step 572 loss 0.040162503719329834\n",
            "epoch 7 step 573 loss 0.05638203024864197\n",
            "epoch 7 step 574 loss 0.03676538169384003\n",
            "epoch 7 step 575 loss 0.056718237698078156\n",
            "epoch 7 step 576 loss 0.04626060277223587\n",
            "epoch 7 step 577 loss 0.04266173765063286\n",
            "epoch 7 step 578 loss 0.033845383673906326\n",
            "epoch 7 step 579 loss 0.07030332088470459\n",
            "epoch 7 step 580 loss 0.02528129704296589\n",
            "epoch 7 step 581 loss 0.04428628459572792\n",
            "epoch 7 step 582 loss 0.09390674531459808\n",
            "epoch 7 step 583 loss 0.024204038083553314\n",
            "epoch 7 step 584 loss 0.07038708031177521\n",
            "epoch 7 step 585 loss 0.10239621251821518\n",
            "epoch 7 step 586 loss 0.06363565474748611\n",
            "epoch 7 step 587 loss 0.06037968397140503\n",
            "epoch 7 step 588 loss 0.04113594442605972\n",
            "epoch 7 step 589 loss 0.06602180004119873\n",
            "epoch 7 step 590 loss 0.07005579769611359\n",
            "epoch 7 step 591 loss 0.051101528108119965\n",
            "epoch 7 step 592 loss 0.055550895631313324\n",
            "epoch 7 step 593 loss 0.06455618888139725\n",
            "epoch 7 step 594 loss 0.0665416270494461\n",
            "epoch 7 step 595 loss 0.07849159836769104\n",
            "epoch 7 step 596 loss 0.03519320487976074\n",
            "epoch 7 step 597 loss 0.05721999704837799\n",
            "epoch 7 step 598 loss 0.04030027613043785\n",
            "epoch 7 step 599 loss 0.0683215782046318\n",
            "epoch 7 step 600 loss 0.09475341439247131\n",
            "epoch 7 step 601 loss 0.053439490497112274\n",
            "epoch 7 step 602 loss 0.05950722098350525\n",
            "epoch 7 step 603 loss 0.06284158676862717\n",
            "epoch 7 step 604 loss 0.06758404523134232\n",
            "epoch 7 step 605 loss 0.04384934902191162\n",
            "epoch 7 step 606 loss 0.06740914285182953\n",
            "epoch 7 step 607 loss 0.03686334937810898\n",
            "epoch 7 step 608 loss 0.04861125349998474\n",
            "epoch 7 step 609 loss 0.05896483361721039\n",
            "epoch 7 step 610 loss 0.04886293783783913\n",
            "epoch 7 step 611 loss 0.07555188238620758\n",
            "epoch 7 step 612 loss 0.026048608124256134\n",
            "epoch 7 step 613 loss 0.009428609162569046\n",
            "epoch 7 step 614 loss 0.04596832022070885\n",
            "epoch 7 step 615 loss 0.04675940051674843\n",
            "epoch 7 step 616 loss 0.049602776765823364\n",
            "epoch 7 step 617 loss 0.060069866478443146\n",
            "epoch 7 step 618 loss 0.08326176553964615\n",
            "epoch 7 step 619 loss 0.0824020728468895\n",
            "epoch 7 step 620 loss 0.07895544171333313\n",
            "epoch 7 step 621 loss 0.037389904260635376\n",
            "epoch 7 step 622 loss 0.05003989860415459\n",
            "epoch 7 step 623 loss 0.0731378048658371\n",
            "epoch 7 step 624 loss 0.03972519189119339\n",
            "epoch 7 step 625 loss 0.04543445259332657\n",
            "epoch 7 step 626 loss 0.034397564828395844\n",
            "epoch 7 step 627 loss 0.06460290402173996\n",
            "epoch 7 step 628 loss 0.053350672125816345\n",
            "epoch 7 step 629 loss 0.06939812004566193\n",
            "epoch 7 step 630 loss 0.04286016523838043\n",
            "epoch 7 step 631 loss 0.06447772681713104\n",
            "epoch 7 step 632 loss 0.07587136328220367\n",
            "epoch 7 step 633 loss 0.04693392664194107\n",
            "epoch 7 step 634 loss 0.08996070921421051\n",
            "epoch 7 step 635 loss 0.08224049210548401\n",
            "epoch 7 step 636 loss 0.07095170766115189\n",
            "epoch 7 step 637 loss 0.03525739535689354\n",
            "epoch 7 step 638 loss 0.03167952597141266\n",
            "epoch 7 step 639 loss 0.08622485399246216\n",
            "epoch 7 step 640 loss 0.0505644828081131\n",
            "epoch 7 step 641 loss 0.12076342105865479\n",
            "epoch 7 step 642 loss 0.06838729977607727\n",
            "epoch 7 step 643 loss 0.05188007652759552\n",
            "epoch 7 step 644 loss 0.052013546228408813\n",
            "epoch 7 step 645 loss 0.05784115940332413\n",
            "epoch 7 step 646 loss 0.06942766159772873\n",
            "epoch 7 step 647 loss 0.05029342323541641\n",
            "epoch 7 step 648 loss 0.03933107852935791\n",
            "epoch 7 step 649 loss 0.05144592002034187\n",
            "epoch 7 step 650 loss 0.02595973014831543\n",
            "epoch 7 step 651 loss 0.00988001562654972\n",
            "epoch 7 step 652 loss 0.07511088997125626\n",
            "epoch 7 step 653 loss 0.04128457233309746\n",
            "epoch 7 step 654 loss 0.09058837592601776\n",
            "epoch 7 step 655 loss 0.05535799264907837\n",
            "epoch 7 step 656 loss 0.044923555105924606\n",
            "epoch 7 step 657 loss 0.02183813415467739\n",
            "epoch 7 step 658 loss 0.07426784932613373\n",
            "epoch 7 step 659 loss 0.024056341499090195\n",
            "epoch 7 step 660 loss 0.023039450868964195\n",
            "epoch 7 step 661 loss 0.06620128452777863\n",
            "epoch 7 step 662 loss 0.0699501782655716\n",
            "epoch 7 step 663 loss 0.1054699644446373\n",
            "epoch 7 step 664 loss 0.07720959931612015\n",
            "epoch 7 step 665 loss 0.05973614752292633\n",
            "epoch 7 step 666 loss 0.03433217480778694\n",
            "epoch 7 step 667 loss 0.04493938386440277\n",
            "epoch 7 step 668 loss 0.049452342092990875\n",
            "epoch 7 step 669 loss 0.032329265028238297\n",
            "epoch 7 step 670 loss 0.05134870484471321\n",
            "epoch 7 step 671 loss 0.030037613585591316\n",
            "epoch 7 step 672 loss 0.021021807566285133\n",
            "epoch 7 step 673 loss 0.03144344687461853\n",
            "epoch 7 step 674 loss 0.015466723591089249\n",
            "epoch 7 step 675 loss 0.03146485239267349\n",
            "epoch 7 step 676 loss 0.03395126760005951\n",
            "epoch 7 step 677 loss 0.04457000643014908\n",
            "epoch 7 step 678 loss 0.021445218473672867\n",
            "epoch 7 step 679 loss 0.05504653602838516\n",
            "epoch 7 step 680 loss 0.06615929305553436\n",
            "epoch 7 step 681 loss 0.028834249824285507\n",
            "epoch 7 step 682 loss 0.03872029110789299\n",
            "epoch 7 step 683 loss 0.031190648674964905\n",
            "epoch 7 step 684 loss 0.036122314631938934\n",
            "epoch 7 step 685 loss 0.023665642365813255\n",
            "epoch 7 step 686 loss 0.02758023515343666\n",
            "epoch 7 step 687 loss 0.06484536826610565\n",
            "epoch 7 step 688 loss 0.02583172172307968\n",
            "epoch 7 step 689 loss 0.03963489085435867\n",
            "epoch 7 step 690 loss 0.012440950609743595\n",
            "epoch 7 step 691 loss 0.020865051075816154\n",
            "epoch 7 step 692 loss 0.0504741296172142\n",
            "epoch 7 step 693 loss 0.061697106808423996\n",
            "epoch 7 step 694 loss 0.0795465037226677\n",
            "epoch 7 step 695 loss 0.04001837968826294\n",
            "epoch 7 step 696 loss 0.04862957447767258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a4e6d8402f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0my3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-baaacc860196>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xt, flag)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOQLMXUdM4zV",
        "outputId": "41d0ea6b-4408-442b-c0b0-4223cdc58a44"
      },
      "source": [
        "epochs = 40\n",
        "for j in range(epochs):\n",
        "   for i,(xt,yt) in enumerate(souptrain) :\n",
        "    xt = xt.to(device)\n",
        "    #xt = torch.reshape(xt,[xt.shape[0],-1,1,1])\n",
        "    yt = yt.to(device)\n",
        "    #xt = torch.squeeze(xt,dim = 2)\n",
        "    #xt = torch.squeeze(xt,dim = 2)\n",
        "    y_pred = model(xt,False)\n",
        "    y_pred = torch.squeeze(y_pred)\n",
        "    lossc = costc(y_pred,yt)          \n",
        "    optimizerc.zero_grad()\n",
        "    lossc.backward()\n",
        "    optimizerc.step()\n",
        "    if (i%20==0):\n",
        "      acc = test(soupval)\n",
        "      print(f'epoch {j+1} step {i} loss {lossc} test_accuracy {acc} train_accuracy {test(souptrain)}')\n",
        "    else:\n",
        "      print(f'epoch {j+1} step {i} loss {lossc}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1 step 0 loss 2.308993101119995 test_accuracy 10.800000190734863 train_accuracy 8.59375\n",
            "epoch 1 step 1 loss 2.305510997772217\n",
            "epoch 1 step 2 loss 2.252635955810547\n",
            "epoch 1 step 3 loss 2.3051791191101074\n",
            "epoch 1 step 4 loss 2.3354108333587646\n",
            "epoch 1 step 5 loss 2.2735636234283447\n",
            "epoch 1 step 6 loss 2.3025429248809814\n",
            "epoch 1 step 7 loss 2.2513794898986816\n",
            "epoch 1 step 8 loss 2.227808952331543\n",
            "epoch 1 step 9 loss 2.310154676437378\n",
            "epoch 1 step 10 loss 2.2273454666137695\n",
            "epoch 1 step 11 loss 2.2776827812194824\n",
            "epoch 1 step 12 loss 2.2411158084869385\n",
            "epoch 1 step 13 loss 2.2005467414855957\n",
            "epoch 1 step 14 loss 2.2553844451904297\n",
            "epoch 1 step 15 loss 2.270887613296509\n",
            "epoch 1 step 16 loss 2.252403497695923\n",
            "epoch 1 step 17 loss 2.2104296684265137\n",
            "epoch 1 step 18 loss 2.2278342247009277\n",
            "epoch 1 step 19 loss 2.2626090049743652\n",
            "epoch 1 step 20 loss 2.2274460792541504 test_accuracy 20.0 train_accuracy 20.3125\n",
            "epoch 1 step 21 loss 2.211432695388794\n",
            "epoch 1 step 22 loss 2.296591281890869\n",
            "epoch 1 step 23 loss 2.197084903717041\n",
            "epoch 1 step 24 loss 2.2311453819274902\n",
            "epoch 1 step 25 loss 2.191375255584717\n",
            "epoch 1 step 26 loss 2.255065441131592\n",
            "epoch 1 step 27 loss 2.2101094722747803\n",
            "epoch 1 step 28 loss 2.234955310821533\n",
            "epoch 1 step 29 loss 2.2161877155303955\n",
            "epoch 1 step 30 loss 2.329285144805908\n",
            "epoch 1 step 31 loss 2.24908709526062\n",
            "epoch 1 step 32 loss 2.177198648452759\n",
            "epoch 1 step 33 loss 2.201646566390991\n",
            "epoch 1 step 34 loss 2.2383768558502197\n",
            "epoch 1 step 35 loss 2.3566346168518066\n",
            "epoch 2 step 0 loss 2.2339680194854736 test_accuracy 16.400001525878906 train_accuracy 22.65625\n",
            "epoch 2 step 1 loss 2.2191555500030518\n",
            "epoch 2 step 2 loss 2.288710117340088\n",
            "epoch 2 step 3 loss 2.1933441162109375\n",
            "epoch 2 step 4 loss 2.186598062515259\n",
            "epoch 2 step 5 loss 2.217607021331787\n",
            "epoch 2 step 6 loss 2.2158496379852295\n",
            "epoch 2 step 7 loss 2.207341194152832\n",
            "epoch 2 step 8 loss 2.1906180381774902\n",
            "epoch 2 step 9 loss 2.2441787719726562\n",
            "epoch 2 step 10 loss 2.205310583114624\n",
            "epoch 2 step 11 loss 2.2138521671295166\n",
            "epoch 2 step 12 loss 2.2317545413970947\n",
            "epoch 2 step 13 loss 2.204723596572876\n",
            "epoch 2 step 14 loss 2.1953208446502686\n",
            "epoch 2 step 15 loss 2.232746124267578\n",
            "epoch 2 step 16 loss 2.2392871379852295\n",
            "epoch 2 step 17 loss 2.243600606918335\n",
            "epoch 2 step 18 loss 2.2292351722717285\n",
            "epoch 2 step 19 loss 2.2422752380371094\n",
            "epoch 2 step 20 loss 2.188084363937378 test_accuracy 18.400001525878906 train_accuracy 23.4375\n",
            "epoch 2 step 21 loss 2.2024359703063965\n",
            "epoch 2 step 22 loss 2.229219913482666\n",
            "epoch 2 step 23 loss 2.223423480987549\n",
            "epoch 2 step 24 loss 2.233177661895752\n",
            "epoch 2 step 25 loss 2.225684881210327\n",
            "epoch 2 step 26 loss 2.2158422470092773\n",
            "epoch 2 step 27 loss 2.2018394470214844\n",
            "epoch 2 step 28 loss 2.179847240447998\n",
            "epoch 2 step 29 loss 2.31135892868042\n",
            "epoch 2 step 30 loss 2.1921348571777344\n",
            "epoch 2 step 31 loss 2.2924113273620605\n",
            "epoch 2 step 32 loss 2.2362873554229736\n",
            "epoch 2 step 33 loss 2.2560901641845703\n",
            "epoch 2 step 34 loss 2.2403151988983154\n",
            "epoch 2 step 35 loss 2.2810463905334473\n",
            "epoch 3 step 0 loss 2.2107582092285156 test_accuracy 19.200000762939453 train_accuracy 24.21875\n",
            "epoch 3 step 1 loss 2.263732433319092\n",
            "epoch 3 step 2 loss 2.250183582305908\n",
            "epoch 3 step 3 loss 2.221924066543579\n",
            "epoch 3 step 4 loss 2.2669012546539307\n",
            "epoch 3 step 5 loss 2.1976540088653564\n",
            "epoch 3 step 6 loss 2.2316908836364746\n",
            "epoch 3 step 7 loss 2.1619155406951904\n",
            "epoch 3 step 8 loss 2.2505416870117188\n",
            "epoch 3 step 9 loss 2.236656665802002\n",
            "epoch 3 step 10 loss 2.238571882247925\n",
            "epoch 3 step 11 loss 2.195157527923584\n",
            "epoch 3 step 12 loss 2.195765733718872\n",
            "epoch 3 step 13 loss 2.2258057594299316\n",
            "epoch 3 step 14 loss 2.2496488094329834\n",
            "epoch 3 step 15 loss 2.2308387756347656\n",
            "epoch 3 step 16 loss 2.2106144428253174\n",
            "epoch 3 step 17 loss 2.19541072845459\n",
            "epoch 3 step 18 loss 2.2230849266052246\n",
            "epoch 3 step 19 loss 2.209548234939575\n",
            "epoch 3 step 20 loss 2.224548578262329 test_accuracy 19.80000114440918 train_accuracy 19.53125\n",
            "epoch 3 step 21 loss 2.191005229949951\n",
            "epoch 3 step 22 loss 2.2277557849884033\n",
            "epoch 3 step 23 loss 2.290951728820801\n",
            "epoch 3 step 24 loss 2.225731134414673\n",
            "epoch 3 step 25 loss 2.2126245498657227\n",
            "epoch 3 step 26 loss 2.204853057861328\n",
            "epoch 3 step 27 loss 2.2178144454956055\n",
            "epoch 3 step 28 loss 2.1825883388519287\n",
            "epoch 3 step 29 loss 2.2313060760498047\n",
            "epoch 3 step 30 loss 2.208678722381592\n",
            "epoch 3 step 31 loss 2.2567813396453857\n",
            "epoch 3 step 32 loss 2.2124290466308594\n",
            "epoch 3 step 33 loss 2.244906187057495\n",
            "epoch 3 step 34 loss 2.2036666870117188\n",
            "epoch 3 step 35 loss 2.3321399688720703\n",
            "epoch 4 step 0 loss 2.24448561668396 test_accuracy 19.0 train_accuracy 24.21875\n",
            "epoch 4 step 1 loss 2.2918198108673096\n",
            "epoch 4 step 2 loss 2.2300171852111816\n",
            "epoch 4 step 3 loss 2.235455274581909\n",
            "epoch 4 step 4 loss 2.164525032043457\n",
            "epoch 4 step 5 loss 2.1899614334106445\n",
            "epoch 4 step 6 loss 2.2407708168029785\n",
            "epoch 4 step 7 loss 2.2534828186035156\n",
            "epoch 4 step 8 loss 2.2077364921569824\n",
            "epoch 4 step 9 loss 2.2687180042266846\n",
            "epoch 4 step 10 loss 2.2525510787963867\n",
            "epoch 4 step 11 loss 2.2066116333007812\n",
            "epoch 4 step 12 loss 2.228111982345581\n",
            "epoch 4 step 13 loss 2.2072596549987793\n",
            "epoch 4 step 14 loss 2.2048065662384033\n",
            "epoch 4 step 15 loss 2.1955623626708984\n",
            "epoch 4 step 16 loss 2.2262163162231445\n",
            "epoch 4 step 17 loss 2.203727960586548\n",
            "epoch 4 step 18 loss 2.2162137031555176\n",
            "epoch 4 step 19 loss 2.2672219276428223\n",
            "epoch 4 step 20 loss 2.200268268585205 test_accuracy 21.000001907348633 train_accuracy 25.78125\n",
            "epoch 4 step 21 loss 2.2726762294769287\n",
            "epoch 4 step 22 loss 2.136617660522461\n",
            "epoch 4 step 23 loss 2.2120742797851562\n",
            "epoch 4 step 24 loss 2.177035093307495\n",
            "epoch 4 step 25 loss 2.1949851512908936\n",
            "epoch 4 step 26 loss 2.2325828075408936\n",
            "epoch 4 step 27 loss 2.223130941390991\n",
            "epoch 4 step 28 loss 2.2063634395599365\n",
            "epoch 4 step 29 loss 2.249643087387085\n",
            "epoch 4 step 30 loss 2.201760768890381\n",
            "epoch 4 step 31 loss 2.133894920349121\n",
            "epoch 4 step 32 loss 2.271358013153076\n",
            "epoch 4 step 33 loss 2.2119522094726562\n",
            "epoch 4 step 34 loss 2.174870014190674\n",
            "epoch 4 step 35 loss 2.22733998298645\n",
            "epoch 5 step 0 loss 2.235511064529419 test_accuracy 21.400001525878906 train_accuracy 21.875\n",
            "epoch 5 step 1 loss 2.2497472763061523\n",
            "epoch 5 step 2 loss 2.1885251998901367\n",
            "epoch 5 step 3 loss 2.259967803955078\n",
            "epoch 5 step 4 loss 2.1587512493133545\n",
            "epoch 5 step 5 loss 2.2688241004943848\n",
            "epoch 5 step 6 loss 2.2368581295013428\n",
            "epoch 5 step 7 loss 2.2395551204681396\n",
            "epoch 5 step 8 loss 2.1734731197357178\n",
            "epoch 5 step 9 loss 2.167499303817749\n",
            "epoch 5 step 10 loss 2.2471601963043213\n",
            "epoch 5 step 11 loss 2.2657856941223145\n",
            "epoch 5 step 12 loss 2.194704294204712\n",
            "epoch 5 step 13 loss 2.2680530548095703\n",
            "epoch 5 step 14 loss 2.2371158599853516\n",
            "epoch 5 step 15 loss 2.1882829666137695\n",
            "epoch 5 step 16 loss 2.23665452003479\n",
            "epoch 5 step 17 loss 2.1611220836639404\n",
            "epoch 5 step 18 loss 2.1562414169311523\n",
            "epoch 5 step 19 loss 2.196096658706665\n",
            "epoch 5 step 20 loss 2.255654811859131 test_accuracy 21.000001907348633 train_accuracy 28.125\n",
            "epoch 5 step 21 loss 2.172517776489258\n",
            "epoch 5 step 22 loss 2.2219772338867188\n",
            "epoch 5 step 23 loss 2.197627544403076\n",
            "epoch 5 step 24 loss 2.186566114425659\n",
            "epoch 5 step 25 loss 2.230778694152832\n",
            "epoch 5 step 26 loss 2.1922214031219482\n",
            "epoch 5 step 27 loss 2.186277389526367\n",
            "epoch 5 step 28 loss 2.219911575317383\n",
            "epoch 5 step 29 loss 2.2017581462860107\n",
            "epoch 5 step 30 loss 2.2591161727905273\n",
            "epoch 5 step 31 loss 2.21770977973938\n",
            "epoch 5 step 32 loss 2.2600698471069336\n",
            "epoch 5 step 33 loss 2.226557731628418\n",
            "epoch 5 step 34 loss 2.2325446605682373\n",
            "epoch 5 step 35 loss 2.230109691619873\n",
            "epoch 6 step 0 loss 2.2140111923217773 test_accuracy 20.200000762939453 train_accuracy 26.5625\n",
            "epoch 6 step 1 loss 2.2226603031158447\n",
            "epoch 6 step 2 loss 2.145594596862793\n",
            "epoch 6 step 3 loss 2.226547956466675\n",
            "epoch 6 step 4 loss 2.221017837524414\n",
            "epoch 6 step 5 loss 2.1967318058013916\n",
            "epoch 6 step 6 loss 2.2102549076080322\n",
            "epoch 6 step 7 loss 2.1612637042999268\n",
            "epoch 6 step 8 loss 2.1820318698883057\n",
            "epoch 6 step 9 loss 2.222938060760498\n",
            "epoch 6 step 10 loss 2.213923931121826\n",
            "epoch 6 step 11 loss 2.1769180297851562\n",
            "epoch 6 step 12 loss 2.1100220680236816\n",
            "epoch 6 step 13 loss 2.1638095378875732\n",
            "epoch 6 step 14 loss 2.1929659843444824\n",
            "epoch 6 step 15 loss 2.199280023574829\n",
            "epoch 6 step 16 loss 2.1588099002838135\n",
            "epoch 6 step 17 loss 2.2092125415802\n",
            "epoch 6 step 18 loss 2.158806562423706\n",
            "epoch 6 step 19 loss 2.218594551086426\n",
            "epoch 6 step 20 loss 2.266813039779663 test_accuracy 22.400001525878906 train_accuracy 28.125\n",
            "epoch 6 step 21 loss 2.222324848175049\n",
            "epoch 6 step 22 loss 2.1998372077941895\n",
            "epoch 6 step 23 loss 2.2127139568328857\n",
            "epoch 6 step 24 loss 2.231977701187134\n",
            "epoch 6 step 25 loss 2.174879550933838\n",
            "epoch 6 step 26 loss 2.210319995880127\n",
            "epoch 6 step 27 loss 2.193624973297119\n",
            "epoch 6 step 28 loss 2.24969744682312\n",
            "epoch 6 step 29 loss 2.2568583488464355\n",
            "epoch 6 step 30 loss 2.2706522941589355\n",
            "epoch 6 step 31 loss 2.1562647819519043\n",
            "epoch 6 step 32 loss 2.263561725616455\n",
            "epoch 6 step 33 loss 2.1942086219787598\n",
            "epoch 6 step 34 loss 2.1790926456451416\n",
            "epoch 6 step 35 loss 2.056976795196533\n",
            "epoch 7 step 0 loss 2.2505829334259033 test_accuracy 25.000001907348633 train_accuracy 21.875\n",
            "epoch 7 step 1 loss 2.170828342437744\n",
            "epoch 7 step 2 loss 2.2042808532714844\n",
            "epoch 7 step 3 loss 2.1782026290893555\n",
            "epoch 7 step 4 loss 2.193873405456543\n",
            "epoch 7 step 5 loss 2.250886917114258\n",
            "epoch 7 step 6 loss 2.207554817199707\n",
            "epoch 7 step 7 loss 2.237576484680176\n",
            "epoch 7 step 8 loss 2.229928731918335\n",
            "epoch 7 step 9 loss 2.1830673217773438\n",
            "epoch 7 step 10 loss 2.16914963722229\n",
            "epoch 7 step 11 loss 2.1888818740844727\n",
            "epoch 7 step 12 loss 2.2203009128570557\n",
            "epoch 7 step 13 loss 2.1763083934783936\n",
            "epoch 7 step 14 loss 2.19754958152771\n",
            "epoch 7 step 15 loss 2.2422986030578613\n",
            "epoch 7 step 16 loss 2.2134950160980225\n",
            "epoch 7 step 17 loss 2.1728336811065674\n",
            "epoch 7 step 18 loss 2.156022310256958\n",
            "epoch 7 step 19 loss 2.166630268096924\n",
            "epoch 7 step 20 loss 2.1944973468780518 test_accuracy 22.400001525878906 train_accuracy 23.4375\n",
            "epoch 7 step 21 loss 2.1859261989593506\n",
            "epoch 7 step 22 loss 2.1679491996765137\n",
            "epoch 7 step 23 loss 2.225386381149292\n",
            "epoch 7 step 24 loss 2.242992877960205\n",
            "epoch 7 step 25 loss 2.2021877765655518\n",
            "epoch 7 step 26 loss 2.265420913696289\n",
            "epoch 7 step 27 loss 2.235373020172119\n",
            "epoch 7 step 28 loss 2.1886534690856934\n",
            "epoch 7 step 29 loss 2.2303013801574707\n",
            "epoch 7 step 30 loss 2.2060272693634033\n",
            "epoch 7 step 31 loss 2.171898126602173\n",
            "epoch 7 step 32 loss 2.2426395416259766\n",
            "epoch 7 step 33 loss 2.2454278469085693\n",
            "epoch 7 step 34 loss 2.2380247116088867\n",
            "epoch 7 step 35 loss 2.175415277481079\n",
            "epoch 8 step 0 loss 2.245448589324951 test_accuracy 22.000001907348633 train_accuracy 23.4375\n",
            "epoch 8 step 1 loss 2.218954086303711\n",
            "epoch 8 step 2 loss 2.2102415561676025\n",
            "epoch 8 step 3 loss 2.203918695449829\n",
            "epoch 8 step 4 loss 2.212799072265625\n",
            "epoch 8 step 5 loss 2.1300365924835205\n",
            "epoch 8 step 6 loss 2.2596182823181152\n",
            "epoch 8 step 7 loss 2.186431407928467\n",
            "epoch 8 step 8 loss 2.1928162574768066\n",
            "epoch 8 step 9 loss 2.2137579917907715\n",
            "epoch 8 step 10 loss 2.1500675678253174\n",
            "epoch 8 step 11 loss 2.1761505603790283\n",
            "epoch 8 step 12 loss 2.1660027503967285\n",
            "epoch 8 step 13 loss 2.2052669525146484\n",
            "epoch 8 step 14 loss 2.166513204574585\n",
            "epoch 8 step 15 loss 2.2238426208496094\n",
            "epoch 8 step 16 loss 2.232696056365967\n",
            "epoch 8 step 17 loss 2.220663070678711\n",
            "epoch 8 step 18 loss 2.2519264221191406\n",
            "epoch 8 step 19 loss 2.2294952869415283\n",
            "epoch 8 step 20 loss 2.1749837398529053 test_accuracy 24.400001525878906 train_accuracy 21.09375\n",
            "epoch 8 step 21 loss 2.19051194190979\n",
            "epoch 8 step 22 loss 2.1693830490112305\n",
            "epoch 8 step 23 loss 2.1771762371063232\n",
            "epoch 8 step 24 loss 2.202192544937134\n",
            "epoch 8 step 25 loss 2.1954386234283447\n",
            "epoch 8 step 26 loss 2.214352607727051\n",
            "epoch 8 step 27 loss 2.2054617404937744\n",
            "epoch 8 step 28 loss 2.1841537952423096\n",
            "epoch 8 step 29 loss 2.1633384227752686\n",
            "epoch 8 step 30 loss 2.155988931655884\n",
            "epoch 8 step 31 loss 2.2071762084960938\n",
            "epoch 8 step 32 loss 2.2276010513305664\n",
            "epoch 8 step 33 loss 2.18257474899292\n",
            "epoch 8 step 34 loss 2.1789209842681885\n",
            "epoch 8 step 35 loss 2.219048023223877\n",
            "epoch 9 step 0 loss 2.2288684844970703 test_accuracy 22.80000114440918 train_accuracy 26.5625\n",
            "epoch 9 step 1 loss 2.2989249229431152\n",
            "epoch 9 step 2 loss 2.226318597793579\n",
            "epoch 9 step 3 loss 2.214160203933716\n",
            "epoch 9 step 4 loss 2.1983518600463867\n",
            "epoch 9 step 5 loss 2.22037672996521\n",
            "epoch 9 step 6 loss 2.2358243465423584\n",
            "epoch 9 step 7 loss 2.1779630184173584\n",
            "epoch 9 step 8 loss 2.21659517288208\n",
            "epoch 9 step 9 loss 2.153038263320923\n",
            "epoch 9 step 10 loss 2.1869287490844727\n",
            "epoch 9 step 11 loss 2.201380729675293\n",
            "epoch 9 step 12 loss 2.2154271602630615\n",
            "epoch 9 step 13 loss 2.202610492706299\n",
            "epoch 9 step 14 loss 2.250389575958252\n",
            "epoch 9 step 15 loss 2.207298517227173\n",
            "epoch 9 step 16 loss 2.1542916297912598\n",
            "epoch 9 step 17 loss 2.188380002975464\n",
            "epoch 9 step 18 loss 2.1989009380340576\n",
            "epoch 9 step 19 loss 2.1929104328155518\n",
            "epoch 9 step 20 loss 2.2291479110717773 test_accuracy 23.80000114440918 train_accuracy 28.125\n",
            "epoch 9 step 21 loss 2.2148897647857666\n",
            "epoch 9 step 22 loss 2.184138536453247\n",
            "epoch 9 step 23 loss 2.216899871826172\n",
            "epoch 9 step 24 loss 2.1244421005249023\n",
            "epoch 9 step 25 loss 2.219508171081543\n",
            "epoch 9 step 26 loss 2.1869430541992188\n",
            "epoch 9 step 27 loss 2.184896230697632\n",
            "epoch 9 step 28 loss 2.2057785987854004\n",
            "epoch 9 step 29 loss 2.108407974243164\n",
            "epoch 9 step 30 loss 2.230154275894165\n",
            "epoch 9 step 31 loss 2.130525588989258\n",
            "epoch 9 step 32 loss 2.177630662918091\n",
            "epoch 9 step 33 loss 2.203005075454712\n",
            "epoch 9 step 34 loss 2.20352840423584\n",
            "epoch 9 step 35 loss 2.2592246532440186\n",
            "epoch 10 step 0 loss 2.1744790077209473 test_accuracy 22.80000114440918 train_accuracy 22.65625\n",
            "epoch 10 step 1 loss 2.2437021732330322\n",
            "epoch 10 step 2 loss 2.166987419128418\n",
            "epoch 10 step 3 loss 2.1305603981018066\n",
            "epoch 10 step 4 loss 2.202332019805908\n",
            "epoch 10 step 5 loss 2.181790828704834\n",
            "epoch 10 step 6 loss 2.2130730152130127\n",
            "epoch 10 step 7 loss 2.1462345123291016\n",
            "epoch 10 step 8 loss 2.2156178951263428\n",
            "epoch 10 step 9 loss 2.206051826477051\n",
            "epoch 10 step 10 loss 2.2189180850982666\n",
            "epoch 10 step 11 loss 2.1652138233184814\n",
            "epoch 10 step 12 loss 2.1799845695495605\n",
            "epoch 10 step 13 loss 2.1475257873535156\n",
            "epoch 10 step 14 loss 2.229330539703369\n",
            "epoch 10 step 15 loss 2.1900227069854736\n",
            "epoch 10 step 16 loss 2.1401138305664062\n",
            "epoch 10 step 17 loss 2.1867454051971436\n",
            "epoch 10 step 18 loss 2.232430934906006\n",
            "epoch 10 step 19 loss 2.1769180297851562\n",
            "epoch 10 step 20 loss 2.169943332672119 test_accuracy 24.600000381469727 train_accuracy 21.875\n",
            "epoch 10 step 21 loss 2.2264184951782227\n",
            "epoch 10 step 22 loss 2.1906321048736572\n",
            "epoch 10 step 23 loss 2.177520751953125\n",
            "epoch 10 step 24 loss 2.242971658706665\n",
            "epoch 10 step 25 loss 2.1587154865264893\n",
            "epoch 10 step 26 loss 2.1434693336486816\n",
            "epoch 10 step 27 loss 2.189347743988037\n",
            "epoch 10 step 28 loss 2.127840757369995\n",
            "epoch 10 step 29 loss 2.1738436222076416\n",
            "epoch 10 step 30 loss 2.1799042224884033\n",
            "epoch 10 step 31 loss 2.197655439376831\n",
            "epoch 10 step 32 loss 2.2088327407836914\n",
            "epoch 10 step 33 loss 2.239259719848633\n",
            "epoch 10 step 34 loss 2.1681172847747803\n",
            "epoch 10 step 35 loss 2.168067455291748\n",
            "epoch 11 step 0 loss 2.2068166732788086 test_accuracy 27.000001907348633 train_accuracy 23.4375\n",
            "epoch 11 step 1 loss 2.170316219329834\n",
            "epoch 11 step 2 loss 2.208437204360962\n",
            "epoch 11 step 3 loss 2.1052584648132324\n",
            "epoch 11 step 4 loss 2.2619106769561768\n",
            "epoch 11 step 5 loss 2.208810806274414\n",
            "epoch 11 step 6 loss 2.0987277030944824\n",
            "epoch 11 step 7 loss 2.19381046295166\n",
            "epoch 11 step 8 loss 2.1977694034576416\n",
            "epoch 11 step 9 loss 2.2038283348083496\n",
            "epoch 11 step 10 loss 2.17263126373291\n",
            "epoch 11 step 11 loss 2.139799118041992\n",
            "epoch 11 step 12 loss 2.2158663272857666\n",
            "epoch 11 step 13 loss 2.200939655303955\n",
            "epoch 11 step 14 loss 2.156982421875\n",
            "epoch 11 step 15 loss 2.257122039794922\n",
            "epoch 11 step 16 loss 2.1784653663635254\n",
            "epoch 11 step 17 loss 2.171344757080078\n",
            "epoch 11 step 18 loss 2.2255451679229736\n",
            "epoch 11 step 19 loss 2.2139716148376465\n",
            "epoch 11 step 20 loss 2.105159282684326 test_accuracy 24.600000381469727 train_accuracy 25.0\n",
            "epoch 11 step 21 loss 2.163760185241699\n",
            "epoch 11 step 22 loss 2.2248826026916504\n",
            "epoch 11 step 23 loss 2.1419968605041504\n",
            "epoch 11 step 24 loss 2.1981163024902344\n",
            "epoch 11 step 25 loss 2.1449477672576904\n",
            "epoch 11 step 26 loss 2.18643856048584\n",
            "epoch 11 step 27 loss 2.1840574741363525\n",
            "epoch 11 step 28 loss 2.2122180461883545\n",
            "epoch 11 step 29 loss 2.17539119720459\n",
            "epoch 11 step 30 loss 2.245579242706299\n",
            "epoch 11 step 31 loss 2.2248036861419678\n",
            "epoch 11 step 32 loss 2.189049243927002\n",
            "epoch 11 step 33 loss 2.2019360065460205\n",
            "epoch 11 step 34 loss 2.27938175201416\n",
            "epoch 11 step 35 loss 2.2036142349243164\n",
            "epoch 12 step 0 loss 2.128161668777466 test_accuracy 26.200000762939453 train_accuracy 23.4375\n",
            "epoch 12 step 1 loss 2.255723476409912\n",
            "epoch 12 step 2 loss 2.119964838027954\n",
            "epoch 12 step 3 loss 2.171785831451416\n",
            "epoch 12 step 4 loss 2.2045366764068604\n",
            "epoch 12 step 5 loss 2.2222447395324707\n",
            "epoch 12 step 6 loss 2.1986968517303467\n",
            "epoch 12 step 7 loss 2.1405458450317383\n",
            "epoch 12 step 8 loss 2.18222975730896\n",
            "epoch 12 step 9 loss 2.2210628986358643\n",
            "epoch 12 step 10 loss 2.2350594997406006\n",
            "epoch 12 step 11 loss 2.147468328475952\n",
            "epoch 12 step 12 loss 2.1205451488494873\n",
            "epoch 12 step 13 loss 2.2603588104248047\n",
            "epoch 12 step 14 loss 2.193251132965088\n",
            "epoch 12 step 15 loss 2.2040176391601562\n",
            "epoch 12 step 16 loss 2.1856529712677\n",
            "epoch 12 step 17 loss 2.198887348175049\n",
            "epoch 12 step 18 loss 2.203418254852295\n",
            "epoch 12 step 19 loss 2.1554551124572754\n",
            "epoch 12 step 20 loss 2.1554043292999268 test_accuracy 25.600000381469727 train_accuracy 26.5625\n",
            "epoch 12 step 21 loss 2.1444172859191895\n",
            "epoch 12 step 22 loss 2.227877378463745\n",
            "epoch 12 step 23 loss 2.2304415702819824\n",
            "epoch 12 step 24 loss 2.1978209018707275\n",
            "epoch 12 step 25 loss 2.161421298980713\n",
            "epoch 12 step 26 loss 2.218430995941162\n",
            "epoch 12 step 27 loss 2.182403087615967\n",
            "epoch 12 step 28 loss 2.2300949096679688\n",
            "epoch 12 step 29 loss 2.2037887573242188\n",
            "epoch 12 step 30 loss 2.169660806655884\n",
            "epoch 12 step 31 loss 2.191246747970581\n",
            "epoch 12 step 32 loss 2.1496033668518066\n",
            "epoch 12 step 33 loss 2.1719086170196533\n",
            "epoch 12 step 34 loss 2.188854694366455\n",
            "epoch 12 step 35 loss 2.203437089920044\n",
            "epoch 13 step 0 loss 2.1705262660980225 test_accuracy 24.000001907348633 train_accuracy 25.78125\n",
            "epoch 13 step 1 loss 2.2333967685699463\n",
            "epoch 13 step 2 loss 2.1966118812561035\n",
            "epoch 13 step 3 loss 2.181203603744507\n",
            "epoch 13 step 4 loss 2.200477361679077\n",
            "epoch 13 step 5 loss 2.220195770263672\n",
            "epoch 13 step 6 loss 2.147655963897705\n",
            "epoch 13 step 7 loss 2.2451984882354736\n",
            "epoch 13 step 8 loss 2.1226627826690674\n",
            "epoch 13 step 9 loss 2.183445453643799\n",
            "epoch 13 step 10 loss 2.1535158157348633\n",
            "epoch 13 step 11 loss 2.2213518619537354\n",
            "epoch 13 step 12 loss 2.1902010440826416\n",
            "epoch 13 step 13 loss 2.200599193572998\n",
            "epoch 13 step 14 loss 2.2455408573150635\n",
            "epoch 13 step 15 loss 2.2258055210113525\n",
            "epoch 13 step 16 loss 2.2383291721343994\n",
            "epoch 13 step 17 loss 2.1839499473571777\n",
            "epoch 13 step 18 loss 2.213009834289551\n",
            "epoch 13 step 19 loss 2.164431095123291\n",
            "epoch 13 step 20 loss 2.2041375637054443 test_accuracy 24.600000381469727 train_accuracy 22.65625\n",
            "epoch 13 step 21 loss 2.1865286827087402\n",
            "epoch 13 step 22 loss 2.236051559448242\n",
            "epoch 13 step 23 loss 2.2398486137390137\n",
            "epoch 13 step 24 loss 2.1264045238494873\n",
            "epoch 13 step 25 loss 2.1807610988616943\n",
            "epoch 13 step 26 loss 2.1834523677825928\n",
            "epoch 13 step 27 loss 2.1667606830596924\n",
            "epoch 13 step 28 loss 2.228421211242676\n",
            "epoch 13 step 29 loss 2.165888547897339\n",
            "epoch 13 step 30 loss 2.1939375400543213\n",
            "epoch 13 step 31 loss 2.205787181854248\n",
            "epoch 13 step 32 loss 2.1276168823242188\n",
            "epoch 13 step 33 loss 2.2049484252929688\n",
            "epoch 13 step 34 loss 2.13804030418396\n",
            "epoch 13 step 35 loss 2.1517300605773926\n",
            "epoch 14 step 0 loss 2.195517063140869 test_accuracy 24.000001907348633 train_accuracy 29.6875\n",
            "epoch 14 step 1 loss 2.1943767070770264\n",
            "epoch 14 step 2 loss 2.2461636066436768\n",
            "epoch 14 step 3 loss 2.1781439781188965\n",
            "epoch 14 step 4 loss 2.133021593093872\n",
            "epoch 14 step 5 loss 2.2239692211151123\n",
            "epoch 14 step 6 loss 2.1878623962402344\n",
            "epoch 14 step 7 loss 2.200589656829834\n",
            "epoch 14 step 8 loss 2.2043297290802\n",
            "epoch 14 step 9 loss 2.200119972229004\n",
            "epoch 14 step 10 loss 2.1720216274261475\n",
            "epoch 14 step 11 loss 2.201414108276367\n",
            "epoch 14 step 12 loss 2.1066489219665527\n",
            "epoch 14 step 13 loss 2.1627211570739746\n",
            "epoch 14 step 14 loss 2.161210298538208\n",
            "epoch 14 step 15 loss 2.1502201557159424\n",
            "epoch 14 step 16 loss 2.128042221069336\n",
            "epoch 14 step 17 loss 2.1597044467926025\n",
            "epoch 14 step 18 loss 2.1091806888580322\n",
            "epoch 14 step 19 loss 2.190694570541382\n",
            "epoch 14 step 20 loss 2.2014291286468506 test_accuracy 25.80000114440918 train_accuracy 24.21875\n",
            "epoch 14 step 21 loss 2.153062582015991\n",
            "epoch 14 step 22 loss 2.1867830753326416\n",
            "epoch 14 step 23 loss 2.260300636291504\n",
            "epoch 14 step 24 loss 2.183786153793335\n",
            "epoch 14 step 25 loss 2.200212240219116\n",
            "epoch 14 step 26 loss 2.2439355850219727\n",
            "epoch 14 step 27 loss 2.1720569133758545\n",
            "epoch 14 step 28 loss 2.200364828109741\n",
            "epoch 14 step 29 loss 2.1541500091552734\n",
            "epoch 14 step 30 loss 2.191779136657715\n",
            "epoch 14 step 31 loss 2.182286500930786\n",
            "epoch 14 step 32 loss 2.183380603790283\n",
            "epoch 14 step 33 loss 2.1871650218963623\n",
            "epoch 14 step 34 loss 2.200705051422119\n",
            "epoch 14 step 35 loss 2.254958391189575\n",
            "epoch 15 step 0 loss 2.2030537128448486 test_accuracy 24.400001525878906 train_accuracy 19.53125\n",
            "epoch 15 step 1 loss 2.1618752479553223\n",
            "epoch 15 step 2 loss 2.185877799987793\n",
            "epoch 15 step 3 loss 2.207747459411621\n",
            "epoch 15 step 4 loss 2.1421632766723633\n",
            "epoch 15 step 5 loss 2.213428020477295\n",
            "epoch 15 step 6 loss 2.1770577430725098\n",
            "epoch 15 step 7 loss 2.195681095123291\n",
            "epoch 15 step 8 loss 2.1263678073883057\n",
            "epoch 15 step 9 loss 2.1924946308135986\n",
            "epoch 15 step 10 loss 2.183525800704956\n",
            "epoch 15 step 11 loss 2.1412763595581055\n",
            "epoch 15 step 12 loss 2.162649393081665\n",
            "epoch 15 step 13 loss 2.17873215675354\n",
            "epoch 15 step 14 loss 2.139584541320801\n",
            "epoch 15 step 15 loss 2.1534478664398193\n",
            "epoch 15 step 16 loss 2.1392276287078857\n",
            "epoch 15 step 17 loss 2.149521827697754\n",
            "epoch 15 step 18 loss 2.296102285385132\n",
            "epoch 15 step 19 loss 2.2054219245910645\n",
            "epoch 15 step 20 loss 2.242149829864502 test_accuracy 26.400001525878906 train_accuracy 28.90625\n",
            "epoch 15 step 21 loss 2.1677629947662354\n",
            "epoch 15 step 22 loss 2.1387178897857666\n",
            "epoch 15 step 23 loss 2.1902284622192383\n",
            "epoch 15 step 24 loss 2.203789472579956\n",
            "epoch 15 step 25 loss 2.174046754837036\n",
            "epoch 15 step 26 loss 2.1853280067443848\n",
            "epoch 15 step 27 loss 2.2556958198547363\n",
            "epoch 15 step 28 loss 2.192767858505249\n",
            "epoch 15 step 29 loss 2.159564733505249\n",
            "epoch 15 step 30 loss 2.174041748046875\n",
            "epoch 15 step 31 loss 2.1896755695343018\n",
            "epoch 15 step 32 loss 2.2260329723358154\n",
            "epoch 15 step 33 loss 2.2011795043945312\n",
            "epoch 15 step 34 loss 2.122291326522827\n",
            "epoch 15 step 35 loss 2.2560713291168213\n",
            "epoch 16 step 0 loss 2.204779863357544 test_accuracy 27.400001525878906 train_accuracy 32.8125\n",
            "epoch 16 step 1 loss 2.173306703567505\n",
            "epoch 16 step 2 loss 2.185194969177246\n",
            "epoch 16 step 3 loss 2.172123432159424\n",
            "epoch 16 step 4 loss 2.212777614593506\n",
            "epoch 16 step 5 loss 2.1668484210968018\n",
            "epoch 16 step 6 loss 2.1443099975585938\n",
            "epoch 16 step 7 loss 2.192060708999634\n",
            "epoch 16 step 8 loss 2.1950249671936035\n",
            "epoch 16 step 9 loss 2.2234764099121094\n",
            "epoch 16 step 10 loss 2.197876453399658\n",
            "epoch 16 step 11 loss 2.1669468879699707\n",
            "epoch 16 step 12 loss 2.190809488296509\n",
            "epoch 16 step 13 loss 2.1760144233703613\n",
            "epoch 16 step 14 loss 2.1804659366607666\n",
            "epoch 16 step 15 loss 2.1573593616485596\n",
            "epoch 16 step 16 loss 2.1021084785461426\n",
            "epoch 16 step 17 loss 2.1991615295410156\n",
            "epoch 16 step 18 loss 2.2339463233947754\n",
            "epoch 16 step 19 loss 2.207932949066162\n",
            "epoch 16 step 20 loss 2.149474620819092 test_accuracy 23.400001525878906 train_accuracy 30.46875\n",
            "epoch 16 step 21 loss 2.171022415161133\n",
            "epoch 16 step 22 loss 2.171513795852661\n",
            "epoch 16 step 23 loss 2.1039211750030518\n",
            "epoch 16 step 24 loss 2.1899545192718506\n",
            "epoch 16 step 25 loss 2.167276382446289\n",
            "epoch 16 step 26 loss 2.1482338905334473\n",
            "epoch 16 step 27 loss 2.2932217121124268\n",
            "epoch 16 step 28 loss 2.1548945903778076\n",
            "epoch 16 step 29 loss 2.2141683101654053\n",
            "epoch 16 step 30 loss 2.2075634002685547\n",
            "epoch 16 step 31 loss 2.2343711853027344\n",
            "epoch 16 step 32 loss 2.2392799854278564\n",
            "epoch 16 step 33 loss 2.1056838035583496\n",
            "epoch 16 step 34 loss 2.1647701263427734\n",
            "epoch 16 step 35 loss 2.130066394805908\n",
            "epoch 17 step 0 loss 2.1469240188598633 test_accuracy 23.200000762939453 train_accuracy 28.125\n",
            "epoch 17 step 1 loss 2.2233736515045166\n",
            "epoch 17 step 2 loss 2.2039501667022705\n",
            "epoch 17 step 3 loss 2.190366506576538\n",
            "epoch 17 step 4 loss 2.1836893558502197\n",
            "epoch 17 step 5 loss 2.1957144737243652\n",
            "epoch 17 step 6 loss 2.1147916316986084\n",
            "epoch 17 step 7 loss 2.179776430130005\n",
            "epoch 17 step 8 loss 2.1307952404022217\n",
            "epoch 17 step 9 loss 2.1868972778320312\n",
            "epoch 17 step 10 loss 2.142892837524414\n",
            "epoch 17 step 11 loss 2.2078824043273926\n",
            "epoch 17 step 12 loss 2.1419436931610107\n",
            "epoch 17 step 13 loss 2.185532808303833\n",
            "epoch 17 step 14 loss 2.2475087642669678\n",
            "epoch 17 step 15 loss 2.1616768836975098\n",
            "epoch 17 step 16 loss 2.1224958896636963\n",
            "epoch 17 step 17 loss 2.2130022048950195\n",
            "epoch 17 step 18 loss 2.162970542907715\n",
            "epoch 17 step 19 loss 2.176029920578003\n",
            "epoch 17 step 20 loss 2.1246204376220703 test_accuracy 26.200000762939453 train_accuracy 31.25\n",
            "epoch 17 step 21 loss 2.1128132343292236\n",
            "epoch 17 step 22 loss 2.1970229148864746\n",
            "epoch 17 step 23 loss 2.2928037643432617\n",
            "epoch 17 step 24 loss 2.1626787185668945\n",
            "epoch 17 step 25 loss 2.247741222381592\n",
            "epoch 17 step 26 loss 2.1863410472869873\n",
            "epoch 17 step 27 loss 2.1490399837493896\n",
            "epoch 17 step 28 loss 2.1652116775512695\n",
            "epoch 17 step 29 loss 2.192636728286743\n",
            "epoch 17 step 30 loss 2.145314931869507\n",
            "epoch 17 step 31 loss 2.176433801651001\n",
            "epoch 17 step 32 loss 2.1938982009887695\n",
            "epoch 17 step 33 loss 2.1714460849761963\n",
            "epoch 17 step 34 loss 2.121591567993164\n",
            "epoch 17 step 35 loss 2.1646111011505127\n",
            "epoch 18 step 0 loss 2.127629280090332 test_accuracy 25.80000114440918 train_accuracy 35.9375\n",
            "epoch 18 step 1 loss 2.2007453441619873\n",
            "epoch 18 step 2 loss 2.1886703968048096\n",
            "epoch 18 step 3 loss 2.162327289581299\n",
            "epoch 18 step 4 loss 2.2420408725738525\n",
            "epoch 18 step 5 loss 2.228184461593628\n",
            "epoch 18 step 6 loss 2.158060073852539\n",
            "epoch 18 step 7 loss 2.1995503902435303\n",
            "epoch 18 step 8 loss 2.1738433837890625\n",
            "epoch 18 step 9 loss 2.1394500732421875\n",
            "epoch 18 step 10 loss 2.107071876525879\n",
            "epoch 18 step 11 loss 2.1591382026672363\n",
            "epoch 18 step 12 loss 2.1941421031951904\n",
            "epoch 18 step 13 loss 2.182830333709717\n",
            "epoch 18 step 14 loss 2.207226514816284\n",
            "epoch 18 step 15 loss 2.18514347076416\n",
            "epoch 18 step 16 loss 2.1344282627105713\n",
            "epoch 18 step 17 loss 2.2072653770446777\n",
            "epoch 18 step 18 loss 2.210428476333618\n",
            "epoch 18 step 19 loss 2.2327191829681396\n",
            "epoch 18 step 20 loss 2.155216693878174 test_accuracy 26.000001907348633 train_accuracy 28.90625\n",
            "epoch 18 step 21 loss 2.204005002975464\n",
            "epoch 18 step 22 loss 2.1194963455200195\n",
            "epoch 18 step 23 loss 2.1870875358581543\n",
            "epoch 18 step 24 loss 2.1611900329589844\n",
            "epoch 18 step 25 loss 2.193113088607788\n",
            "epoch 18 step 26 loss 2.1692864894866943\n",
            "epoch 18 step 27 loss 2.16300892829895\n",
            "epoch 18 step 28 loss 2.129032850265503\n",
            "epoch 18 step 29 loss 2.1908583641052246\n",
            "epoch 18 step 30 loss 2.0927317142486572\n",
            "epoch 18 step 31 loss 2.174835681915283\n",
            "epoch 18 step 32 loss 2.2020862102508545\n",
            "epoch 18 step 33 loss 2.072291135787964\n",
            "epoch 18 step 34 loss 2.153968334197998\n",
            "epoch 18 step 35 loss 2.2691760063171387\n",
            "epoch 19 step 0 loss 2.2044992446899414 test_accuracy 27.200000762939453 train_accuracy 30.46875\n",
            "epoch 19 step 1 loss 2.1853604316711426\n",
            "epoch 19 step 2 loss 2.1499733924865723\n",
            "epoch 19 step 3 loss 2.1795032024383545\n",
            "epoch 19 step 4 loss 2.1498703956604004\n",
            "epoch 19 step 5 loss 2.1522023677825928\n",
            "epoch 19 step 6 loss 2.1877949237823486\n",
            "epoch 19 step 7 loss 2.204685688018799\n",
            "epoch 19 step 8 loss 2.185575485229492\n",
            "epoch 19 step 9 loss 2.2677974700927734\n",
            "epoch 19 step 10 loss 2.2106120586395264\n",
            "epoch 19 step 11 loss 2.153762102127075\n",
            "epoch 19 step 12 loss 2.178234577178955\n",
            "epoch 19 step 13 loss 2.1475470066070557\n",
            "epoch 19 step 14 loss 2.168860673904419\n",
            "epoch 19 step 15 loss 2.1367580890655518\n",
            "epoch 19 step 16 loss 2.166274309158325\n",
            "epoch 19 step 17 loss 2.1536998748779297\n",
            "epoch 19 step 18 loss 2.2145729064941406\n",
            "epoch 19 step 19 loss 2.2059552669525146\n",
            "epoch 19 step 20 loss 2.158970355987549 test_accuracy 26.80000114440918 train_accuracy 27.34375\n",
            "epoch 19 step 21 loss 2.229668140411377\n",
            "epoch 19 step 22 loss 2.1820738315582275\n",
            "epoch 19 step 23 loss 2.0952610969543457\n",
            "epoch 19 step 24 loss 2.12532639503479\n",
            "epoch 19 step 25 loss 2.152024269104004\n",
            "epoch 19 step 26 loss 2.1875641345977783\n",
            "epoch 19 step 27 loss 2.206979274749756\n",
            "epoch 19 step 28 loss 2.2622742652893066\n",
            "epoch 19 step 29 loss 2.1820671558380127\n",
            "epoch 19 step 30 loss 2.209540843963623\n",
            "epoch 19 step 31 loss 2.1356911659240723\n",
            "epoch 19 step 32 loss 2.121290445327759\n",
            "epoch 19 step 33 loss 2.1519196033477783\n",
            "epoch 19 step 34 loss 2.1264727115631104\n",
            "epoch 19 step 35 loss 2.154663562774658\n",
            "epoch 20 step 0 loss 2.200824022293091 test_accuracy 29.400001525878906 train_accuracy 26.5625\n",
            "epoch 20 step 1 loss 2.1335864067077637\n",
            "epoch 20 step 2 loss 2.17984938621521\n",
            "epoch 20 step 3 loss 2.124723434448242\n",
            "epoch 20 step 4 loss 2.162475347518921\n",
            "epoch 20 step 5 loss 2.161074638366699\n",
            "epoch 20 step 6 loss 2.1723766326904297\n",
            "epoch 20 step 7 loss 2.1737217903137207\n",
            "epoch 20 step 8 loss 2.224104166030884\n",
            "epoch 20 step 9 loss 2.175220489501953\n",
            "epoch 20 step 10 loss 2.145848035812378\n",
            "epoch 20 step 11 loss 2.1873953342437744\n",
            "epoch 20 step 12 loss 2.1969470977783203\n",
            "epoch 20 step 13 loss 2.1401050090789795\n",
            "epoch 20 step 14 loss 2.1443254947662354\n",
            "epoch 20 step 15 loss 2.225890636444092\n",
            "epoch 20 step 16 loss 2.186829090118408\n",
            "epoch 20 step 17 loss 2.189415693283081\n",
            "epoch 20 step 18 loss 2.1549956798553467\n",
            "epoch 20 step 19 loss 2.186229944229126\n",
            "epoch 20 step 20 loss 2.1689510345458984 test_accuracy 25.600000381469727 train_accuracy 35.9375\n",
            "epoch 20 step 21 loss 2.181931495666504\n",
            "epoch 20 step 22 loss 2.1814091205596924\n",
            "epoch 20 step 23 loss 2.1439030170440674\n",
            "epoch 20 step 24 loss 2.188185691833496\n",
            "epoch 20 step 25 loss 2.1516473293304443\n",
            "epoch 20 step 26 loss 2.16066837310791\n",
            "epoch 20 step 27 loss 2.2353460788726807\n",
            "epoch 20 step 28 loss 2.1318657398223877\n",
            "epoch 20 step 29 loss 2.1951208114624023\n",
            "epoch 20 step 30 loss 2.0699312686920166\n",
            "epoch 20 step 31 loss 2.0709352493286133\n",
            "epoch 20 step 32 loss 2.2157652378082275\n",
            "epoch 20 step 33 loss 2.1924855709075928\n",
            "epoch 20 step 34 loss 2.1600661277770996\n",
            "epoch 20 step 35 loss 2.208034038543701\n",
            "epoch 21 step 0 loss 2.151313304901123 test_accuracy 28.60000228881836 train_accuracy 28.90625\n",
            "epoch 21 step 1 loss 2.147789478302002\n",
            "epoch 21 step 2 loss 2.1487083435058594\n",
            "epoch 21 step 3 loss 2.1416244506835938\n",
            "epoch 21 step 4 loss 2.2508010864257812\n",
            "epoch 21 step 5 loss 2.199110984802246\n",
            "epoch 21 step 6 loss 2.190141201019287\n",
            "epoch 21 step 7 loss 2.148003339767456\n",
            "epoch 21 step 8 loss 2.1863009929656982\n",
            "epoch 21 step 9 loss 2.0939996242523193\n",
            "epoch 21 step 10 loss 2.1287684440612793\n",
            "epoch 21 step 11 loss 2.1506338119506836\n",
            "epoch 21 step 12 loss 2.192840099334717\n",
            "epoch 21 step 13 loss 2.172044038772583\n",
            "epoch 21 step 14 loss 2.171222448348999\n",
            "epoch 21 step 15 loss 2.1410653591156006\n",
            "epoch 21 step 16 loss 2.2022244930267334\n",
            "epoch 21 step 17 loss 2.0967326164245605\n",
            "epoch 21 step 18 loss 2.1668782234191895\n",
            "epoch 21 step 19 loss 2.2235729694366455\n",
            "epoch 21 step 20 loss 2.2163238525390625 test_accuracy 25.600000381469727 train_accuracy 28.90625\n",
            "epoch 21 step 21 loss 2.1720833778381348\n",
            "epoch 21 step 22 loss 2.153768539428711\n",
            "epoch 21 step 23 loss 2.159785032272339\n",
            "epoch 21 step 24 loss 2.172661542892456\n",
            "epoch 21 step 25 loss 2.1758458614349365\n",
            "epoch 21 step 26 loss 2.077319622039795\n",
            "epoch 21 step 27 loss 2.150056838989258\n",
            "epoch 21 step 28 loss 2.2076876163482666\n",
            "epoch 21 step 29 loss 2.2090578079223633\n",
            "epoch 21 step 30 loss 2.2455852031707764\n",
            "epoch 21 step 31 loss 2.1252894401550293\n",
            "epoch 21 step 32 loss 2.1669766902923584\n",
            "epoch 21 step 33 loss 2.103759527206421\n",
            "epoch 21 step 34 loss 2.187626838684082\n",
            "epoch 21 step 35 loss 2.2890546321868896\n",
            "epoch 22 step 0 loss 2.201307773590088 test_accuracy 25.400001525878906 train_accuracy 34.375\n",
            "epoch 22 step 1 loss 2.1480355262756348\n",
            "epoch 22 step 2 loss 2.153017282485962\n",
            "epoch 22 step 3 loss 2.195683479309082\n",
            "epoch 22 step 4 loss 2.1962547302246094\n",
            "epoch 22 step 5 loss 2.1920807361602783\n",
            "epoch 22 step 6 loss 2.2291646003723145\n",
            "epoch 22 step 7 loss 2.2044849395751953\n",
            "epoch 22 step 8 loss 2.2301278114318848\n",
            "epoch 22 step 9 loss 2.1968839168548584\n",
            "epoch 22 step 10 loss 2.1308281421661377\n",
            "epoch 22 step 11 loss 2.1793015003204346\n",
            "epoch 22 step 12 loss 2.203094244003296\n",
            "epoch 22 step 13 loss 2.195565700531006\n",
            "epoch 22 step 14 loss 2.1867427825927734\n",
            "epoch 22 step 15 loss 2.1061532497406006\n",
            "epoch 22 step 16 loss 2.1689507961273193\n",
            "epoch 22 step 17 loss 2.1335933208465576\n",
            "epoch 22 step 18 loss 2.2102599143981934\n",
            "epoch 22 step 19 loss 2.1417369842529297\n",
            "epoch 22 step 20 loss 2.2129769325256348 test_accuracy 26.000001907348633 train_accuracy 28.125\n",
            "epoch 22 step 21 loss 2.2205874919891357\n",
            "epoch 22 step 22 loss 2.157606363296509\n",
            "epoch 22 step 23 loss 2.0967235565185547\n",
            "epoch 22 step 24 loss 2.1583523750305176\n",
            "epoch 22 step 25 loss 2.1215531826019287\n",
            "epoch 22 step 26 loss 2.1484856605529785\n",
            "epoch 22 step 27 loss 2.1501641273498535\n",
            "epoch 22 step 28 loss 2.158109426498413\n",
            "epoch 22 step 29 loss 2.1915507316589355\n",
            "epoch 22 step 30 loss 2.143601894378662\n",
            "epoch 22 step 31 loss 2.179908514022827\n",
            "epoch 22 step 32 loss 2.134230136871338\n",
            "epoch 22 step 33 loss 2.14387845993042\n",
            "epoch 22 step 34 loss 2.1528944969177246\n",
            "epoch 22 step 35 loss 2.1389102935791016\n",
            "epoch 23 step 0 loss 2.1619620323181152 test_accuracy 26.80000114440918 train_accuracy 28.90625\n",
            "epoch 23 step 1 loss 2.1003010272979736\n",
            "epoch 23 step 2 loss 2.171046018600464\n",
            "epoch 23 step 3 loss 2.167602777481079\n",
            "epoch 23 step 4 loss 2.1227989196777344\n",
            "epoch 23 step 5 loss 2.160745620727539\n",
            "epoch 23 step 6 loss 2.187070608139038\n",
            "epoch 23 step 7 loss 2.161195755004883\n",
            "epoch 23 step 8 loss 2.1553332805633545\n",
            "epoch 23 step 9 loss 2.1558279991149902\n",
            "epoch 23 step 10 loss 2.1846156120300293\n",
            "epoch 23 step 11 loss 2.1490883827209473\n",
            "epoch 23 step 12 loss 2.1896800994873047\n",
            "epoch 23 step 13 loss 2.1049280166625977\n",
            "epoch 23 step 14 loss 2.153562545776367\n",
            "epoch 23 step 15 loss 2.138397455215454\n",
            "epoch 23 step 16 loss 2.1047236919403076\n",
            "epoch 23 step 17 loss 2.14933180809021\n",
            "epoch 23 step 18 loss 2.1334609985351562\n",
            "epoch 23 step 19 loss 2.181468963623047\n",
            "epoch 23 step 20 loss 2.2858798503875732 test_accuracy 26.400001525878906 train_accuracy 25.0\n",
            "epoch 23 step 21 loss 2.175341844558716\n",
            "epoch 23 step 22 loss 2.177116870880127\n",
            "epoch 23 step 23 loss 2.1700775623321533\n",
            "epoch 23 step 24 loss 2.19702410697937\n",
            "epoch 23 step 25 loss 2.21069598197937\n",
            "epoch 23 step 26 loss 2.1871328353881836\n",
            "epoch 23 step 27 loss 2.1645843982696533\n",
            "epoch 23 step 28 loss 2.1796178817749023\n",
            "epoch 23 step 29 loss 2.095033884048462\n",
            "epoch 23 step 30 loss 2.1919872760772705\n",
            "epoch 23 step 31 loss 2.0943331718444824\n",
            "epoch 23 step 32 loss 2.2123379707336426\n",
            "epoch 23 step 33 loss 2.20699143409729\n",
            "epoch 23 step 34 loss 2.131895065307617\n",
            "epoch 23 step 35 loss 2.1165614128112793\n",
            "epoch 24 step 0 loss 2.1472949981689453 test_accuracy 28.000001907348633 train_accuracy 25.0\n",
            "epoch 24 step 1 loss 2.121173620223999\n",
            "epoch 24 step 2 loss 2.194042444229126\n",
            "epoch 24 step 3 loss 2.1541271209716797\n",
            "epoch 24 step 4 loss 2.166259527206421\n",
            "epoch 24 step 5 loss 2.1537275314331055\n",
            "epoch 24 step 6 loss 2.1935877799987793\n",
            "epoch 24 step 7 loss 2.1324265003204346\n",
            "epoch 24 step 8 loss 2.176755666732788\n",
            "epoch 24 step 9 loss 2.153498888015747\n",
            "epoch 24 step 10 loss 2.14202880859375\n",
            "epoch 24 step 11 loss 2.2305939197540283\n",
            "epoch 24 step 12 loss 2.212611198425293\n",
            "epoch 24 step 13 loss 2.1835460662841797\n",
            "epoch 24 step 14 loss 2.191166877746582\n",
            "epoch 24 step 15 loss 2.2009668350219727\n",
            "epoch 24 step 16 loss 2.2136881351470947\n",
            "epoch 24 step 17 loss 2.178328037261963\n",
            "epoch 24 step 18 loss 2.142299175262451\n",
            "epoch 24 step 19 loss 2.1970460414886475\n",
            "epoch 24 step 20 loss 2.1996326446533203 test_accuracy 23.200000762939453 train_accuracy 26.5625\n",
            "epoch 24 step 21 loss 2.186854839324951\n",
            "epoch 24 step 22 loss 2.171557664871216\n",
            "epoch 24 step 23 loss 2.135038375854492\n",
            "epoch 24 step 24 loss 2.175995349884033\n",
            "epoch 24 step 25 loss 2.167705535888672\n",
            "epoch 24 step 26 loss 2.169245958328247\n",
            "epoch 24 step 27 loss 2.102642297744751\n",
            "epoch 24 step 28 loss 2.15093731880188\n",
            "epoch 24 step 29 loss 2.1598453521728516\n",
            "epoch 24 step 30 loss 2.1804006099700928\n",
            "epoch 24 step 31 loss 2.1555235385894775\n",
            "epoch 24 step 32 loss 2.1941978931427\n",
            "epoch 24 step 33 loss 2.178492546081543\n",
            "epoch 24 step 34 loss 2.145690679550171\n",
            "epoch 24 step 35 loss 2.06604266166687\n",
            "epoch 25 step 0 loss 2.1650936603546143 test_accuracy 26.80000114440918 train_accuracy 35.9375\n",
            "epoch 25 step 1 loss 2.1577436923980713\n",
            "epoch 25 step 2 loss 2.1518449783325195\n",
            "epoch 25 step 3 loss 2.200531482696533\n",
            "epoch 25 step 4 loss 2.1526286602020264\n",
            "epoch 25 step 5 loss 2.165457248687744\n",
            "epoch 25 step 6 loss 2.19112491607666\n",
            "epoch 25 step 7 loss 2.217291831970215\n",
            "epoch 25 step 8 loss 2.182723045349121\n",
            "epoch 25 step 9 loss 2.258122205734253\n",
            "epoch 25 step 10 loss 2.195066452026367\n",
            "epoch 25 step 11 loss 2.130974054336548\n",
            "epoch 25 step 12 loss 2.2131810188293457\n",
            "epoch 25 step 13 loss 2.201232433319092\n",
            "epoch 25 step 14 loss 2.1407971382141113\n",
            "epoch 25 step 15 loss 2.2078824043273926\n",
            "epoch 25 step 16 loss 2.2039575576782227\n",
            "epoch 25 step 17 loss 2.201366901397705\n",
            "epoch 25 step 18 loss 2.2001426219940186\n",
            "epoch 25 step 19 loss 2.1356194019317627\n",
            "epoch 25 step 20 loss 2.2301225662231445 test_accuracy 25.600000381469727 train_accuracy 26.5625\n",
            "epoch 25 step 21 loss 2.119913101196289\n",
            "epoch 25 step 22 loss 2.131781816482544\n",
            "epoch 25 step 23 loss 2.124638557434082\n",
            "epoch 25 step 24 loss 2.164426565170288\n",
            "epoch 25 step 25 loss 2.223912000656128\n",
            "epoch 25 step 26 loss 2.1501576900482178\n",
            "epoch 25 step 27 loss 2.116567850112915\n",
            "epoch 25 step 28 loss 2.079949378967285\n",
            "epoch 25 step 29 loss 2.165264368057251\n",
            "epoch 25 step 30 loss 2.157771587371826\n",
            "epoch 25 step 31 loss 2.1685240268707275\n",
            "epoch 25 step 32 loss 2.133352279663086\n",
            "epoch 25 step 33 loss 2.1210503578186035\n",
            "epoch 25 step 34 loss 2.1831700801849365\n",
            "epoch 25 step 35 loss 2.237363338470459\n",
            "epoch 26 step 0 loss 2.2198989391326904 test_accuracy 26.400001525878906 train_accuracy 22.65625\n",
            "epoch 26 step 1 loss 2.2119386196136475\n",
            "epoch 26 step 2 loss 2.198629379272461\n",
            "epoch 26 step 3 loss 2.135590076446533\n",
            "epoch 26 step 4 loss 2.095641613006592\n",
            "epoch 26 step 5 loss 2.1025514602661133\n",
            "epoch 26 step 6 loss 2.1880500316619873\n",
            "epoch 26 step 7 loss 2.1475565433502197\n",
            "epoch 26 step 8 loss 2.1445164680480957\n",
            "epoch 26 step 9 loss 2.177788734436035\n",
            "epoch 26 step 10 loss 2.1871578693389893\n",
            "epoch 26 step 11 loss 2.2154853343963623\n",
            "epoch 26 step 12 loss 2.1128296852111816\n",
            "epoch 26 step 13 loss 2.1641159057617188\n",
            "epoch 26 step 14 loss 2.165904998779297\n",
            "epoch 26 step 15 loss 2.185469388961792\n",
            "epoch 26 step 16 loss 2.1990158557891846\n",
            "epoch 26 step 17 loss 2.1540181636810303\n",
            "epoch 26 step 18 loss 2.138181209564209\n",
            "epoch 26 step 19 loss 2.1274349689483643\n",
            "epoch 26 step 20 loss 2.136625051498413 test_accuracy 28.60000228881836 train_accuracy 33.59375\n",
            "epoch 26 step 21 loss 2.165893077850342\n",
            "epoch 26 step 22 loss 2.1790547370910645\n",
            "epoch 26 step 23 loss 2.1776230335235596\n",
            "epoch 26 step 24 loss 2.161952257156372\n",
            "epoch 26 step 25 loss 2.158261775970459\n",
            "epoch 26 step 26 loss 2.2086358070373535\n",
            "epoch 26 step 27 loss 2.145674467086792\n",
            "epoch 26 step 28 loss 2.1862804889678955\n",
            "epoch 26 step 29 loss 2.2141354084014893\n",
            "epoch 26 step 30 loss 2.2516934871673584\n",
            "epoch 26 step 31 loss 2.1653518676757812\n",
            "epoch 26 step 32 loss 2.120492935180664\n",
            "epoch 26 step 33 loss 2.2252445220947266\n",
            "epoch 26 step 34 loss 2.134080648422241\n",
            "epoch 26 step 35 loss 2.018899440765381\n",
            "epoch 27 step 0 loss 2.1138288974761963 test_accuracy 25.80000114440918 train_accuracy 28.125\n",
            "epoch 27 step 1 loss 2.1825709342956543\n",
            "epoch 27 step 2 loss 2.1671905517578125\n",
            "epoch 27 step 3 loss 2.1548752784729004\n",
            "epoch 27 step 4 loss 2.2441861629486084\n",
            "epoch 27 step 5 loss 2.168774366378784\n",
            "epoch 27 step 6 loss 2.1272706985473633\n",
            "epoch 27 step 7 loss 2.140474319458008\n",
            "epoch 27 step 8 loss 2.206148147583008\n",
            "epoch 27 step 9 loss 2.155132532119751\n",
            "epoch 27 step 10 loss 2.1423654556274414\n",
            "epoch 27 step 11 loss 2.181291103363037\n",
            "epoch 27 step 12 loss 2.103203296661377\n",
            "epoch 27 step 13 loss 2.1539671421051025\n",
            "epoch 27 step 14 loss 2.192941427230835\n",
            "epoch 27 step 15 loss 2.180811643600464\n",
            "epoch 27 step 16 loss 2.160830020904541\n",
            "epoch 27 step 17 loss 2.144084930419922\n",
            "epoch 27 step 18 loss 2.170663356781006\n",
            "epoch 27 step 19 loss 2.2029731273651123\n",
            "epoch 27 step 20 loss 2.146890878677368 test_accuracy 28.80000114440918 train_accuracy 38.28125\n",
            "epoch 27 step 21 loss 2.1237542629241943\n",
            "epoch 27 step 22 loss 2.11449933052063\n",
            "epoch 27 step 23 loss 2.168300151824951\n",
            "epoch 27 step 24 loss 2.1350855827331543\n",
            "epoch 27 step 25 loss 2.164574146270752\n",
            "epoch 27 step 26 loss 2.1166324615478516\n",
            "epoch 27 step 27 loss 2.2225289344787598\n",
            "epoch 27 step 28 loss 2.1958868503570557\n",
            "epoch 27 step 29 loss 2.182704448699951\n",
            "epoch 27 step 30 loss 2.1386806964874268\n",
            "epoch 27 step 31 loss 2.1719555854797363\n",
            "epoch 27 step 32 loss 2.109147787094116\n",
            "epoch 27 step 33 loss 2.1864116191864014\n",
            "epoch 27 step 34 loss 2.222515821456909\n",
            "epoch 27 step 35 loss 2.187891721725464\n",
            "epoch 28 step 0 loss 2.205561637878418 test_accuracy 27.000001907348633 train_accuracy 28.90625\n",
            "epoch 28 step 1 loss 2.0910470485687256\n",
            "epoch 28 step 2 loss 2.1938045024871826\n",
            "epoch 28 step 3 loss 2.12725567817688\n",
            "epoch 28 step 4 loss 2.219578742980957\n",
            "epoch 28 step 5 loss 2.1864066123962402\n",
            "epoch 28 step 6 loss 2.1224825382232666\n",
            "epoch 28 step 7 loss 2.1803956031799316\n",
            "epoch 28 step 8 loss 2.198707103729248\n",
            "epoch 28 step 9 loss 2.193603992462158\n",
            "epoch 28 step 10 loss 2.178692102432251\n",
            "epoch 28 step 11 loss 2.1396775245666504\n",
            "epoch 28 step 12 loss 2.113266706466675\n",
            "epoch 28 step 13 loss 2.1357336044311523\n",
            "epoch 28 step 14 loss 2.1646127700805664\n",
            "epoch 28 step 15 loss 2.2297980785369873\n",
            "epoch 28 step 16 loss 2.1613104343414307\n",
            "epoch 28 step 17 loss 2.1400201320648193\n",
            "epoch 28 step 18 loss 2.176553964614868\n",
            "epoch 28 step 19 loss 2.173626184463501\n",
            "epoch 28 step 20 loss 2.1583304405212402 test_accuracy 26.400001525878906 train_accuracy 32.8125\n",
            "epoch 28 step 21 loss 2.1458892822265625\n",
            "epoch 28 step 22 loss 2.119094133377075\n",
            "epoch 28 step 23 loss 2.2346861362457275\n",
            "epoch 28 step 24 loss 2.1171302795410156\n",
            "epoch 28 step 25 loss 2.1822519302368164\n",
            "epoch 28 step 26 loss 2.216177463531494\n",
            "epoch 28 step 27 loss 2.164552688598633\n",
            "epoch 28 step 28 loss 2.14951229095459\n",
            "epoch 28 step 29 loss 2.137801170349121\n",
            "epoch 28 step 30 loss 2.207341194152832\n",
            "epoch 28 step 31 loss 2.1463420391082764\n",
            "epoch 28 step 32 loss 2.197047710418701\n",
            "epoch 28 step 33 loss 2.1728367805480957\n",
            "epoch 28 step 34 loss 2.192857503890991\n",
            "epoch 28 step 35 loss 2.2451694011688232\n",
            "epoch 29 step 0 loss 2.1719441413879395 test_accuracy 28.80000114440918 train_accuracy 24.21875\n",
            "epoch 29 step 1 loss 2.153677463531494\n",
            "epoch 29 step 2 loss 2.2518320083618164\n",
            "epoch 29 step 3 loss 2.176591157913208\n",
            "epoch 29 step 4 loss 2.1735880374908447\n",
            "epoch 29 step 5 loss 2.1298322677612305\n",
            "epoch 29 step 6 loss 2.172262668609619\n",
            "epoch 29 step 7 loss 2.1398019790649414\n",
            "epoch 29 step 8 loss 2.1006698608398438\n",
            "epoch 29 step 9 loss 2.14107084274292\n",
            "epoch 29 step 10 loss 2.1871891021728516\n",
            "epoch 29 step 11 loss 2.1291627883911133\n",
            "epoch 29 step 12 loss 2.0751891136169434\n",
            "epoch 29 step 13 loss 2.20293927192688\n",
            "epoch 29 step 14 loss 2.215454339981079\n",
            "epoch 29 step 15 loss 2.133598804473877\n",
            "epoch 29 step 16 loss 2.155099391937256\n",
            "epoch 29 step 17 loss 2.120978355407715\n",
            "epoch 29 step 18 loss 2.2330007553100586\n",
            "epoch 29 step 19 loss 2.1582581996917725\n",
            "epoch 29 step 20 loss 2.153334856033325 test_accuracy 29.000001907348633 train_accuracy 37.5\n",
            "epoch 29 step 21 loss 2.1494596004486084\n",
            "epoch 29 step 22 loss 2.1588213443756104\n",
            "epoch 29 step 23 loss 2.1667258739471436\n",
            "epoch 29 step 24 loss 2.1428751945495605\n",
            "epoch 29 step 25 loss 2.18129301071167\n",
            "epoch 29 step 26 loss 2.1542418003082275\n",
            "epoch 29 step 27 loss 2.1807947158813477\n",
            "epoch 29 step 28 loss 2.151698350906372\n",
            "epoch 29 step 29 loss 2.114037275314331\n",
            "epoch 29 step 30 loss 2.1931161880493164\n",
            "epoch 29 step 31 loss 2.205697774887085\n",
            "epoch 29 step 32 loss 2.1698005199432373\n",
            "epoch 29 step 33 loss 2.180649518966675\n",
            "epoch 29 step 34 loss 2.10862398147583\n",
            "epoch 29 step 35 loss 2.1041204929351807\n",
            "epoch 30 step 0 loss 2.174482822418213 test_accuracy 27.400001525878906 train_accuracy 28.90625\n",
            "epoch 30 step 1 loss 2.1744232177734375\n",
            "epoch 30 step 2 loss 2.243758201599121\n",
            "epoch 30 step 3 loss 2.2157390117645264\n",
            "epoch 30 step 4 loss 2.145705461502075\n",
            "epoch 30 step 5 loss 2.1452901363372803\n",
            "epoch 30 step 6 loss 2.1350343227386475\n",
            "epoch 30 step 7 loss 2.1644465923309326\n",
            "epoch 30 step 8 loss 2.1747684478759766\n",
            "epoch 30 step 9 loss 2.1932010650634766\n",
            "epoch 30 step 10 loss 2.1071956157684326\n",
            "epoch 30 step 11 loss 2.1406443119049072\n",
            "epoch 30 step 12 loss 2.1587092876434326\n",
            "epoch 30 step 13 loss 2.1278083324432373\n",
            "epoch 30 step 14 loss 2.1547045707702637\n",
            "epoch 30 step 15 loss 2.1832785606384277\n",
            "epoch 30 step 16 loss 2.1833715438842773\n",
            "epoch 30 step 17 loss 2.163353204727173\n",
            "epoch 30 step 18 loss 2.1593849658966064\n",
            "epoch 30 step 19 loss 2.2057945728302\n",
            "epoch 30 step 20 loss 2.171539068222046 test_accuracy 29.000001907348633 train_accuracy 34.375\n",
            "epoch 30 step 21 loss 2.1400153636932373\n",
            "epoch 30 step 22 loss 2.1635055541992188\n",
            "epoch 30 step 23 loss 2.160574197769165\n",
            "epoch 30 step 24 loss 2.202969551086426\n",
            "epoch 30 step 25 loss 2.134913921356201\n",
            "epoch 30 step 26 loss 2.1666784286499023\n",
            "epoch 30 step 27 loss 2.1261329650878906\n",
            "epoch 30 step 28 loss 2.1851205825805664\n",
            "epoch 30 step 29 loss 2.1463019847869873\n",
            "epoch 30 step 30 loss 2.133544921875\n",
            "epoch 30 step 31 loss 2.1808056831359863\n",
            "epoch 30 step 32 loss 2.1193618774414062\n",
            "epoch 30 step 33 loss 2.196423053741455\n",
            "epoch 30 step 34 loss 2.1619746685028076\n",
            "epoch 30 step 35 loss 2.1794352531433105\n",
            "epoch 31 step 0 loss 2.1629867553710938 test_accuracy 27.80000114440918 train_accuracy 27.34375\n",
            "epoch 31 step 1 loss 2.115020275115967\n",
            "epoch 31 step 2 loss 2.210660457611084\n",
            "epoch 31 step 3 loss 2.157529592514038\n",
            "epoch 31 step 4 loss 2.162081241607666\n",
            "epoch 31 step 5 loss 2.058464765548706\n",
            "epoch 31 step 6 loss 2.1915042400360107\n",
            "epoch 31 step 7 loss 2.1965014934539795\n",
            "epoch 31 step 8 loss 2.1232306957244873\n",
            "epoch 31 step 9 loss 2.143312692642212\n",
            "epoch 31 step 10 loss 2.128575563430786\n",
            "epoch 31 step 11 loss 2.148958921432495\n",
            "epoch 31 step 12 loss 2.1666033267974854\n",
            "epoch 31 step 13 loss 2.0675621032714844\n",
            "epoch 31 step 14 loss 2.1658642292022705\n",
            "epoch 31 step 15 loss 2.176154375076294\n",
            "epoch 31 step 16 loss 2.203530788421631\n",
            "epoch 31 step 17 loss 2.1292195320129395\n",
            "epoch 31 step 18 loss 2.1653711795806885\n",
            "epoch 31 step 19 loss 2.11002779006958\n",
            "epoch 31 step 20 loss 2.1689906120300293 test_accuracy 26.200000762939453 train_accuracy 29.6875\n",
            "epoch 31 step 21 loss 2.1608195304870605\n",
            "epoch 31 step 22 loss 2.1804933547973633\n",
            "epoch 31 step 23 loss 2.1771838665008545\n",
            "epoch 31 step 24 loss 2.1778922080993652\n",
            "epoch 31 step 25 loss 2.1429085731506348\n",
            "epoch 31 step 26 loss 2.2339279651641846\n",
            "epoch 31 step 27 loss 2.177180290222168\n",
            "epoch 31 step 28 loss 2.1529691219329834\n",
            "epoch 31 step 29 loss 2.2027790546417236\n",
            "epoch 31 step 30 loss 2.148913860321045\n",
            "epoch 31 step 31 loss 2.167722702026367\n",
            "epoch 31 step 32 loss 2.1746625900268555\n",
            "epoch 31 step 33 loss 2.206835985183716\n",
            "epoch 31 step 34 loss 2.1180806159973145\n",
            "epoch 31 step 35 loss 2.1889662742614746\n",
            "epoch 32 step 0 loss 2.2012314796447754 test_accuracy 25.600000381469727 train_accuracy 32.8125\n",
            "epoch 32 step 1 loss 2.171114444732666\n",
            "epoch 32 step 2 loss 2.1399569511413574\n",
            "epoch 32 step 3 loss 2.1969101428985596\n",
            "epoch 32 step 4 loss 2.143073797225952\n",
            "epoch 32 step 5 loss 2.084897756576538\n",
            "epoch 32 step 6 loss 2.1443233489990234\n",
            "epoch 32 step 7 loss 2.1333491802215576\n",
            "epoch 32 step 8 loss 2.1613965034484863\n",
            "epoch 32 step 9 loss 2.210179090499878\n",
            "epoch 32 step 10 loss 2.1963765621185303\n",
            "epoch 32 step 11 loss 2.2102534770965576\n",
            "epoch 32 step 12 loss 2.231128215789795\n",
            "epoch 32 step 13 loss 2.17335844039917\n",
            "epoch 32 step 14 loss 2.1486642360687256\n",
            "epoch 32 step 15 loss 2.1504967212677\n",
            "epoch 32 step 16 loss 2.2460215091705322\n",
            "epoch 32 step 17 loss 2.1701724529266357\n",
            "epoch 32 step 18 loss 2.162839651107788\n",
            "epoch 32 step 19 loss 2.090202569961548\n",
            "epoch 32 step 20 loss 2.2126052379608154 test_accuracy 28.400001525878906 train_accuracy 30.46875\n",
            "epoch 32 step 21 loss 2.20436954498291\n",
            "epoch 32 step 22 loss 2.135791540145874\n",
            "epoch 32 step 23 loss 2.1971938610076904\n",
            "epoch 32 step 24 loss 2.140773296356201\n",
            "epoch 32 step 25 loss 2.1828696727752686\n",
            "epoch 32 step 26 loss 2.1659953594207764\n",
            "epoch 32 step 27 loss 2.1448023319244385\n",
            "epoch 32 step 28 loss 2.173893690109253\n",
            "epoch 32 step 29 loss 2.17668080329895\n",
            "epoch 32 step 30 loss 2.1582701206207275\n",
            "epoch 32 step 31 loss 2.1253883838653564\n",
            "epoch 32 step 32 loss 2.0823862552642822\n",
            "epoch 32 step 33 loss 2.127316951751709\n",
            "epoch 32 step 34 loss 2.138930559158325\n",
            "epoch 32 step 35 loss 2.032594680786133\n",
            "epoch 33 step 0 loss 2.185987949371338 test_accuracy 25.400001525878906 train_accuracy 27.34375\n",
            "epoch 33 step 1 loss 2.1247997283935547\n",
            "epoch 33 step 2 loss 2.150670051574707\n",
            "epoch 33 step 3 loss 2.1827266216278076\n",
            "epoch 33 step 4 loss 2.2160227298736572\n",
            "epoch 33 step 5 loss 2.22518253326416\n",
            "epoch 33 step 6 loss 2.1920156478881836\n",
            "epoch 33 step 7 loss 2.1190967559814453\n",
            "epoch 33 step 8 loss 2.2551798820495605\n",
            "epoch 33 step 9 loss 2.186600685119629\n",
            "epoch 33 step 10 loss 2.140249729156494\n",
            "epoch 33 step 11 loss 2.146904706954956\n",
            "epoch 33 step 12 loss 2.1129019260406494\n",
            "epoch 33 step 13 loss 2.142177104949951\n",
            "epoch 33 step 14 loss 2.0974221229553223\n",
            "epoch 33 step 15 loss 2.142723798751831\n",
            "epoch 33 step 16 loss 2.191021203994751\n",
            "epoch 33 step 17 loss 2.175887107849121\n",
            "epoch 33 step 18 loss 2.204545497894287\n",
            "epoch 33 step 19 loss 2.072587013244629\n",
            "epoch 33 step 20 loss 2.1038339138031006 test_accuracy 24.80000114440918 train_accuracy 35.9375\n",
            "epoch 33 step 21 loss 2.169138193130493\n",
            "epoch 33 step 22 loss 2.1899969577789307\n",
            "epoch 33 step 23 loss 2.1664178371429443\n",
            "epoch 33 step 24 loss 2.1370763778686523\n",
            "epoch 33 step 25 loss 2.148777723312378\n",
            "epoch 33 step 26 loss 2.1554667949676514\n",
            "epoch 33 step 27 loss 2.156365394592285\n",
            "epoch 33 step 28 loss 2.189708948135376\n",
            "epoch 33 step 29 loss 2.106471300125122\n",
            "epoch 33 step 30 loss 2.2060885429382324\n",
            "epoch 33 step 31 loss 2.1610755920410156\n",
            "epoch 33 step 32 loss 2.0966124534606934\n",
            "epoch 33 step 33 loss 2.18803071975708\n",
            "epoch 33 step 34 loss 2.1648921966552734\n",
            "epoch 33 step 35 loss 2.193063974380493\n",
            "epoch 34 step 0 loss 2.0920426845550537 test_accuracy 26.000001907348633 train_accuracy 30.46875\n",
            "epoch 34 step 1 loss 2.153424024581909\n",
            "epoch 34 step 2 loss 2.1329798698425293\n",
            "epoch 34 step 3 loss 2.1992788314819336\n",
            "epoch 34 step 4 loss 2.06819748878479\n",
            "epoch 34 step 5 loss 2.0922110080718994\n",
            "epoch 34 step 6 loss 2.1382009983062744\n",
            "epoch 34 step 7 loss 2.161874532699585\n",
            "epoch 34 step 8 loss 2.124873638153076\n",
            "epoch 34 step 9 loss 2.1589670181274414\n",
            "epoch 34 step 10 loss 2.241886615753174\n",
            "epoch 34 step 11 loss 2.2308995723724365\n",
            "epoch 34 step 12 loss 2.116919994354248\n",
            "epoch 34 step 13 loss 2.1527929306030273\n",
            "epoch 34 step 14 loss 2.2014176845550537\n",
            "epoch 34 step 15 loss 2.178182363510132\n",
            "epoch 34 step 16 loss 2.066030740737915\n",
            "epoch 34 step 17 loss 2.2359201908111572\n",
            "epoch 34 step 18 loss 2.1892569065093994\n",
            "epoch 34 step 19 loss 2.211881637573242\n",
            "epoch 34 step 20 loss 2.1565864086151123 test_accuracy 26.400001525878906 train_accuracy 32.03125\n",
            "epoch 34 step 21 loss 2.1502299308776855\n",
            "epoch 34 step 22 loss 2.168256998062134\n",
            "epoch 34 step 23 loss 2.1986286640167236\n",
            "epoch 34 step 24 loss 2.199727773666382\n",
            "epoch 34 step 25 loss 2.1448631286621094\n",
            "epoch 34 step 26 loss 2.1685614585876465\n",
            "epoch 34 step 27 loss 2.1190502643585205\n",
            "epoch 34 step 28 loss 2.159705400466919\n",
            "epoch 34 step 29 loss 2.1369800567626953\n",
            "epoch 34 step 30 loss 2.146355152130127\n",
            "epoch 34 step 31 loss 2.1649036407470703\n",
            "epoch 34 step 32 loss 2.1877923011779785\n",
            "epoch 34 step 33 loss 2.174694061279297\n",
            "epoch 34 step 34 loss 2.1689038276672363\n",
            "epoch 34 step 35 loss 2.0347182750701904\n",
            "epoch 35 step 0 loss 2.1778228282928467 test_accuracy 26.200000762939453 train_accuracy 34.375\n",
            "epoch 35 step 1 loss 2.11313533782959\n",
            "epoch 35 step 2 loss 2.2009658813476562\n",
            "epoch 35 step 3 loss 2.101341485977173\n",
            "epoch 35 step 4 loss 2.0746653079986572\n",
            "epoch 35 step 5 loss 2.195802688598633\n",
            "epoch 35 step 6 loss 2.13684344291687\n",
            "epoch 35 step 7 loss 2.1108226776123047\n",
            "epoch 35 step 8 loss 2.143440008163452\n",
            "epoch 35 step 9 loss 2.1294102668762207\n",
            "epoch 35 step 10 loss 2.1788227558135986\n",
            "epoch 35 step 11 loss 2.17537784576416\n",
            "epoch 35 step 12 loss 2.192239999771118\n",
            "epoch 35 step 13 loss 2.219069719314575\n",
            "epoch 35 step 14 loss 2.143411159515381\n",
            "epoch 35 step 15 loss 2.231727123260498\n",
            "epoch 35 step 16 loss 2.171455144882202\n",
            "epoch 35 step 17 loss 2.1720187664031982\n",
            "epoch 35 step 18 loss 2.1657140254974365\n",
            "epoch 35 step 19 loss 2.200615167617798\n",
            "epoch 35 step 20 loss 2.17389178276062 test_accuracy 25.80000114440918 train_accuracy 29.6875\n",
            "epoch 35 step 21 loss 2.1904382705688477\n",
            "epoch 35 step 22 loss 2.1794979572296143\n",
            "epoch 35 step 23 loss 2.1746597290039062\n",
            "epoch 35 step 24 loss 2.1434097290039062\n",
            "epoch 35 step 25 loss 2.149892568588257\n",
            "epoch 35 step 26 loss 2.099368095397949\n",
            "epoch 35 step 27 loss 2.1114609241485596\n",
            "epoch 35 step 28 loss 2.1906864643096924\n",
            "epoch 35 step 29 loss 2.2032101154327393\n",
            "epoch 35 step 30 loss 2.1286098957061768\n",
            "epoch 35 step 31 loss 2.1181998252868652\n",
            "epoch 35 step 32 loss 2.1455018520355225\n",
            "epoch 35 step 33 loss 2.1367485523223877\n",
            "epoch 35 step 34 loss 2.0779595375061035\n",
            "epoch 35 step 35 loss 2.209817409515381\n",
            "epoch 36 step 0 loss 2.1695196628570557 test_accuracy 23.200000762939453 train_accuracy 29.6875\n",
            "epoch 36 step 1 loss 2.1393988132476807\n",
            "epoch 36 step 2 loss 2.19132399559021\n",
            "epoch 36 step 3 loss 2.1823391914367676\n",
            "epoch 36 step 4 loss 2.207948684692383\n",
            "epoch 36 step 5 loss 2.103764533996582\n",
            "epoch 36 step 6 loss 2.1200273036956787\n",
            "epoch 36 step 7 loss 2.1592025756835938\n",
            "epoch 36 step 8 loss 2.270949363708496\n",
            "epoch 36 step 9 loss 2.191216230392456\n",
            "epoch 36 step 10 loss 2.160487413406372\n",
            "epoch 36 step 11 loss 2.1623189449310303\n",
            "epoch 36 step 12 loss 2.1330182552337646\n",
            "epoch 36 step 13 loss 2.1642112731933594\n",
            "epoch 36 step 14 loss 2.106673240661621\n",
            "epoch 36 step 15 loss 2.139918804168701\n",
            "epoch 36 step 16 loss 2.151167631149292\n",
            "epoch 36 step 17 loss 2.109257459640503\n",
            "epoch 36 step 18 loss 2.1567459106445312\n",
            "epoch 36 step 19 loss 2.2168524265289307\n",
            "epoch 36 step 20 loss 2.186138153076172 test_accuracy 26.80000114440918 train_accuracy 32.03125\n",
            "epoch 36 step 21 loss 2.164595603942871\n",
            "epoch 36 step 22 loss 2.1961300373077393\n",
            "epoch 36 step 23 loss 2.175520658493042\n",
            "epoch 36 step 24 loss 2.1708950996398926\n",
            "epoch 36 step 25 loss 2.1464128494262695\n",
            "epoch 36 step 26 loss 2.2069928646087646\n",
            "epoch 36 step 27 loss 2.1240999698638916\n",
            "epoch 36 step 28 loss 2.1720263957977295\n",
            "epoch 36 step 29 loss 2.159447193145752\n",
            "epoch 36 step 30 loss 2.1218512058258057\n",
            "epoch 36 step 31 loss 2.1301724910736084\n",
            "epoch 36 step 32 loss 2.1978302001953125\n",
            "epoch 36 step 33 loss 2.1793320178985596\n",
            "epoch 36 step 34 loss 2.2567567825317383\n",
            "epoch 36 step 35 loss 2.3951239585876465\n",
            "epoch 37 step 0 loss 2.151201009750366 test_accuracy 25.000001907348633 train_accuracy 30.46875\n",
            "epoch 37 step 1 loss 2.1516549587249756\n",
            "epoch 37 step 2 loss 2.201388120651245\n",
            "epoch 37 step 3 loss 2.124044895172119\n",
            "epoch 37 step 4 loss 2.1585352420806885\n",
            "epoch 37 step 5 loss 2.1694486141204834\n",
            "epoch 37 step 6 loss 2.172290563583374\n",
            "epoch 37 step 7 loss 2.1520638465881348\n",
            "epoch 37 step 8 loss 2.1651461124420166\n",
            "epoch 37 step 9 loss 2.180774211883545\n",
            "epoch 37 step 10 loss 2.2065930366516113\n",
            "epoch 37 step 11 loss 2.2336580753326416\n",
            "epoch 37 step 12 loss 2.1933796405792236\n",
            "epoch 37 step 13 loss 2.114241600036621\n",
            "epoch 37 step 14 loss 2.1371006965637207\n",
            "epoch 37 step 15 loss 2.140545129776001\n",
            "epoch 37 step 16 loss 2.15059757232666\n",
            "epoch 37 step 17 loss 2.0744073390960693\n",
            "epoch 37 step 18 loss 2.1857504844665527\n",
            "epoch 37 step 19 loss 2.195739269256592\n",
            "epoch 37 step 20 loss 2.223034620285034 test_accuracy 23.600000381469727 train_accuracy 31.25\n",
            "epoch 37 step 21 loss 2.137244701385498\n",
            "epoch 37 step 22 loss 2.1972832679748535\n",
            "epoch 37 step 23 loss 2.101320266723633\n",
            "epoch 37 step 24 loss 2.145404100418091\n",
            "epoch 37 step 25 loss 2.127068519592285\n",
            "epoch 37 step 26 loss 2.2060258388519287\n",
            "epoch 37 step 27 loss 2.166692018508911\n",
            "epoch 37 step 28 loss 2.2040228843688965\n",
            "epoch 37 step 29 loss 2.193896770477295\n",
            "epoch 37 step 30 loss 2.1667051315307617\n",
            "epoch 37 step 31 loss 2.1910760402679443\n",
            "epoch 37 step 32 loss 2.240225315093994\n",
            "epoch 37 step 33 loss 2.1591203212738037\n",
            "epoch 37 step 34 loss 2.148193836212158\n",
            "epoch 37 step 35 loss 2.0863358974456787\n",
            "epoch 38 step 0 loss 2.198652505874634 test_accuracy 29.60000228881836 train_accuracy 29.6875\n",
            "epoch 38 step 1 loss 2.135207176208496\n",
            "epoch 38 step 2 loss 2.145216226577759\n",
            "epoch 38 step 3 loss 2.18204665184021\n",
            "epoch 38 step 4 loss 2.1570656299591064\n",
            "epoch 38 step 5 loss 2.188900947570801\n",
            "epoch 38 step 6 loss 2.121216058731079\n",
            "epoch 38 step 7 loss 2.0888588428497314\n",
            "epoch 38 step 8 loss 2.175281286239624\n",
            "epoch 38 step 9 loss 2.0491933822631836\n",
            "epoch 38 step 10 loss 2.1824615001678467\n",
            "epoch 38 step 11 loss 2.1124935150146484\n",
            "epoch 38 step 12 loss 2.1462960243225098\n",
            "epoch 38 step 13 loss 2.2105321884155273\n",
            "epoch 38 step 14 loss 2.198093891143799\n",
            "epoch 38 step 15 loss 2.1614229679107666\n",
            "epoch 38 step 16 loss 2.136378526687622\n",
            "epoch 38 step 17 loss 2.0977349281311035\n",
            "epoch 38 step 18 loss 2.142561197280884\n",
            "epoch 38 step 19 loss 2.1513569355010986\n",
            "epoch 38 step 20 loss 2.1646957397460938 test_accuracy 26.000001907348633 train_accuracy 23.4375\n",
            "epoch 38 step 21 loss 2.2103047370910645\n",
            "epoch 38 step 22 loss 2.1029345989227295\n",
            "epoch 38 step 23 loss 2.168703556060791\n",
            "epoch 38 step 24 loss 2.0536677837371826\n",
            "epoch 38 step 25 loss 2.1534273624420166\n",
            "epoch 38 step 26 loss 2.214960813522339\n",
            "epoch 38 step 27 loss 2.11716365814209\n",
            "epoch 38 step 28 loss 2.165719747543335\n",
            "epoch 38 step 29 loss 2.2137668132781982\n",
            "epoch 38 step 30 loss 2.1596786975860596\n",
            "epoch 38 step 31 loss 2.2352147102355957\n",
            "epoch 38 step 32 loss 2.1684634685516357\n",
            "epoch 38 step 33 loss 2.1826138496398926\n",
            "epoch 38 step 34 loss 2.176213502883911\n",
            "epoch 38 step 35 loss 1.9622936248779297\n",
            "epoch 39 step 0 loss 2.1581919193267822 test_accuracy 25.400001525878906 train_accuracy 28.90625\n",
            "epoch 39 step 1 loss 2.140730381011963\n",
            "epoch 39 step 2 loss 2.0789637565612793\n",
            "epoch 39 step 3 loss 2.198983669281006\n",
            "epoch 39 step 4 loss 2.162773609161377\n",
            "epoch 39 step 5 loss 2.07846736907959\n",
            "epoch 39 step 6 loss 2.168762445449829\n",
            "epoch 39 step 7 loss 2.16125750541687\n",
            "epoch 39 step 8 loss 2.1469366550445557\n",
            "epoch 39 step 9 loss 2.0961365699768066\n",
            "epoch 39 step 10 loss 2.1534228324890137\n",
            "epoch 39 step 11 loss 2.193772554397583\n",
            "epoch 39 step 12 loss 2.1243929862976074\n",
            "epoch 39 step 13 loss 2.2521331310272217\n",
            "epoch 39 step 14 loss 2.1221072673797607\n",
            "epoch 39 step 15 loss 2.149474620819092\n",
            "epoch 39 step 16 loss 2.181529998779297\n",
            "epoch 39 step 17 loss 2.1718974113464355\n",
            "epoch 39 step 18 loss 2.2154793739318848\n",
            "epoch 39 step 19 loss 2.1871283054351807\n",
            "epoch 39 step 20 loss 2.178102493286133 test_accuracy 26.000001907348633 train_accuracy 31.25\n",
            "epoch 39 step 21 loss 2.1420342922210693\n",
            "epoch 39 step 22 loss 2.1373653411865234\n",
            "epoch 39 step 23 loss 2.155081272125244\n",
            "epoch 39 step 24 loss 2.1656405925750732\n",
            "epoch 39 step 25 loss 2.194373846054077\n",
            "epoch 39 step 26 loss 2.134467363357544\n",
            "epoch 39 step 27 loss 2.164236307144165\n",
            "epoch 39 step 28 loss 2.1289992332458496\n",
            "epoch 39 step 29 loss 2.134495496749878\n",
            "epoch 39 step 30 loss 2.1611862182617188\n",
            "epoch 39 step 31 loss 2.1634345054626465\n",
            "epoch 39 step 32 loss 2.162369728088379\n",
            "epoch 39 step 33 loss 2.0591049194335938\n",
            "epoch 39 step 34 loss 2.145799398422241\n",
            "epoch 39 step 35 loss 2.0180933475494385\n",
            "epoch 40 step 0 loss 2.2043514251708984 test_accuracy 26.80000114440918 train_accuracy 24.21875\n",
            "epoch 40 step 1 loss 2.184143304824829\n",
            "epoch 40 step 2 loss 2.1451849937438965\n",
            "epoch 40 step 3 loss 2.2112443447113037\n",
            "epoch 40 step 4 loss 2.0668721199035645\n",
            "epoch 40 step 5 loss 2.142026901245117\n",
            "epoch 40 step 6 loss 2.1667799949645996\n",
            "epoch 40 step 7 loss 2.1903419494628906\n",
            "epoch 40 step 8 loss 2.20043683052063\n",
            "epoch 40 step 9 loss 2.208296298980713\n",
            "epoch 40 step 10 loss 2.0911812782287598\n",
            "epoch 40 step 11 loss 2.1973321437835693\n",
            "epoch 40 step 12 loss 2.133784055709839\n",
            "epoch 40 step 13 loss 2.195988655090332\n",
            "epoch 40 step 14 loss 2.168973684310913\n",
            "epoch 40 step 15 loss 2.15958833694458\n",
            "epoch 40 step 16 loss 2.1829402446746826\n",
            "epoch 40 step 17 loss 2.092637300491333\n",
            "epoch 40 step 18 loss 2.2335784435272217\n",
            "epoch 40 step 19 loss 2.1542210578918457\n",
            "epoch 40 step 20 loss 2.157867193222046 test_accuracy 29.60000228881836 train_accuracy 36.71875\n",
            "epoch 40 step 21 loss 2.172908067703247\n",
            "epoch 40 step 22 loss 2.162710666656494\n",
            "epoch 40 step 23 loss 2.1950466632843018\n",
            "epoch 40 step 24 loss 2.147329807281494\n",
            "epoch 40 step 25 loss 2.112191677093506\n",
            "epoch 40 step 26 loss 2.2072081565856934\n",
            "epoch 40 step 27 loss 2.1864755153656006\n",
            "epoch 40 step 28 loss 2.1976351737976074\n",
            "epoch 40 step 29 loss 2.136636734008789\n",
            "epoch 40 step 30 loss 2.097963809967041\n",
            "epoch 40 step 31 loss 2.1505074501037598\n",
            "epoch 40 step 32 loss 2.154644727706909\n",
            "epoch 40 step 33 loss 2.1235241889953613\n",
            "epoch 40 step 34 loss 2.1446895599365234\n",
            "epoch 40 step 35 loss 2.1900391578674316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbzN9Iu9rph5",
        "outputId": "20b47e94-30c0-4e81-cd3a-369e5fc833c5"
      },
      "source": [
        "souptest = DataLoader(dataset=test_data,batch_size = 64,shuffle = True)\n",
        "def teste(data):\n",
        "    c = 0\n",
        "    s = 0\n",
        "    for x,y in (data):\n",
        "        with torch.no_grad():\n",
        "            x =x.to(device)\n",
        "            y = y.to(device)\n",
        "            yt = model(x)\n",
        "            yt = torch.squeeze(yt)\n",
        "            yt = torch.argmax(yt, dim= 1)\n",
        "            c += (y == yt).sum()\n",
        "            s += y.shape[0]\n",
        "    return (100*c/s).item()\n",
        "\n",
        "teste(souptest)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.400001525878906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}